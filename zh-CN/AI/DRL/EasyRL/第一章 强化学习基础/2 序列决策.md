## 智能体和环境

强化学习研究的问题是智能体与环境交互的问题，下图中左边的智能体一直在与右边的环境进行交互。智能体把它的动作（action）输出给环境，环境取得这个动作后会进行下一步，把下一步的观测（state）与这个动作带来的奖励（reward）返还给智能体。这样的交互会产生很多观测，智能体的目的是从这些观测之中学到能最大化奖励的策略

![[pic-20250307165833492.png]]

## 奖励

奖励是由环境给的一种**标量**的反馈信号（scalar feedback signal），这种信号可显示智能体在某一步采取某个策略的表现如何。强化学习的目的就是最大化智能体可以获得的奖励，智能体在环境里面存在的目的就是最大化它的**期望的累积奖励**（expected cumulative reward）。不同的环境中，奖励也是不同的：

1. 比如下棋，目的就是赢棋，当棋局结束时，就会得到一个正奖励（赢）或负奖励（输）
2. 在股票管理里，奖励由股票获取的奖励和损失决定

## 序列决策

在强化学习中，智能体的目标是通过一系列动作最大化奖励，但奖励通常是延迟的，智能体需要**在近期奖励和远期奖励之间进行权衡**（trade-off），研究怎么让智能体取得更多的远期奖励

在与环境的交互过程中，智能体会获得很多观测。针对每一个观测，智能体会采取一个动作，也会得到一个奖励。所以历史是观测 $o_{t}$、动作 $a_{t}$、奖励 $r_{t}$ 的序列：

$$
H_t = o_1, a_1, r_1, \dots, o_t, a_t, r_t
$$

智能体在采取当前动作的时候会依赖于它之前得到的历史，所以我们可以把整个游戏的状态看成关于这个历史的函数：

$$
S_{t} = f(H_t)
$$

## 状态和观测的关系

状态是对世界的完整描述，**不会隐藏**世界的信息。观测是对状态的部分描述，可能会遗漏一些信息。在深度强化学习中，我们几乎总是用实值的向量、矩阵或者更高阶的张量来表示状态和观测。例如，我们可以用 RGB 像素值的矩阵来表示一个视觉的观测，可以用机器人关节的角度和速度来表示一个机器人的状态

