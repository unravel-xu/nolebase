## 智能体和环境

强化学习研究的问题是智能体与环境交互的问题，下图中左边的智能体一直在与右边的环境进行交互。智能体把它的动作（action）输出给环境，环境取得这个动作后会进行下一步，把下一步的观测（state）与这个动作带来的奖励（reward）返还给智能体。这样的交互会产生很多观测，智能体的目的是从这些观测之中学到能最大化奖励的策略

![[pic-20250307165833492.png]]

## 奖励

奖励是由环境给的一种**标量**的反馈信号（scalar feedback signal），这种信号可显示智能体在某一步采取某个策略的表现如何。强化学习的目的就是最大化智能体可以获得的奖励，智能体在环境里面存在的目的就是最大化它的**期望的累积奖励**（expected cumulative reward）。不同的环境中，奖励也是不同的：

1. 比如下棋，目的就是赢棋，当棋局结束时，就会得到一个正奖励（赢）或负奖励（输）
2. 在股票管理里，奖励由股票获取的奖励和损失决定

## 序列决策

在强化学习中，智能体的目标是通过一系列动作最大化奖励，但奖励通常是延迟的，智能体需要**在近期奖励和远期奖励之间进行权衡**（trade-off），研究怎么让智能体取得更多的远期奖励

在与环境的交互过程中，智能体会获得对于所处环境状态的观测（有可能只是**一部分**）。针对每一个观测，智能体会采取一个动作，也会得到一个奖励。所以历史是观测 $o_{t}$、动作 $a_{t}$、奖励 $r_{t}$ 的序列：

$$
H_t = o_1, a_1, r_1, \dots, o_t, a_t, r_t
$$

> [!important]
> 强化学习中，智能体理论上应该是基于环境状态决定采取的动作，但实际无法完全知道状态，只能通过观测了解部分状态。所以一些理论公式会写 $s_1 \to a_1, r_1$ 而一些会写 $o_1\to a_1, r_1$

智能体在采取当前动作的时候会依赖于它之前得到的历史，所以我们可以把整个游戏的状态看成关于这个历史的函数：

$$
S_{t} = f(H_t)
$$




## 状态和观测的关系

状态是对世界的完整描述，**不会隐藏**世界的信息。观测是对状态的部分描述，可能会遗漏一些信息。在深度强化学习中，我们几乎总是用实值的向量、矩阵或者更高阶的张量来表示状态和观测

环境有自己的函数 $s_{t}^e=f^e(H_t)$ 来更新（环境）状态，在智能体的内部也有一个函数 $s_t^a=f^a(H_t)$ 来更新（智能体）状态。当智能体的状态和环境的状态等价时，即当智能体能够观察到环境的所有状态时，我们称这个环境是**完全可观测**的（fully observed）。在这种情况下面，强化学习通常被建模成一个**马尔可夫决策过程 （Markov decision process，MDP）** 的问题。在马尔可夫决策过程中：

$$
o_t = s_t^{env} = s_t^{agent}
$$

在强化学习的设定里面， 环境的状态才是真正的所有状态。比如智能体在玩纸牌游戏，它能看到的其实是牌面上的牌。当智能体只能看到部分的观测，我们就称这个环境是**部分可观测的**（partially observed）。 在这种情况下，强化学习通常被建模成**部分可观测马尔可夫决策过程**（partially observable Markov decision process, POMDP）的问题。部分可观测马尔可夫决策过程是马尔可夫决策过程的一种泛化。 **部分可观测马尔可夫决策过程依然具有马尔可夫性质**，其过程可以用一个七元组描述：

$$
(S, A, T, R, \Omega, O, \gamma)
$$

- S：状态空间
- A：动作空间
- $T(s'|s,a)$：状态转移概率
- R：奖励函数
- $\Omega(o|s,a)$：观测概率
- O：观测空间
- $\gamma$：折扣系数