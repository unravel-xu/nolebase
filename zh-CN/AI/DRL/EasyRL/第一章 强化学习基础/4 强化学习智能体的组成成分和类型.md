对于一个强化学习智能体，它可能有一个或多个如下的组成成分

- 策略（policy）：智能体会用策略来选取下一步的动作
- 价值函数（value function）：用价值函数来对**当前状态**进行评估。价值函数用于评估智能体进入某个状态后，可以对后面的奖励带来多大的影响。价值函数值越大，说明智能体进入这个状态越有利
- 模型（model）：模型表示智能体对环境的状态进行理解，它决定了环境中世界的运行方式

## 策略

策略其实就是一个函数，用于将输入的状态变为动作。分为随机性策略和确定性策略：

- **随机性策略**（stochastic policy）用 $\pi$ 函数表示，即 $\pi(a\mid s)=p(a_t=a\mid s_t=s)$，输入一个状态 s，输出一个概率，这个概率是智能体**所有动作的概率**。或用 $a_t \sim \pi_{\theta}(\cdot\mid s_t)$ 表示，其中 $\theta$ 是策略的参数
- **确定性策略**（deterministic policy）就是智能体直接采取最有可能的动作，即 $a^* = argmax_a\pi(a\mid s)$ 或用 $a_t = \mu_{\theta}(s_t)$ 表示

因为策略本质上就是智能体的“大脑”，所以很多时候“策略”和“智能体”这两个名词经常互换，例如我们会说：“策略的目的是最大化奖励”

通常情况下，强化学习一般使用随机性策略，随机性策略有很多优点。比如，在学习时可以通过引入一定的随机性来更好地探索环境

## 轨迹

轨迹（trajectory）$\tau$ 指的是状态和动作的序列：

$$
\tau = (s_0, a_0, s_1, a_1, \dots)
$$

第一个状态 $s_0$，是从**开始状态分布**中随机采样的，有时候表示为 $\rho_{0}(\cdot)$：

$$
s_{0} \sim \rho_{0}(\cdot)
$$

轨迹常常也被称为**回合**（episodes）或者 **rollouts**

## 奖励和回报

强化学习中，奖励函数 R 非常重要。它由当前状态、已经执行的动作和下一步的状态共同决定：

$$
r_{t} = R(s_{t},\ a_{t},\ s_{t+1})
$$

有时这个公式会被改成只依赖当前的状态 $r_{t} = R(s_{t})$，或者状态-动作对 $r_{t} = R(s_{t},\ a_{t})$

智能体的目标是最大化行动轨迹的累计奖励，将轨迹 $\tau$ 的累计奖励记作 $R(\tau)$

$T$ 步累计奖励，指的是在一个固定窗口步数 $T$ 内获得的累计奖励：

$$
R(\tau) = \sum_{t=0}^T r_{t}
$$

还有一种叫 $\gamma$ 折扣奖励，指的是智能体获得的奖励会随着获得的时间不同而衰减，用 $\gamma \in(0,1)$ 表示衰减率：

$$
R(\tau) = \sum_{t=0}^\infty \gamma^t \cdot r_t
$$

之所以要加上衰减率，是因为无限多个奖励之和可能**不收敛**，有了衰减率和适当的约束条件，数值才会收敛

为了衡量从某一状态出发可以得到的奖励，引入回报 G，表示在某一状态获得的即时奖励和所有持久奖励等一切奖励的加权和：

$$
\begin{aligned}
G_{t} & = r_{t} + r_{t+1} + r_{t+2} + \dots \\
& = r_{t} + G_{t+1}
\end{aligned}
$$

若采用折扣奖励，则回报 G 表示为：

$$
\begin{aligned}
G_{t} & = r_{t} + \gamma \cdot r_{t+1} + {\gamma}^2 \cdot r_{t+2} + \dots \\
& = r_{t} + \gamma \cdot \left(r_{t+1}+\gamma \cdot r_{t+2} + \dots \right) \\
& = r_{t} + \gamma \cdot G_{t+1}
\end{aligned}
$$

## 强化学习问题

我们假设环境转换和策略都是随机的，这种情况下，$T$ 步行动轨迹是：

$$
\begin{aligned}
P(\tau \mid \pi) & = P(s_0, a_0, s_1, a_1, \dots \mid \pi)\\
& = P(s_0)\cdot P(a_{0}\mid s_{0})\cdot P(s_1\mid s_0,\ a_0) \cdot P(s_{1}, a_{1}, \dots \mid \pi) \\
& = \rho_{0}(s_0)\cdot \pi(a_0\mid s_0)\cdot P(s_1\mid s_0,\ a_0) \cdot P(s_{1}, a_{1}, \dots \mid \pi) \\
& = \rho_{0}(s_0)\cdot \pi(a_0\mid s_0)\cdot P(s_1\mid s_0,\ a_0) \cdot \pi(a_1\mid s_1)\cdot P(s_2\mid s_1,\ a_1) \dots \\
& = \rho_{0}(s_0)\cdot \prod_{t=0}^{T-1}P(s_{t+1}\mid s_{t},\ a_{t}) \cdot \pi(a_t\mid s_t)
\end{aligned}
$$

预期收益是 $J(\pi)$：

$$
J(\pi) = \int_{\tau} P(\tau \mid \pi)\cdot R(\tau) = E_{\tau\sim \pi}[R(\tau)]
$$

即依据策略 $\pi$ 得到的所有可能路径的累计奖励期望

强化学习中的核心优化问题就是找到一个策略使得预期收益最大化，可以表示为：

$$
\pi^* = argmax_{\pi}J(\pi)
$$

$\pi^*$ 是最优策略

## 价值函数

知道一个状态的价值或状态-动作对（state-action pair）的价值很有用。这里的价值指的是，如果你从某一个状态或者状态-动作对开始，一直按照某个策略运行下去最终获得的期望回报

这里介绍四种主要价值函数：

1. 同策略价值函数：$V_{\pi}(s)$ [^1]，从某个状态 s 开始，之后每一步动作都按策略 $\pi$ 执行：
$$
V_{\pi}(s) = E_{\tau\sim \pi}[R(\tau)\mid s_0=s]
$$
2. 同策略动作-价值函数：$Q_{\pi}(s,a)$，从某个状态 s 开始，先随便执行一个动作 a（有可能不是按照策略走的），之后每一步都按照固定的策略执行：
$$
Q_{\pi}(s,a) = E_{\tau \sim \pi}[R(\tau)\mid s_{0} = s,\ a_{0}=a]
$$
3. 最优价值函数：$V_{*}(s)$，从某一个状态 s 开始，之后每一步都按照*最优策略* $\pi$ 执行：
$$
V_{*}(s) = \max_{\pi} E_{\tau \sim \pi}[R(\tau)\mid s_0=s]
$$
4. 最优动作-价值函数：$Q_{*}(s,a)$，从某个状态 s 开始，先随便执行一个动作 a（有可能不是按照策略走的），之后每一步都按照*最优策略*执行 $\pi$：
$$
Q_{*}(s,a) = \max_{\pi}E_{\tau\sim \pi}[R(\tau)\mid s_0=s, \ a_0=a]
$$

> [!important]
> 在讨论价值函数时，如果没有考虑时间参数 t ，那么奖励函数默认为折扣累计奖励。如果要使用普通累计奖励，就需要传入时间参数



[^1]:OPENAI 的文档中，策略 $\pi$ 放在了上标