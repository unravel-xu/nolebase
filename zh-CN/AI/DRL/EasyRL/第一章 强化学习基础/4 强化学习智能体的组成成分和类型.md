对于一个强化学习智能体，它可能有一个或多个如下的组成成分

- 策略（policy）：智能体会用策略来选取下一步的动作
- 价值函数（value function）：用价值函数来对**当前状态**进行评估。价值函数用于评估智能体进入某个状态后，可以对后面的奖励带来多大的影响。价值函数值越大，说明智能体进入这个状态越有利
- 模型（model）：模型表示智能体对环境的状态进行理解，它决定了环境中世界的运行方式

## 策略

策略其实就是一个函数，用于将输入的状态变为动作。分为随机性策略和确定性策略：

- **随机性策略**（stochastic policy）用 $\pi$ 函数表示，即 $\pi(a|s)=p(a_t=a|s_t=s)$，输入一个状态 s，输出一个概率，这个概率是智能体**所有动作的概率**。或用 $a_t \sim \pi_{\theta}(\cdot|s_t)$ 表示，其中 $\theta$ 是策略的参数
- **确定性策略**（deterministic policy）就是智能体直接采取最有可能的动作，即 $a^* = argmax_a\pi(a|s)$ 或用 $a_t = \mu_{\theta}(s_t)$ 表示

因为策略本质上就是智能体的“大脑”，所以很多时候“策略”和“智能体”这两个名词经常互换，例如我们会说：“策略的目的是最大化奖励”

通常情况下，强化学习一般使用随机性策略，随机性策略有很多优点。比如，在学习时可以通过引入一定的随机性来更好地探索环境

## 轨迹

轨迹（trajectory）$\tau$ 指的是状态和动作的序列：

$$
\tau = (s_0, a_0, s_1, a_1, \dots)
$$

第一个状态 $s_0$，是从**开始状态分布**中随机采样的，有时候表示为 $\rho_{0}(\cdot)$：

$$
s_{0} \sim \rho_{0}(\cdot)
$$

轨迹常常也被称为**回合**（episodes）或者 **rollouts**

## 价值函数

知道一个状态的价值或状态-动作对（state-action pair）的价值很有用。这里的价值指的是，如果你从某一个状态或者状态-动作对开始，一直按照某个策略运行下去最终获得的期望回报

这里介绍四种主要价值函数：

1. 同策略价值函数：$V_{\pi}(s)$ [^1]，从某个状态 s 开始，之后每一步动作都按策略 $\pi$ 执行：
$$
V_{\pi}(s) = E_{\tau\sim \pi}[R(\tau)|s_0=s]
$$
1. 同策略动作-价值函数：$Q_{\pi}(s,a)$，从某个状态 s 开始，先随便执行一个动作 a（有可能不是按照策略走的），之后每一步都按照固定的策略执行：
$$
Q_{\pi}(s,a) = E_{\tau \sim \pi}
$$


[^1]:OPENAI 的文档中，策略 $\pi$ 放在了上标