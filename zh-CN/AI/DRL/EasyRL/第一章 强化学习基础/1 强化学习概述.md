
**强化学习**（reinforcement learning，RL）讨论的问题是智能体（agent）怎么在复杂、不确定的环境（environment）中最大化它能获得的奖励

强化学习由两部分组成：智能体和环境

![[pic-20250307104814283.png|图1.1 强化学习示意]]

定义：

- 智能体：agent
- t 时刻动作：$Action_{t}$ 简记为 $A_{t}$
- 决策：智能体在环境中获取某个状态后，根据该状态输出一个动作

## 强化学习与监督学习

监督学习（supervised learning）过程中，有两个假设：

1. 输入的数据（标注的数据）都应该是没有关联的（i.i.d.）
2. 需要告诉学习器（learner）正确的标签是什么，这样它可以通过正确的标签来修正自己的预测

> 通常假设样本空间中全体样本服从一个未知分布，我们获得的每个样本都是独立地从这个分布上采样获得的，即独立同分布(independent and identically distributed，简称 i.i.d.)

而在强化学习中，上述两个假设都无法保证
- 对于假设 1，例如训练智能体玩游戏，上下帧之间有非常强的连续性，得到的数据是相关的时间序列数据，不满足独立同分布
- 对于假设 2，我们不知道当前状态下哪个动作是正确动作

强化学习和监督学习的区别如下：

1. 强化学习输入的样本是序列数据，而不像监督学习里面样本都是独立的
2. 学习器并没有告诉我们每一步正确的动作应该是什么，学习器需要自己去发现哪些动作可以带来 最多的奖励，只能通过不停地尝试来发现最有利的动作
3. 智能体获得自己能力的过程，其实是不断地试错探索（trial-and-error exploration）的过程。探索 （exploration）和利用（exploitation）是强化学习里面非常核心的问题。其中，探索指尝试一些新的动作， 这些新的动作有可能会使我们得到更多的奖励，也有可能使我们“一无所有”；利用指采取已知的可以获得最多奖励的动作，重复执行这个动作，因为我们知道这样做可以获得一定的奖励。因此，我们需要在探索和利用之间进行权衡，这也是在监督学习里面没有的情况
4. 在强化学习过程中，没有监督者（supervisor），只有**奖励信号**（reward signal），并且奖励信号是**延迟的**，即环境会在很久以后告诉我们之前我们采取的动作到底是不是有效的。因为我们没有得 到即时反馈，所以智能体使用强化学习来学习就非常困难。当我们采取一个动作后，如果我们使用监督学习，我们就可以立刻获得一个指导，比如，我们现在采取了一个错误的动作，正确的动作应该是什么。而在强化学习里面，环境可能会告诉我们这个动作是错误的，但是它并没有告诉我们正确的动作是什么。而且更困难的是，它可能是在一两分钟过后告诉我们这个动作是错误的。所以这也是强化学习和监督学习不同的地方

## 强化学习的例子

非常重要的一个原因就是强化学习得到的模型可以有超人类的表现。监督学习算法的上限（upper bound）就是人类的表现，标注结果决定了它的表现永远不可能超越人类。但是对于强化学习，它在环境里面自己探索，有非常大的潜力，它可以获得超越人类的能力的表现！

称下图是**预演**（rollout）的一个过程。预演是指我们从当前帧对动作进行采样，生成很多局游戏。我们将当前的智能体与环境交互，会得到一系列观测。每一个观测可看成一个轨迹（trajectory）。 轨迹就是当前帧以及它采取的策略，即状态和动作的序列：

$$
\tau = (s_0, a_0, s_1, a_1, \dots)
$$

![[pic-20250307164307116.png]]

一场游戏称为一个回合（episode）或者试验（trial）

## 强化学习的历史

早期的强化学习，我们称其为标准强化学习。现在业界把强化学习与深度学习结合起来，就形成了深度强化学习（deep reinforcement learning），因此，深度强化学习 = 深度学习 + 强化学习

- 标准强化学习：比如 TD-Gammon 玩 Backgammon 游戏的过程，其实就是**设计特征**，然后训练价值函数的过程，如图所示，标准强化学习先设计很多特征，这些特征可以描述现在整个状态。 得到这些特征后，我们就可以通过训练一个分类网络或者分别训练一个价值估计函数来采取动作

![[pic-20250307164808723.png]]

- 深度强化学习：自从我们有了深度学习，有了神经网络，就可以把智能体玩游戏的过程改进成一个端到端训练（end-to-end training）的过程，如图所示。我们不需要设计特征，直接输入状态就可以输出动作。我们**可以用一个神经网络来拟合价值函数或策略网络**，省去特征工程（feature engineering）的过程

![[pic-20250307164848979.png]]

## 强化学习应用

（1）各类机器人运动，例如学习走路，学习操作

（2）LLM，特别是最近很火的 deepseek