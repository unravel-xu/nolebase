
偶然间看到 CSDN 很棒的一篇博客：[强化学习极简入门：通俗理解MDP、DP MC TD和Q学习、策略梯度、PPO](https://blog.csdn.net/v_JULY_v/article/details/128965854)

如下做转载以及一点自己的修改

## 基本概念

![[pic-20250228234127151.png]]

强化学习(Reinforcement Learning，简称RL)，是指基于智能体在复杂、不确定的环境中最大化它能获得的奖励，从而达到自主决策的目的

- Agent，一般译为智能体，就是我们要训练的模型，类似玩超级玛丽的时候操纵马里奥做出相应的动作，而这个马里奥就是Agent
- action(简记为 a)，玩超级玛丽的时候你会控制马里奥做三个动作，即向左走、向右走和向上跳，而马里奥做的这三个动作就是action
- Environment，即环境，它是提供reward的某个对象，它可以是AlphaGo中的人类棋手，也可以是自动驾驶中的人类驾驶员，甚至可以是某些游戏AI里的游戏规则

RL 为得到最优策略从而获取最大化奖励，有两种方法：

- 基于价值（Value-Based）
- 基于策略（Policy-Based）

它们的核心差异在于**优化目标**和**动作选择方式**

## 序列决策

### 奖励

奖励是由环境给的一种**标量**的反馈信号（scalar feedback signal），这种信号可显示智能体在**某一步**采取某个策略的表现如何。强化学习的目的就是最大化智能体可以获得的奖励，智能体在环境里面存在的目的就是最大化它期望的**累积奖励（expected cumulative reward）**。不同的环境中，奖励也是不同的：

1. 比如一个象棋选手，他的目的是赢棋，在最后棋局结束的时候，他就会得到一个正奖励（赢）或者负奖励（输）
2. 在股票管理里面，奖励由股票获取的奖励与损失决定
3. 在玩雅达利游戏的时候，奖励就是增加或减少的游戏的分数，奖励本身的稀疏程度决定了游戏的难度

### 序列决策

在一个强化学习环境里面，智能体的目的就是选取一系列的动作来最大化奖励，所以这些选取的动作 必须有长期的影响。但在这个过程里面，智能体的奖励其实是被延迟了的，就是我们现在选取的某一步动作，**可能要等到很久后**才知道这一步到底产生了什么样的影响。强化学习里面一个重要的课题就是近期奖励和远期奖励的权衡 （trade-off），研究怎么让智能体取得更多的远期奖励。在与环境的交互过程中，智能体会获得很多观测。针对每一个观测，智能体会采取一个动作，也会得到一个奖励。所以历史是**观测、动作、奖励**的序列：

$$
H_t = (o_1, a_1, r_1), \dots, (o_t, a_t, r_t)
$$

智能体在采取当前动作的时候会依赖于它之前得到的历史，所以我们可以把整个游戏的状态看成关于这个历史的函数：

$$
S_t = f(H_t)
$$

![[pic-20250301125636437.png]]

### 状态和观测

- 状态：是对世界的完整描述，不会隐藏世界的信息

- 环境状态：整个环境的完整描述，t 时刻的环境状态表示为：$s_{t}^e$

- 智能体状态：智能体的完整描述（例如机器人此时各关节角度等），t 时刻的智能体状态表示为：$s_{t}^a$

- 观测：是对状态的部分描述，可能遗漏一些信息，t 时刻的观测表示为：$o_{t}$

显然智能体可以观测自身的全部状态

当智能体的状态和环境的状态等价时，即当智能体能够观察到环境的所有状态时，称这个环境是**完全可观测的（fully observed）**，这时，强化学习通常被建模成一个马尔可夫决策过程（Markov decision process，MDP）问题。在 MDP 中，$o_{t}=s_{t}^e=s_{t}^a$

当智能体无法观测到环境的所有状态时，例如智能体玩牌类游戏，它只能知道自己的手牌而无法知道其他人的手牌，称这个环境是部分可观测的（partially observed）。这时，强化学习通常被建模成**部分可观测的马尔可夫决策过程（partially observable Markov decision process, POMDP）**。POMDP 是 MDP 的一种泛化。POMDP 依然具有马尔可夫性质，但是假设智能体无法感知环境的状态，只能知道部分观测值。比如在自动驾驶中，智能体只能感知传感器采集的有限的环境信息。部分可观测马尔可夫决策过程可以用一个七元组描述：$(S,A,T,R,\Omega,O,\gamma)$。其中：
- S：状态空间，是隐变量
- A：动作空间
- T：$T(s'|s,a)$ 状态转移概率
- R：奖励函数
- $\Omega$：$\Omega(o|s,a)$ 观测概率
- O：观测空间
- $\gamma$：折扣系数

## 动作空间

不同环境允许不同种类的动作，在给定的环境中，有效动作的集合经常被称为动作空间（action space）例如，走迷宫机器人如果只有往东、往南、往西、往北这 4 种移动方式，则其动作空间为离散动作空 间；如果机器人可以向 360 度中的任意角度进行移动，则其动作空间为连续动作空间。在连续动作空间中，动作是实值的向量

## 强化学习智能体的组成成分和类型

对于一个强化学习智能体，它可能有一个或多个如下的组成成分：

- 策略（policy）：智能体会用策略来选取下一步的动作。

- 价值函数（value function）。我们用价值函数来对当前状态进行评估。**价值函数用于评估智能体进 入某个状态后，可以对后面的奖励带来多大的影响**。价值函数值越大，说明智能体进入这个状态越有利

- 模型（model）。模型表示智能体对环境的状态进行理解，它决定了环境中世界的运行方式

### 策略

策略是智能体的动作模型，它**决定了智能体的动作**。它其实是一个函数，用于**把输入的状态变成动作**。策略可分为两种：随机性策略和确定性策略

随机性策略（stochastic policy）就是 $\pi$ 函数，即 $\pi(a|s)=p(a_t=a|s_t=s)$ 。输入一个状态 s，输出一个智能体采取某个动作 a 的概率

确定性策略（deterministic policy）就是智能体直接采取最有可能的动作，即 $a^*=\underset{a}{\arg \max } \pi(a \mid s)$

### 价值函数

奖励函数，某个状态 s 的奖励 R(s)，是指转移到该状态 s 时可以获得奖励的期望，有 $R(s)=E[R_{t+1}|S_{t}=s]$，**注意，有的书上奖励函数和下面回报公式中的 $R_{t+1}$ 的下标 t+1 写为 t，其实严格来说，先有 t 时刻的状态/动作之后才有 t+1 时刻的奖励，但应用中两种下标法又都存在，读者注意辨别**

回报 G：当前状态获得的奖励以及**未来**所有获得奖励的加权和，所以 t 时刻获得回报为 $G_t=R_{t+1}+ R_{t+2} + R_{t+3}+\dots = \sum_{k=1}^{+\infty}R_{t+k}$

价值函数的值是对**未来奖励**的预测，我们用它来评估状态的好坏。如果我们希望智能体更加关注短期内的回报，同时对未来的收益进行一定的考虑，可以使用累积折扣奖励（Discounted Cumulative Reward），它引入了折扣因子 $\gamma$，则 t 时刻获得回报为 $G_t = \sum_{k=1}^{+\infty}\gamma^{k-1} R_{t+k}$ ，每个未来奖励都根据 $\gamma$ 进行折扣，$\gamma$ 越接近 1，未来的奖励影响越大。一个状态的**期望回报**就称为这个状态的价值，用公式 $V_{\pi}(s)=\mathbb{E}_{\pi}[G_t|S_t=s]$ 表示，其中下标 $\pi$ 表示使用策略 $\pi$，调整不同的策略，就有不同的公式

如下，我们对价值函数进行变形：

首先累计折扣奖励可以化为递推形式：

$$
\begin{aligned}
G_t & =R_{t+1}+\gamma \cdot R_{t+2}+\gamma^2 \cdot R_{t+3}+\gamma^3 \cdot R_{t+4}+\cdots \\
& =R_{t+1}+\gamma\left(R_{t+2}+\gamma \cdot R_{t+3}+\gamma^2 \cdot R_{t+4}+\cdots\right) \\
& =R_{t+1}+\gamma G_{t+1}
\end{aligned}
$$

带入价值函数得：

$$
\begin{aligned}V_{\pi}(s) & =\mathbb{E}_{\pi}[G_t|S_t=s] \\ 
& = \mathbb{E}_{\pi}[R_{t+1}+\gamma G_{t+1}|S_t = s] \\
& = \mathbb{E}_{\pi}[R_{t+1}|S_{t}=s] + \gamma\mathbb{E}_{\pi}[G_{t+1}|S_t=s]
\end{aligned}
$$

继续展开：

$$
\begin{aligned}
\mathbb{E}_{\pi}\left[G_{t+1} \mid S_t=s\right] & =\sum G_{t+1} \cdot P\left\{G_{t+1} \mid S_t=s\right\} \\
& =\sum G_{t+1} \cdot (\sum_{s^{\prime}} P\left\{G_{t+1} \mid S_{t+1}=s^{\prime}, S_t=s\right\} \cdot P\left\{S_{t+1}=s^{\prime} \mid S_t=s\right\}) \\
& =\sum_{s^{\prime}} [(\sum G_{t+1} \cdot P\left\{G_{t+1} \mid S_{t+1}=s^{\prime}, S_t=s\right\})\cdot P\left\{S_{t+1}=s^{\prime} \mid S_t=s\right\}] \\
& =\sum_{s^{\prime}} \mathbb{E}_{\pi}\left[G_{t+1} \mid S_{t+1}=s^{\prime}, S_t=s\right] \cdot P\left\{S_{t+1}=s^{\prime} \mid S_t=s\right\} \\
& \xlongequal{\text{马尔可夫性质，见下一节}} \sum_{s^{\prime}} \mathbb{E}_{\pi}\left[G_{t+1} \mid S_{t+1}=s^{\prime}\right] \cdot P\left\{S_{t+1}=s^{\prime} \mid S_t=s\right\} \\
& =\sum_{s^{\prime}} V_{\pi}\left(S_{t+1}\right) \cdot P\left\{S_{t+1}=s^{\prime} \mid S_t=s\right\} \\
& =\mathbb{E}_{\pi}\left[V_{\pi}\left(S_{t+1}\right) \mid S_t=s\right]
\end{aligned}
$$

所以有：

$$
\begin{aligned}
V_{\pi}(s) & = \mathbb{E}_{\pi}[R_{t+1}|S_{t}=s] + \gamma\mathbb{E}_{\pi}[G_{t+1}|S_t=s] \\
& = R(s) + \gamma \mathbb{E}_{\pi}\left[V_{\pi}\left(S_{t+1}\right) \mid S_t=s\right] \\
& = R(s) + \gamma \sum_{s'\in S}P(s'|s) \cdot V(s')
\end{aligned}
$$

该公式也称**贝尔曼方程(bellman equation)**

$R(s)$：表示 t 时刻处于 s 状态得到的即时奖励

$\sum_{s'\in S}P(s'|s) \cdot V(s')$：表示从 t 时刻状态 s 转移到 t+1 时刻所有可能的状态 s' 的价值的期望

### 模型

模型决定了下一步的状态。下一步的状态取决于当前的状态以及当前采取的动作。它由状态转移概率和奖励函数两个部分组成  

## 马尔可夫决策过程

生活中有一类现象是确定性的现象，比如红绿灯系统，红灯之后一定是红黄、接着绿灯、黄灯，最后又红灯，每一个状态之间的变化是确定的

但还有一类现象是不确定的，比如今天是晴天，谁也没法百分百确定明天一定是晴天还是雨天、阴天

对于这种假设具有 M 个状态的模型
- 共有 $M^2$ 个状态转移，因为任何两个状态之间都可能转移（今天晴天，明天各种天气都有可能）
- 每一个状态转移都有一个概率值，称为状态转移概率，相当于从一个状态转移到另一个状态的概率
- 所有的 $M^2$ 个概率可以用一个**状态转移矩阵**表示

随机过程的研究对象是随时间演变的随机现象(比如天气随时间的变化)：

- 随机现象在某时刻 t 的取值是一个向量随机变量，用 $S_t$ 表示
- 在时刻 t 的状态 $S_t$ 通常取决于 t 时刻前的状态，在已知历史信息 $(S_1,\dots,S_t)$ 时，下一个时刻的状态 $S_{t+1}$ 的概率表示成：$P(S_{t+1}|S_1,\cdots,S_t)$，由此，可以定义包含所有状态转移对之间的转移概率矩阵：

$$
P=\left[\begin{array}{c}
P\left(s_1 \mid s_1\right) P\left(s_2 \mid s_1\right) P\left(s_3 \mid s_1\right) \cdots P\left(s_n \mid s_1\right) \\
P\left(s_1 \mid s_2\right) P\left(s_2 \mid s_2\right) P\left(s_3 \mid s_2\right) \cdots P\left(s_n \mid s_2\right) \\
\cdots \cdots \cdots \\
\cdots \cdots \cdots \\
P\left(s_1 \mid s_n\right) P\left(s_2 \mid s_n\right) P\left(s_3 \mid s_n\right) \cdots P\left(s_n \mid s_n\right)
\end{array}\right]
$$

- 当且仅当某时刻的状态**只取决于上一时刻**的状态时，一个随机过程被称为具有马尔可夫性质，也就是有：$P(S_{t+1}|S_{t})=P(S_{t+1}|S_{1},\dots,S_{t})$，但注意，虽说当前状态只看上一个状态，但上一个状态其实包含了更上一个状态的信息，所以不能说当下与历史是无关的
- 而具有马尔可夫性质的随机过程便是马尔可夫过程

在马尔可夫过程的基础上加入奖励函数和折扣因子，就可以得到马尔可夫奖励过程(Markov reward process，MRP)，如果再次增加一个来自外界的刺激比如智能体的动作，就得到了马尔可夫决策过程(MDP)。MDP 中，$S_t$（t 时刻状态的集合）和 $R_t$（t 时刻奖励的集合）的每个可能的值出现的概率只取决于前一个状态 $S_{t-1}$ 和前一个动作 $A_{t-1}$，并和更早前的状态及动作完全无关

给定当前状态 $S_{t}=s$ 以及采取的动作 $A_t=a$，则下一个状态 $S_{t+1}$ 出现的概率可由状态转移概率矩阵表示如下：

$$
P_{ss'}^a = P(S_{t+1}=s' \mid S_{t} = s, A_{t} = a) = P(s'\mid s, a)
$$

假定在当前状态和当前动作确定后，其对应的奖励则设为 $R_{t+1}=r$，则在状态 s 下采取动作 a 后，转移到下一个状态 s' 并获得奖励 r 的概率，表示为：

$$p(s', r|s,a)=P(S_{t+1}=s', R_{t+1}=r|S_t=s,A_t = a)$$

引入动作 a 后奖励函数变为：

$$
\begin{aligned}
R(s,a) & = E[R_{t+1}|S_t=s,A_t=a] \\
& = \sum_{r \in R} r \cdot p(r|S_t=s,A_t=a) \\
& = \sum_{r\in R} r \cdot \sum_{s'\in S} p(s',r|s,a)
\end{aligned}
$$

这里对所有可能的奖励 r 求和(不少情况下，即使状态转移和动作是已知的，奖励 r 仍然可能是随机的，比如买股票，股票价格随机波动，导致购买之后的盈亏也具有随机性)

如果奖励 r 是确定的，则可以简化公式，去掉对 r 的求和：$R(s,a)=\sum_{s' \in S} p(s',r|s,a) \cdot r$

我们将动作引入价值函数，首先通过“状态价值函数”对当前状态进行评估：

$$
V_{\pi}(s) = E_{\pi}[G_{t}\mid S_{t}=s]
$$

相当于从状态 s 出发遵循策略 $\pi$ 能获得的期望回报

其次，通过“动作价值函数”对动作进行评估：

$$
Q_\pi(s, a) =E_\pi\left[G_t \mid S_t=s, A_t=a\right]
$$

相当于对当前状态 s 依据策略​ $\pi$ 执行动作​ a 得到的期望回报，这就是著名的 Q 函数，未来可以获得奖励的期望取决于当前的状态和当前的动作。Q 函数是强化学习算法里要学习的一个函数。因为当我们得到 Q 函数后，进入某个状态要采取的最优动作可以通过 Q 函数得到

![[pic-20250301230102904.png]]

当有了策略、价值函数和模型三个组成部分后，就形成了一个 MDP，下图决策过程可视化了状态之间的转移以及采取的动作

![[pic-20250301230610139.png]]

通过状态转移概率分布，我们可以揭示状态价值函数和动作价值函数之间的联系

- 在使用策略 $\pi$ 时，状态 s 的价值等于在该状态下基于策略 $\pi$ 采取所有动作的概率与相应的价值相乘再求和：

$$
\begin{aligned}
V_{\pi}(s) & = E_{\pi}[G_{t}|S_{t}=s] \\
& = \sum_{a \in A} \pi(a|s)\cdot E_{\pi}[G_t|S_t=s, A_t=a] \\
& = \sum_{a\in A} \pi(a|s)\cdot Q_{\pi}(s,a)
\end{aligned}
$$

- 在使用策略 $\pi$ 时，在状态 s 下采取动作 a 的价值等于当前奖励 $R(s,a)$，加上经过衰减的所有可能的下一个状态的状态转移概率与相应的价值的乘积