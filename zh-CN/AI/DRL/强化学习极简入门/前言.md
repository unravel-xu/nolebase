
偶然间看到 CSDN 很棒的一篇博客：[强化学习极简入门：通俗理解MDP、DP MC TD和Q学习、策略梯度、PPO](https://blog.csdn.net/v_JULY_v/article/details/128965854)

如下做转载以及一点自己的修改

## 基本概念

![[pic-20250228234127151.png]]

强化学习(Reinforcement Learning，简称RL)，是指基于智能体在复杂、不确定的环境中最大化它能获得的奖励，从而达到自主决策的目的

- Agent，一般译为智能体，就是我们要训练的模型，类似玩超级玛丽的时候操纵马里奥做出相应的动作，而这个马里奥就是Agent
- action(简记为 a)，玩超级玛丽的时候你会控制马里奥做三个动作，即向左走、向右走和向上跳，而马里奥做的这三个动作就是action
- Environment，即环境，它是提供reward的某个对象，它可以是AlphaGo中的人类棋手，也可以是自动驾驶中的人类驾驶员，甚至可以是某些游戏AI里的游戏规则
- reward(简记为 r)，这个奖赏可以类比为在明确目标的情况下，接近目标意味着做得好则奖，远离目标意味着做的不好则惩，最终达到收益/奖励最大化，且这个奖励是强化学习的核心
- State(简记为 s)，可以理解成环境的状态，简称状态

RL 为得到最优策略从而获取最大化奖励，有两种方法：

- 基于价值（Value-Based）
- 基于策略（Policy-Based）

它们的核心差异在于**优化目标**和**动作选择方式**

## 马尔可夫决策过程

生活中有一类现象是确定性的现象，比如红绿灯系统，红灯之后一定是红黄、接着绿灯、黄灯，最后又红灯，每一个状态之间的变化是确定的

但还有一类现象是不确定的，比如今天是晴天，谁也没法百分百确定明天一定是晴天还是雨天、阴天

对于这种假设具有 M 个状态的模型
1. 共有 $M^2$ 个状态转移，因为任何两个状态之间都可能转移（今天晴天，明天各种天气都有可能）
2. 每一个状态转移都有一个概率值，称为状态转移概率，相当于从一个状态转移到另一个状态的概率
3. 所有的 $M^2$ 个概率可以用一个**状态转移矩阵**表示

随机过程的研究对象是随时间演变的随机现象(比如天气随时间的变化)：

- 随机现象在某时刻 t 的取值是一个向量随机变量，用 $S_t$ 表示
- 在时刻 t 的状态 $S_t$ 通常取决于 t 时刻前的状态，在已知历史信息 $(S_1,\dots,S_t)$ 时，下一个时刻的状态 $S_{t+1}$ 的概率表示成：$P(S_{t+1}|S_1,\cdots,S_t)$，由此，可以定义包含所有状态转移对之间的转移概率矩阵：

$$
P=\left[\begin{array}{c}
P\left(s_1 \mid s_1\right) P\left(s_2 \mid s_1\right) P\left(s_3 \mid s_1\right) \cdots P\left(s_n \mid s_1\right) \\
P\left(s_1 \mid s_2\right) P\left(s_2 \mid s_2\right) P\left(s_3 \mid s_2\right) \cdots P\left(s_n \mid s_2\right) \\
\cdots \cdots \cdots \\
\cdots \cdots \cdots \\
P\left(s_1 \mid s_n\right) P\left(s_2 \mid s_n\right) P\left(s_3 \mid s_n\right) \cdots P\left(s_n \mid s_n\right)
\end{array}\right]
$$

- 当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有马尔可夫性质，也就是有：$P(S_{t+1}|S_{t})=P(S_{t+1}|S_{1},\dots,S_{t})$，但注意，虽说当前状态只看上一个状态，但上一个状态其实包含了更上一个状态的信息，所以不能说当下与历史是无关的
- 而具有马尔可夫性质的随机过程便是马尔可夫过程

在马尔可夫过程的基础上加入奖励函数和折扣因子，就可以得到马尔可夫奖励过程(Markov reward process，MRP)。其中：

- 奖励函数，某个状态 s 的奖励 R(s)，是指转移到该状态 s 时可以获得奖励的期望，有 $R(s)=E[R_{t+1}|S_{t}=s]$
注意，有的书上奖励函数和下面回报公式中的的下标写为，其实严格来说，先有时刻的状态/动作之后才有时刻的奖励，但应用中两种下标法又都存在，读者注意辨别