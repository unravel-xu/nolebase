
偶然间看到 CSDN 很棒的一篇博客：[强化学习极简入门：通俗理解MDP、DP MC TD和Q学习、策略梯度、PPO](https://blog.csdn.net/v_JULY_v/article/details/128965854)

如下做转载以及一点自己的修改

## 基本概念

![[pic-20250228234127151.png]]

强化学习(Reinforcement Learning，简称RL)，是指基于智能体在复杂、不确定的环境中最大化它能获得的奖励，从而达到自主决策的目的

- Agent，一般译为智能体，就是我们要训练的模型，类似玩超级玛丽的时候操纵马里奥做出相应的动作，而这个马里奥就是Agent
- action(简记为 a)，玩超级玛丽的时候你会控制马里奥做三个动作，即向左走、向右走和向上跳，而马里奥做的这三个动作就是action
- Environment，即环境，它是提供reward的某个对象，它可以是AlphaGo中的人类棋手，也可以是自动驾驶中的人类驾驶员，甚至可以是某些游戏AI里的游戏规则
- reward(简记为 r)，这个奖赏可以类比为在明确目标的情况下，接近目标意味着做得好则奖，远离目标意味着做的不好则惩，最终达到收益/奖励最大化，且这个奖励是强化学习的核心
- State(简记为 s)，可以理解成环境的状态，简称状态

RL 为得到最优策略从而获取最大化奖励，有两种方法：

- 基于价值（Value-Based）
- 基于策略（Policy-Based）

它们的核心差异在于**优化目标**和**动作选择方式**

## 序列决策

### 奖励

奖励是由环境给的一种**标量**的反馈信号（scalar feedback signal），这种信号可显示智能体在**某一步**采取某个策略的表现如何。强化学习的目的就是最大化智能体可以获得的奖励，智能体在环境里面存在的目的就是最大化它期望的**累积奖励（expected cumulative reward）**。不同的环境中，奖励也是不同的：

1. 比如一个象棋选手，他的目的是赢棋，在最后棋局结束的时候，他就会得到一个正奖励（赢）或者负奖励（输）
2. 在股票管理里面，奖励由股票获取的奖励与损失决定
3. 在玩雅达利游戏的时候，奖励就是增加或减少的游戏的分数，奖励本身的稀疏程度决定了游戏的难度

### 序列决策

在一个强化学习环境里面，智能体的目的就是选取一系列的动作来最大化奖励，所以这些选取的动作 必须有长期的影响。但在这个过程里面，智能体的奖励其实是被延迟了的，就是我们现在选取的某一步动作，**可能要等到很久后**才知道这一步到底产生了什么样的影响。强化学习里面一个重要的课题就是近期奖励和远期奖励的权衡 （trade-off），研究怎么让智能体取得更多的远期奖励。在与环境的交互过程中，智能体会获得很多观测。针对每一个观测，智能体会采取一个动作，也会得到一个奖励。所以历史是**观测、动作、奖励**的序列：

$$
H_t = (o_1, a_1, r_1), \dots, (o_t, a_t, r_t)
$$

智能体在采取当前动作的时候会依赖于它之前得到的历史，所以我们可以把整个游戏的状态看成关于这个历史的函数：

$$
S_t = f(H_t)
$$

![[pic-20250301125636437.png]]



## 马尔可夫决策过程

生活中有一类现象是确定性的现象，比如红绿灯系统，红灯之后一定是红黄、接着绿灯、黄灯，最后又红灯，每一个状态之间的变化是确定的

但还有一类现象是不确定的，比如今天是晴天，谁也没法百分百确定明天一定是晴天还是雨天、阴天

对于这种假设具有 M 个状态的模型
4. 共有 $M^2$ 个状态转移，因为任何两个状态之间都可能转移（今天晴天，明天各种天气都有可能）
5. 每一个状态转移都有一个概率值，称为状态转移概率，相当于从一个状态转移到另一个状态的概率
6. 所有的 $M^2$ 个概率可以用一个**状态转移矩阵**表示

随机过程的研究对象是随时间演变的随机现象(比如天气随时间的变化)：

- 随机现象在某时刻 t 的取值是一个向量随机变量，用 $S_t$ 表示
- 在时刻 t 的状态 $S_t$ 通常取决于 t 时刻前的状态，在已知历史信息 $(S_1,\dots,S_t)$ 时，下一个时刻的状态 $S_{t+1}$ 的概率表示成：$P(S_{t+1}|S_1,\cdots,S_t)$，由此，可以定义包含所有状态转移对之间的转移概率矩阵：

$$
P=\left[\begin{array}{c}
P\left(s_1 \mid s_1\right) P\left(s_2 \mid s_1\right) P\left(s_3 \mid s_1\right) \cdots P\left(s_n \mid s_1\right) \\
P\left(s_1 \mid s_2\right) P\left(s_2 \mid s_2\right) P\left(s_3 \mid s_2\right) \cdots P\left(s_n \mid s_2\right) \\
\cdots \cdots \cdots \\
\cdots \cdots \cdots \\
P\left(s_1 \mid s_n\right) P\left(s_2 \mid s_n\right) P\left(s_3 \mid s_n\right) \cdots P\left(s_n \mid s_n\right)
\end{array}\right]
$$

- 当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有马尔可夫性质，也就是有：$P(S_{t+1}|S_{t})=P(S_{t+1}|S_{1},\dots,S_{t})$，但注意，虽说当前状态只看上一个状态，但上一个状态其实包含了更上一个状态的信息，所以不能说当下与历史是无关的
- 而具有马尔可夫性质的随机过程便是马尔可夫过程

在马尔可夫过程的基础上加入奖励函数和折扣因子，就可以得到马尔可夫奖励过程(Markov reward process，MRP)。其中：

- 奖励函数，某个状态 s 的奖励 R(s)，是指转移到该状态 s 时可以获得奖励的期望，有 $R(s)=E[R_{t+1}|S_{t}=s]$，==注意，有的书上奖励函数和下面回报公式中的 $R_{t+1}$ 的下标 t+1 写为 t，其实严格来说，先有 t 时刻的状态/动作之后才有 t+1 时刻的奖励，但应用中两种下标法又都存在，读者注意辨别==
- 此外，实际中，因为一个状态可以得到的奖励是持久的，所有奖励的衰减之和称为回报，可用 G 表示当下即时奖励和所有持久奖励等一切奖励的加权和(考虑到一般越往后某个状态给的回报率越低，也即奖励因子或折扣因子越小，用 $\gamma$
- 表示)，从而有
