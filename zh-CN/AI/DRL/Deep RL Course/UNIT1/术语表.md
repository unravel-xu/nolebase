# 术语表

这份术语表由社区创建

## 智能体（Agent）

智能体与环境交互，并通过反馈的奖励和惩罚来学习如何做决策

## 环境（Environment）

环境是一个模拟的世界，智能体可以通过与之交互来进行学习

## 马尔可夫性质（Markov Property）

智能体采取的动作仅依赖于当前状态，而不受过去状态或动作的影响

## 观察/状态（Observations/State）

- **状态（State）**：对世界状态的完整描述
- **观察（Observation）**：对环境/世界状态的部分描述

## 动作（Actions）

- **离散动作（Discrete Actions）**：有限数量的动作选择，例如，上、下、左、右
- **连续动作（Continuous Actions）**：无限种可能的动作。例如，在自动驾驶汽车的情景中，存在无限多的动作可能性

## 奖励与折扣（Rewards and Discounting）

- **奖励（Rewards）**：告诉智能体其所采取的动作是否正确
- 强化学习算法专注于最大化累积奖励
- **奖励假设（Reward Hypothesis）**：所有目标都可以被表述为累积回报的最大化
- 折扣处理是因为早期获得的奖励更可能发生，因为它们比长期奖励更容易预测

## 任务类型（Tasks）

- **情节性任务（Episodic）**：有明确开始和结束的任务
- **持续性任务（Continuous）**：有开始但无明确结束的任务

## 探索与利用权衡（Exploration v/s Exploitation Trade-Off）

- **探索（Exploration）**：尝试用随机动作探索环境，并从环境中接收反馈/回报/奖励
- **利用（Exploitation）**：指的是利用已知的环境信息获取最大奖励
- **探索与利用权衡（Exploration-Exploitation Trade-Off）**：平衡我们想要探索新环境的程度以及想要利用已有信息获取奖励的程度

## 策略（Policy）

- **策略（Policy）**：称为智能体的大脑。它告诉我们面对特定状态时应采取什么行动
- **最优策略（Optimal Policy）**：当智能体按照该策略行动时能够最大化预期回报，通过训练得到

## 方法

- **基于策略的方法（Policy-based Methods）**：一种解决 RL 问题的方法，直接学习策略，将每个状态映射到该状态下的最佳对应动作或一组可能动作的概率分布
- **基于价值的方法（Value-based Methods）**：另一种解决 RL 问题的方法，训练一个价值函数，将每个状态映射到在该状态下的预期值，而不是直接训练策略