# è®­ç»ƒç¬¬ä¸€ä¸ª DRL Agent

![[lunarLander.gif]]

åœ¨æœ¬å°èŠ‚ä¸­ï¼Œå°†å­¦ä¼šè®­ç»ƒä¸€ä¸ªæœˆçƒç€é™†å™¨æ™ºèƒ½ä½“ï¼Œä½¿å…¶ä»¥æ­£ç¡®çš„å§¿æ€ç€é™†æœˆçƒ

å¦‚æœæƒ³è¦è·å–è¯ä¹¦ï¼Œéœ€è¦å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¸Šä¼ åˆ° Hugging Face Hub ä¸”åˆ†æ•° $\geq$ 200

å¯ä»¥åœ¨ [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) æ‰¾åˆ°ç»“æœ

åœ¨ [Check-my-progress-Deep-RL-Course](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course) å¯ä»¥æŸ¥çœ‹è¿™é—¨è¯¾çš„å®éªŒè¿›åº¦

ä¸‹é¢é€šè¿‡ Colab å¼€å§‹å®éªŒï¼š<a href="https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit1/unit1.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>

åœ¨å®‰è£…å®Œä¸€ç³»åˆ—ä¾èµ–å

## Gymnasium åº“çš„ä½¿ç”¨

Gymnasium åº“åŠŸèƒ½ï¼š
- æä¾›åˆ›å»º RL ç¯å¢ƒçš„æ¥å£
- åŒ…å«ä¸€ç³»åˆ—çš„ RL ç¯å¢ƒ

![[pic-20250311165734466.png]]

å›é¡¾ä¸€ä¸ª RL å¾ªç¯çš„æ­¥éª¤ï¼š

1. agent ä»ç¯å¢ƒæ¥æ”¶ stateï¼ˆ$S_{0}$ï¼‰
2. åŸºäºæ¥æ”¶çš„ stateï¼ˆ$S_{0}$ï¼‰ï¼Œagent é‡‡å– actionï¼ˆ$A_{0}$ï¼‰
3. ç¯å¢ƒå˜åŒ–ï¼Œè½¬ç§»åˆ°æ–°çš„ stateï¼ˆ$S_{1}$ï¼‰
4. ç¯å¢ƒåé¦ˆ rewardï¼ˆ$R_{1}$ï¼‰ ç»™ agent

ç”¨ [Gymnasium åº“](https://gymnasium.farama.org/api/env/#gymnasium.Env.step)ï¼Œå¯¹åº”çš„å…³é”®ä»£ç å°±æ˜¯ï¼š

0. åˆ›å»ºç¯å¢ƒï¼š`gymnasium.make()`
1. åˆå§‹åŒ–ç¯å¢ƒï¼ˆ$S_{0}$ï¼‰ï¼š`observation = env.reset()`
2. agent é‡‡å– `action = env.action_space.sample()`ï¼ˆç»™çš„ä¾‹å­ä¸­ action æ˜¯éšæœºé€‰å–ï¼‰
3. `env.step(action)`
4. æ¥æ”¶ä¸Šä¸€æ­¥çš„è¿”å›å€¼ `observation, reward, terminated, truncated, info = env.step(action)`
	- observationï¼šnew state
	- terminatedï¼šåˆ¤æ–­æ¸¸æˆæ˜¯å¦ç»“æŸ
	- truncatedï¼šåˆ¤æ–­æ¸¸æˆæ˜¯å¦è¶…æ—¶æˆ– agent æ˜¯å¦è¶…å‡ºç¯å¢ƒè¾¹ç•Œ
	- infoï¼šä»¥å­—å…¸æ ¼å¼ç»™å‡ºä¸€äº›é¢å¤–ä¿¡æ¯

è¿è¡Œç»™çš„ä»£ç æŠ¥é”™ï¼Œè§£å†³æ–¹æ³•ï¼š

ä¿®æ”¹ï¼š`env = gym.make("LunarLander-v3")`

æ·»åŠ ä¾èµ–ï¼š

```bash
!pip install swig
!pip install gymnasium[box2d]
```

é¡ºåˆ©è¿è¡Œå¾—åˆ°ç»“æœ

## åˆ›å»º LunarLander ç¯å¢ƒ

[Lunar Lander ç¯å¢ƒè¯´æ˜](https://gymnasium.farama.org/environments/box2d/lunar_lander/)

æˆ‘ä»¬éœ€è¦å­¦ä¹ è°ƒæ•´ç€é™†å™¨çš„é€Ÿåº¦å’Œå…¶ä½ç½®ï¼ˆåŒ…æ‹¬æ°´å¹³åæ ‡ï¼Œå‚ç›´åæ ‡ï¼Œè§’åº¦ï¼‰

```python
# We create our environment with gym.make("<name_of_the_environment>")
env = gym.make("LunarLander-v3")
env.reset()
print("_____OBSERVATION SPACE_____ \n")
print("Observation Space Shape", env.observation_space.shape)
print("Sample observation", env.observation_space.sample()) # Get a random observation
```

ç¯å¢ƒæ ¼å¼æ˜¯æœ‰ 8 ä¸ªå…ƒç´ çš„ä¸€ç»´å‘é‡ï¼š

```bash
_____OBSERVATION SPACE_____ 

Observation Space Shape (8,)
Sample observation [-0.198397  -0.2969256  -1.9465257  3.6811924  3.5031452  2.0033157  0.23429972  0.580053]
```

å‘é‡çš„ 8 ä¸ªåˆ†é‡åˆ†åˆ«è¡¨ç¤ºï¼š
- æ°´å¹³å«åæ ‡ (x)
- å‚ç›´å«åæ ‡ (y)
- æ°´å¹³é€Ÿåº¦ (x)
- å‚ç›´é€Ÿåº¦ (y)
- è§’åº¦
- è§’é€Ÿåº¦
- å·¦è…¿æ¥è§¦ç‚¹æ˜¯å¦æ¥è§¦åœ°é¢ (å¸ƒå°”å€¼)
- å³è…¿æ¥è§¦ç‚¹æ˜¯å¦æ¥è§¦åœ°é¢ (å¸ƒå°”å€¼)

```python
print("\n _____ACTION SPACE_____ \n")
print("Action Space Shape", env.action_space.n)
print("Action Space Sample", env.action_space.sample()) # Take a random action
```

åŠ¨ä½œç©ºé—´è¾“å‡ºä¸ºï¼š

```bash
 _____ACTION SPACE_____ 

Action Space Shape 4
Action Space Sample 0
```

åŠ¨ä½œç©ºé—´ï¼ˆæ™ºèƒ½ä½“å¯ä»¥é‡‡å–çš„åŠ¨ä½œé›†åˆï¼‰æ˜¯ç¦»æ•£çš„ï¼Œæœ‰ 4 ä¸ªå¯ç”¨çš„åŠ¨ä½œï¼š
- åŠ¨ä½œ 0ï¼šä»€ä¹ˆéƒ½ä¸åš
- åŠ¨ä½œ 1ï¼šå‘åŠ¨å·¦ä¾§å®šå‘å¼•æ“
- åŠ¨ä½œ 2ï¼šå‘åŠ¨ä¸»å¼•æ“
- åŠ¨ä½œ 3ï¼šå‘åŠ¨å³ä¾§å®šå‘å¼•æ“

æ¸¸æˆçš„æ¯ä¸€æ­¥éƒ½æœ‰ rewardï¼Œä¸€ä¸ªå›åˆçš„æ€»å¥–åŠ±æ˜¯è¯¥å›åˆå†…æ‰€æœ‰æ­¥éª¤å¥–åŠ±çš„æ€»å’Œã€‚å…·ä½“ reward æŒ‰å¦‚ä¸‹æ–¹å¼ç»™å‡ºï¼š
- è¶Šé è¿‘/è¶Šè¿œç¦»ç€é™†ç‚¹ï¼Œç€é™†å™¨çš„å¥–åŠ±å°±è¶Šå¤š/è¶Šå°‘
- ç€é™†å™¨ç§»åŠ¨å¾—è¶Šæ…¢/è¶Šå¿«ï¼Œå¥–åŠ±å°±è¶Šå¤š/è¶Šå°‘
- ç€é™†å™¨å€¾æ–œå¾—è¶Šå¤šï¼ˆè§’åº¦éæ°´å¹³ï¼‰ï¼Œå¥–åŠ±å°±è¶Šå°‘
- æ¯æœ‰ä¸€ä¸ªä¸åœ°é¢æ¥è§¦çš„è…¿å¢åŠ 10åˆ†
- æ¯å½“ä¾§å‘å¼•æ“å‘åŠ¨æ—¶ï¼Œå¥–åŠ±å‡å°‘0.03åˆ†
- æ¯å½“ä¸»å¼•æ“å‘åŠ¨æ—¶ï¼Œå¥–åŠ±å‡å°‘0.3åˆ†
- å¦‚æœå æ¯æˆ–å®‰å…¨ç€é™†ï¼Œæƒ…èŠ‚å°†è·å¾—é¢å¤–çš„-100æˆ–+100åˆ†
- å¦‚æœå›åˆå¾—åˆ† $\geq$ 200åˆ†ï¼Œåˆ™è®¤ä¸ºæ˜¯ä¸€ä¸ªè§£å†³æ–¹æ¡ˆ

## å‘é‡åŒ–ç¯å¢ƒ

å‘é‡åŒ–ç¯å¢ƒ Vectorized environmentsï¼šæ˜¯å°†åŒä¸€ç¯å¢ƒçš„å¤šä¸ªç‹¬ç«‹å‰¯æœ¬ç»„ç»‡åœ¨ä¸€èµ·è¿è¡Œï¼Œå®ƒæ¥æ”¶ä¸€æ‰¹ action è¾“å…¥ï¼ŒåŒæ—¶è¿”å›ä¸€æ‰¹ observation

æˆ‘ä»¬åˆ›å»ºåŒ…å« 16 ä¸ªå‰¯æœ¬çš„ç¯å¢ƒå‘é‡ï¼Œåœ¨ä¸€è®ºè®­ç»ƒå°±å¯ä»¥è·å¾— 16 ä¸ª episodeï¼š

```python
# Create the environment
env = make_vec_env('LunarLander-v2', n_envs=16)
```

## åˆ›å»ºæ¨¡å‹

æˆ‘ä»¬ä½¿ç”¨ [Stable Baseline3](https://stable-baselines3.readthedocs.io/en/master/) æ·±åº¦å­¦ä¹ åº“ï¼Œè¿™ä¸ªåº“åŒ…å«äº†ä¸€ç³»åˆ—ç”¨ PyTorch å®ç°çš„ RL ç®—æ³•

é‡‡ç”¨ Stable Baseline3 åº“ä¸­çš„ [PPO](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D) ç®—æ³•è§£å†³è¿™ä¸ªé—®é¢˜

PPO ç®—æ³•æ˜¯ Value-based å’Œ Policy-based çš„ç»“åˆï¼Œç®€å•æ¥è¯´ï¼š
- Value-based æ–¹é¢ï¼šå­¦ä¹ ä¸€ä¸ª action-value å‡½æ•°ï¼Œè€Œä¸æ˜¯ state-value å‡½æ•°ã€‚action-value å‡½æ•°ä¼šå‘Šè¯‰æˆ‘ä»¬åœ¨ç»™å®š <stateï¼Œaction> ä¸‹æœ€æœ‰ä»·å€¼çš„ action 
- Policy-based æ–¹é¢ï¼šå­¦ä¹ ä¸€ä¸ª policyï¼Œå‘Šè¯‰æˆ‘ä»¬å…³äºå„ä¸ª action çš„æ¦‚ç‡åˆ†å¸ƒ

```python
# Create environment
env = gym.make('LunarLander-v2')

# Instantiate the agent
# å®šä¹‰æƒ³è¦ä½¿ç”¨çš„æ¨¡å‹å¹¶å®ä¾‹åŒ–è¯¥æ¨¡å‹
model = PPO('MlpPolicy', env, verbose=1)
# Train the agent
model.learn(total_timesteps=int(2e5))
```

ä¸‹é¢æˆ‘ä»¬è€ƒè™‘è¯¥ç”¨ä»€ä¹ˆæ ·çš„æ¨¡å‹ï¼Œæ¯æ¬¡æ¨¡å‹çš„è¾“å…¥æ˜¯ agent çš„ observationï¼Œåœ¨ä¸Šé¢æˆ‘ä»¬æåˆ° observation æ˜¯å« 8 ä¸ªå…ƒç´ çš„ä¸€ç»´å‘é‡ï¼›æ¨¡å‹çš„è¾“å‡ºæ˜¯ actionï¼Œä¸Šè¿°å¯çŸ¥ï¼Œæ˜¯å« 4 ä¸ªå…ƒç´ çš„ä¸€ç»´å‘é‡ã€‚äºæ˜¯ä¸å¦¨ç”¨æœ€ç®€å•çš„ MLP ç»“æ„ï¼Œå½“ç„¶å¦‚æœè¾“å…¥æ˜¯å›¾åƒï¼Œè¾“å‡ºæ˜¯å‘é‡ï¼Œå°±è¯¥ç”¨ CNN ç»“æ„

```python
# æˆ‘ä»¬æ·»åŠ äº†ä¸€äº›è¶…å‚æ•°æ¥åŠ å¿«è®­ç»ƒ
model = PPO(
    policy = 'MlpPolicy',
    env = env,
    n_steps = 1024,
    batch_size = 64,
    n_epochs = 4,
    gamma = 0.999, # æŠ˜æ‰£å› å­
    gae_lambda = 0.98,
    ent_coef = 0.01,
    verbose=1)
```

## è®­ç»ƒ PPO agent

æˆ‘ä»¬è®­ç»ƒ 1 000 000 æ­¥ï¼Œå¤§æ¦‚éœ€è¦ 20 åˆ†é’Ÿ

```python
# Train it for 1,000,000 timesteps
model.learn(total_timesteps=1000000)
# Save the model
model_name = "ppo-LunarLander-v2"
model.save(model_name)
```

## è¯„ä¼° agent

é¦–å…ˆå°†ç¯å¢ƒæ‰“åŒ…åˆ° [Monitor](https://stable-baselines3.readthedocs.io/en/master/common/monitor.html) é‡Œç›‘æµ‹

æˆ‘ä»¬ä½¿ç”¨ Stable Baselines3 åº“æä¾›çš„ `evaluate_policy` å‡½æ•°æ¥è¯„ä¼°æ€§èƒ½

> [!important]
> åœ¨è¯„ä¼° agent æ—¶ï¼Œä¸åº”è¯¥ä½¿ç”¨è®­ç»ƒç¯å¢ƒï¼Œè€Œåº”è¯¥åˆ›å»ºä¸“é—¨çš„è¯„ä¼°ç¯å¢ƒ 

å…³äº [evaluate_policy](https://stable-baselines3.readthedocs.io/en/master/common/evaluation.html#eval) å‡½æ•°çš„è¯¦ç»†è¯´æ˜

```python
# è¯„ä¼°agent
eval_env = Monitor(gym.make("LunarLander-v2", render_mode='rgb_array'))
mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)
print(f"mean_reward={mean_reward:.2f} +/- {std_reward}")
```

- n_eval_episodes = 10ï¼šè¡¨ç¤ºè¿è¡Œ 10 ä¸ªå›åˆæ¥è¯„ä¼° agent
- deterministic = Trueï¼šè¡¨ç¤ºæˆ‘ä»¬é‡‡ç”¨ç¡®å®šçš„ policy

## å‘å¸ƒè®­ç»ƒå¥½çš„æ¨¡å‹åˆ° Hugging Face Hub

é¦–å…ˆåˆ›å»ºä¸€ä¸ªæœ‰å†™æƒé™çš„ Token

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š
```bash
notebook_login()
!git config --global credential.helper store
```

æœ€ååªéœ€æ‰§è¡Œ `package_to_hub()` å‡½æ•°å³å¯

```python
import gymnasium as gym

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.env_util import make_vec_env

from huggingface_sb3 import package_to_hub

# PLACE the variables you've just defined two cells above
# Define the name of the environment
env_id = "LunarLander-v2"

# TODO: Define the model architecture we used
model_architecture = "PPO"

## Define a repo_id
## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2
## CHANGE WITH YOUR REPO ID
repo_id = "EvaristeL/ppo-LunarLander-v2" # Change with your repo id, you can't push with mine ğŸ˜„

## Define the commit message
commit_message = "Upload PPO LunarLander-v2 trained agent"

# Create the evaluation env and set the render_mode="rgb_array"
eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode="rgb_array")])

# PLACE the package_to_hub function you've just filled here
package_to_hub(model=model, # è®­ç»ƒå¥½çš„æ¨¡å‹
               model_name=model_name, # The name of our trained model
               model_architecture=model_architecture, # The model architecture we used: in our case PPO
               env_id=env_id, # Name of the environment
               eval_env=eval_env, # Evaluation Environment
               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2
               commit_message=commit_message)
```

## ä» Hugging Face Hub åŠ è½½æ¨¡å‹

é¦–å…ˆåˆ° [stable-baselines3åº“](https://huggingface.co/models?library=stable-baselines3)æŸ¥çœ‹ä¿å­˜çš„æ¨¡å‹

å¤åˆ¶ä»“åº“id
![[pic-20250312161510663.png]]

æˆ‘ä»¬è¿˜éœ€è¦çŸ¥é“è¦åŠ è½½çš„æ¨¡å‹æ–‡ä»¶å

ç”±äºç¤ºä¾‹çš„æ¨¡å‹ä½¿ç”¨ Gymï¼ˆGymnasium çš„å‰èº«ï¼‰ åº“è®­ç»ƒçš„ï¼Œæ‰€ä»¥ä½¿ç”¨ [Shimmy](https://github.com/Farama-Foundation/Shimmy) åº“è¿›è¡Œè½¬æ¢ï¼Œä»¥ä¾¿æ­£ç¡®è¿è¡Œç¯å¢ƒ

```bash
!pip install shimmy
```

```python
from huggingface_sb3 import load_from_hub
repo_id = "Classroom-workshop/assignment2-omar" # The repo_id
filename = "ppo-LunarLander-v2.zip" # The model filename.zip

# When the model was trained on Python 3.8 the pickle protocol is 5
# But Python 3.6, 3.7 use protocol 4
# In order to get compatibility we need to:
# 1. Install pickle5 (we done it at the beginning of the colab)
# 2. Create a custom empty object we pass as parameter to PPO.load()
custom_objects = {
            "learning_rate": 0.0,
            "lr_schedule": lambda _: 0.0,
            "clip_range": lambda _: 0.0,
}

checkpoint = load_from_hub(repo_id, filename)
model = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)
```

## ä¸€äº›é¢å¤–çš„æŒ‘æˆ˜

å¯ä»¥è‡ªè¡Œè°ƒæ•´ä¸‹è¶…å‚æ•°ï¼Œæˆ–è€…æ¢ä¸€ä¸ªæ¨¡å‹ä¾‹å¦‚ DQN è¯•è¯•æ•ˆæœ

æ­¤å¤–è¿˜æœ‰å¾ˆå¤šä¸åŒæ¸¸æˆç¯å¢ƒï¼Œå¯ä»¥æŸ¥çœ‹ [Gymnasium Documentation](https://gymnasium.farama.org/)