# 强化学习框架

## The RL Process

![[pic-20250228234127151.png]]

上图来自：[Reinforcement Learning: An Introduction, Richard Sutton and Andrew G. Barto](http://incompleteideas.net/book/RLbook2020.pdf)

我们以一个 agent 学习如何玩游戏来理解框架：

![[pic-20250309171357447.png]]

- agent 从 Environment 接收 state $S_{0}$ ——对应游戏画面的第一帧
- agent 根据接收到的 state $S_{0}$ 采取 action $A_{0}$ ——对应 agent 向右移动
- 由于 agent 发生移动，所以environment 转到一个新的 state $S_{1}$ ——对应游戏画面新的一帧
- environment 给 agent 一些 reward $R_{1}$ ——例如 agent 向右移动不会导致游戏结束（*Positive Reward* +1）
- 重复整个循环

上述强化学习的循环会输出一系列的 state，action，reward 和 next state

![[pic-20250309172130646.png]]

agent 目标就是最大化它获得的**累计奖励**（cumulative reward），称累计奖励的期望值为**期望回报**（expected return）

## 强化学习的核心：reward 假设

> [!important]
> reward 假设：所有的待学习目标都可以被转化成如何最大化期望回报的问题

因此，在强化学习中，为了找到最优的行动方案，核心目标就是学习如何选择 action 使期望回报最大

## Markov 性质

强化学习过程也称 Markov Decision Process（MDP）

在后续章节中会详细介绍 Markov 性质，这里只需知道：满足 Markov 性质，agent 在决策时只需考虑当前 state，而无需考虑过去经历的 states 和做出的 actions，简而言之就是“无记忆性”

## Observations/States Space

Observations/States 是 agent 从环境中得到的信息，以视频游戏为例，Observations 是观测到的一帧画面。很多情况下我们都混用 Observations 和 States，但这两个概念仍有区别：

- State s：包含了所有的信息来完全描述当前环境的情况
![[pic-20250310090500470.png]]
例如在国际象棋游戏中，整个棋盘信息是完全可观察的

- Observation o：只包含了 agent 能够感知到的部分信息，而非环境的完整状态
当 agent 能观测到环境的全部信息时，这样的环境是可完全观测的 （fully observed）；当 agent 只能观测到部分信息时，这样的环境称为可部分观测 partially observed
![[pic-20250310094012670.png]]
例如在马里奥游戏中，我们只能看到关卡的一部分

> [!important]
> 在课程中，我们统一使用“state”来同时表示 state 和 observation，但在具体代码实现中，我们会明确区分两者的概念

## Action Space

动作空间（Action Space）是环境中 agent 所有可能采取动作（actions）的集合

而动作空间又可以分为连续型动作和离散型动作

- 离散型动作空间：可能采取的动作数量是有限的，例如马里奥游戏中，只能采取 4 种动作——左、右、跳、蹲
- 连续型动作空间：可能采取的动作数量是无限的，例如自动驾驶的汽车，它可以向左旋转 20°、21.1°、21.2°……

## Rewards and the discounting

奖励（reward）在强化学习（RL）中至关重要，因为它是 agent 唯一的反馈来源。通过 reward，agent 能够判断所采取的 action 是好是坏，从而调整其策略以优化未来的决策

轨迹（trajectory）$\tau$ 指的是状态和动作的序列：

$$
\tau = (s_0, a_0, s_1, a_1, \dots)
$$

第一个状态 $s_0$，是从**开始状态分布**中随机采样的，有时候表示为 $\rho_{0}(\cdot)$：

$$
s_{0} \sim \rho_{0}(\cdot)
$$

轨迹常常也被称为**回合**（episodes）或者 **rollouts**

累计奖励等于轨迹中每一步得到的奖励之和，累计奖励可以写做：

$$
\begin{aligned}
R(\tau) & = r_{t+1} + r_{t+2} + r_{t+3} + \dots \\
& = \sum_{k=0}^{\infty}r_{t+k+1}
\end{aligned}
$$

但实际我们不能按上式简单将所有奖励相加，下面的实例可以很好的说明这个问题：

![[pic-20250310110826762.png]]

如图，agent 是老鼠，每次老鼠可以移动一格，猫也会移动一格，我们的目标是让老鼠避免被猫抓到的同时吃掉最多的奶酪

显然不同奶酪会有不同的权重，离猫近的奶酪虽然更多，但风险也更大，且随着游戏进行，环境不断变化，我们不能保证一定能获取这些奶酪。而老鼠附近的奶酪，风险小且离得近，吃到的可能性非常大。概括的说：游戏开始易得到的 reward 相较于游戏进行很久才能得到的 reward 具有更强的**可预测性**

所以我们引入折扣率 $\gamma\in (0,1)$ 来调节权重，大多数情况下 $\gamma \in (0.95, 0.99)$

累计折扣奖励写做：

$$
\begin{aligned}
R(\tau) & = r_{t+1} + \gamma \cdot r_{t+2} + \gamma^2\cdot r_{t+3} + \dots \\
& = \sum_{k=0}^{\infty}\gamma^k \cdot r_{t+k+1}
\end{aligned}
$$

$\gamma$ 越大（长期 reward 和短期 reward 权重接近），意味着 agent 对长期 reward 和短期 reward 的关注度接近

$\gamma$ 越小（长期 reward 趋于 0 的速度越快），意味着 agent 更关心短期 reward