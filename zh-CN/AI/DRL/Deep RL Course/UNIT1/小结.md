# 小结

下面我们来总结前面的知识点：

- 强化学习（Reinforcement Learning，RL）是一种通过 actions 来学习的计算方法。我们构建一个 agent，它通过与环境交互，经历试错，并接收 rewards（正面或负面）作为反馈来进行学习

- 任何 RL 智能体的目标都是最大化其预期累计奖励（也称为预期回报），因为 RL 基于这样一个假设：所有目标都可以被描述为预期累计奖励的最大化（奖励假设）

- RL 的过程是一个循环，产生 state，action，reward 进入 next state 的序列。为了计算预期累计奖励（预期回报），我们对 rewards 进行折扣处理：更早得到的 reward（游戏初期）更有可能发生，因为相比长期未来的 reward 它们更加可预测

- 要解决一个 RL 问题，需要找到一个最优 policy。policy 是 agent 的“大脑”，它告诉我们面对特定 state 时应采取什么 action。最优 policy 是指那些能让我们采取获得最大预期回报行动的 policy

- 寻找最优策略有两种主要方法：
	- policy-based methods
	- value-based methods

- 最后，当我们引入“深度”神经网络来估计要采取的 action（policy-based）或评估一个 state 的value（value-based）时，就得到了所谓的“深度”强化学习（Deep RL）