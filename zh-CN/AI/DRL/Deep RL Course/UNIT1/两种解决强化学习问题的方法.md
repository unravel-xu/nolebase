# 两种解决强化学习问题的方法

现在我们已经学习了 RL 的框架，如何构建 agent 完成 RL 问题呢？

## 策略 $\pi$：agent 的大脑

策略 $\pi$ 是 agent 的大脑，它是一个函数，告诉我们在当前 state 下应该采取什么 action。因此，它定义了 agent 在特定时间应采取的行为：

![[pic-20250310161014057.png]]

策略就是我们希望通过 RL 学习到的函数，我们的目标就是找到最优策略 $\pi^{*}$，当 agent 根据这个策略采取 action 时返回最大期望回报

有两种方式可以训练 agent 找到最优策略 $\pi^{*}$：
- Policy-Based 的方法：我们直接告诉 agent 在给定的 state 下，它应该采取什么 action
- Value-Based 的方法：我们不告诉 agent 采取什么 action，而是告诉 agent 哪个 state 更有价值，让 agent 自己决定 action 向更有价值的 state 前进

## Policy-Based

在 Policy-Based 方法中，我们直接学习一个策略（policy）函数。policy 函数给出了每个 state 到对应最佳 action 的映射，或者给出每个 state 下所有可能 action 的概率分布：

![[pic-20250310190325145.png]]

如图所示，policy 函数（确定的）直接告诉 agent 在不同格子中采取什么 action

policy 函数有两类：

- Deterministic（确定型）：在给定状态下，policy 函数总是返回相同的 action

$$
\begin{aligned}
a & = \pi(s) \\
a &: action \\
\pi &: policy \\
s &: state
\end{aligned}
$$

![[pic-20250310191848383.png]]

- Stochastic（随机型）：在给定状态下，policy 函数返回一个关于 action 的概率分布

$$
\pi(a\mid s) = \underbrace{{P[A\mid s]}}_{actions集合的条件概率分布}
$$

![[pic-20250310192555605.png]]

##  Value-based

policy-based 的强化学习方法，核心是学习一个 policy 函数；而 value-based 的强化学习方法，核心是学习一个 value 函数

某个状态的价值（value of a state）是指 agent 从该状态开始，依照“特定策略”运行下去，最终所能获得的期望折扣回报

“特定策略”就是说我们要前往具有最高 value 的 state

![[pic-20250310203041382.png]]

如下图，value 函数为每个可能的 state（格子）都定义了 value

![[pic-20250310203202394.png]]

根据 value 函数给出的值，每一步我们按策略选择前往具有最大值的 state：$-7\to -6\to -5 \dots$ 最终顺利达到终点

policy-based 的方法，核心是训练一个 policy 函数；而 value-based 的方法，核心是学习一个 value 函数


