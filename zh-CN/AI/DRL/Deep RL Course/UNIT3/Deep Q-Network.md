# Deep Q-Network(DQN)

如下是 Deep Q-Learning 网络的结构：

![[pic-20250317204149456.png]]

我们将 4 帧图像堆叠起来作为一个状态输入，传递给神经网络，神经网络会输出该状态下每个可能动作的 Q-value 向量。然后和 Q-Learning 一样，使用 epsilon-greedy policy 选取动作

当神经网络初始化时，Q 值的估计是非常不准确的。但是随着训练进行，DQN（Deep Q-Network）智能体会将当前状态与合适的动作关联起来，并学会如何玩好游戏

## 预处理输入和时序限制

我们需要预处理输入，通过减少状态的复杂度从而减少训练的计算时间

游戏画面的颜色其实并不重要，所以我们可以将 3 通道（RGB）灰度化降低到 1 通道。游戏画面的空白部分对游戏进行也不起作用，我们进一步可以将 $160\times210$ 大小的画面缩小到 $84\times84$

![[pic-20250317233307012.png]]

如上图，我们将 4 张缩小的画面堆叠起来，之所以要堆叠帧，是因为连续的帧隐含时序信息，输入堆叠帧可以帮我们处理时序限制问题。我们以 Pong 游戏举例：

![[pic-20250317233728093.png]]

从上图中能知道小球运动方向吗？显然不能，单独的帧没有足够的前后运动信息

![[pic-20250317233845117.png]]

但给定连续的三个帧，我们就可以