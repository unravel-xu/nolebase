# åŠ¨æ‰‹å®éªŒ

ç°åœ¨æˆ‘ä»¬å·²ç»å­¦å®Œäº† Q-Learning ç®—æ³•ï¼Œè®©æˆ‘ä»¬ä»å¤´å¼€å§‹å®ç°å®ƒï¼Œå¹¶åœ¨ä¸¤ä¸ªç¯å¢ƒä¸­è®­ç»ƒ Agentï¼š

1. Frozen-Lake-v1ï¼ˆéæ»‘åŠ¨å’Œæ»‘åŠ¨ç‰ˆæœ¬ï¼‰ï¼šAgent éœ€è¦åœ¨èµ·å§‹çŠ¶æ€ï¼ˆSï¼‰å’Œç›®æ ‡çŠ¶æ€ï¼ˆGï¼‰ä¹‹é—´ç§»åŠ¨ï¼Œä¸”ç§»åŠ¨æ—¶ä»…èƒ½åœ¨å†°å†»çš„ç“·ç –ï¼ˆFï¼‰ä¸Šè¡Œèµ°ä¸èƒ½æ‰å…¥å†°æ´ï¼ˆHï¼‰
2. An autonomous taxiï¼šéœ€è¦å­¦ä¹ åœ¨åŸå¸‚ä¸­å°†ä¹˜å®¢ä» A ç‚¹è¿è¾“åˆ° B ç‚¹

![[pic-20250316105549771.png]]

## å‡†å¤‡å·¥ä½œ
### ğŸ® ç¯å¢ƒ:

- [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)
- [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)

### ğŸ“š RL-Library:

- Python and NumPy
- [Gymnasium](https://gymnasium.farama.org/)

### å®‰è£…ä¾èµ–

- gymnasium: Contains the FrozenLake-v1 â›„ and Taxi-v3 ğŸš• environments
- pygameï¼šUsed for the FrozenLake-v1 and Taxi-v3 UI
- numpyï¼šç”¨äºå¤„ç† Q-table

```bash
!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt
```

å‡ºç°å¦‚ä¸‹æŠ¥é”™ï¼š

```bash
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.1 which is incompatible.
pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.
langchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < "3.11", but you have async-timeout 5.0.1 which is incompatible.
```

æ‰‹åŠ¨é™çº§ï¼š

```bash
!pip install cryptography==43
!pip install pyopenssl==24.2.1
!pip install async-timeout==4.0.0
```

```bash
!sudo apt-get update
!sudo apt-get install -y python3-opengl
!apt install ffmpeg xvfb
!pip3 install pyvirtualdisplay
```

virtual screen å¯èƒ½éœ€è¦é‡å¯ notebook runtimeï¼Œæ‰€ä»¥æˆ‘ä»¬æ‰§è¡Œå¦‚ä¸‹ä»£ç å¼ºåˆ¶å´©æºƒï¼Œcolab ä¼šè‡ªåŠ¨é‡å¯ï¼Œæˆ‘ä»¬å†ä»ä¸‹ä¸€ä¸ª cell å¼€å§‹è¿è¡Œ

```python
import os
os.kill(os.getpid(), 9)
```

```python
# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()
```

### å¯¼å…¥åŒ…

```python
import numpy as np
import gymnasium as gym
import random
import imageio	# ç”Ÿæˆå›æ”¾è§†é¢‘
import os
import tqdm

import pickle5 as pickle
from tqdm.notebook import tqdm
```

å‡†å¤‡å·¥ä½œå®Œæˆ

## Part 1ï¼šFrozen Lake â›„ (non slippery version)

### åˆ›å»ºå¹¶äº†è§£ FrozenLake environment â›„

ç¯å¢ƒçš„[è¯´æ˜æ–‡æ¡£](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)

æœ‰ä¸¤ç§å¤§å°çš„ç¯å¢ƒï¼š
- `map_name = "4x4"`ï¼šå¤§å°ä¸º $4\times4$ çš„ç½‘æ ¼
- `map_name = "8x8"`ï¼šå¤§å°ä¸º $8\times8$ çš„ç½‘æ ¼

ç¯å¢ƒæœ‰ä¸¤ç§ä¸åŒçš„æ¨¡å¼ï¼š
- `is_slippery=False`ï¼šåœ¨ frozen lake ä¸Šä¸ä¼šæ»‘åŠ¨ï¼ŒAgent å¯ä»¥æŒ‰é¢„æœŸæ–¹å‘ç§»åŠ¨ï¼ˆç¡®å®šçš„ï¼‰
- `is_slippery=True`ï¼šåœ¨ frozen lake ä¸Šä¼šæ‰“æ»‘ï¼ŒAgent å¯èƒ½ä¸æŒ‰é¢„æœŸæ–¹å‘ç§»åŠ¨ï¼ˆéšæœºçš„ï¼‰

ç°åœ¨æˆ‘ä»¬ç”¨æœ€ç®€å•çš„ç¯å¢ƒï¼šå¤§å°ä¸º $4\times4$ ä¸”ä¸ä¼šæ»‘åŠ¨çš„ frozen lakeï¼Œæ·»åŠ ä¸€ä¸ªåä¸º `render_mode` çš„å‚æ•°æ¥æŒ‡å®šç¯å¢ƒå¯è§†åŒ–æ–¹å¼ã€‚æˆ‘ä»¬å¸Œæœ›æœ€åå½•åˆ¶ç¯å¢ƒçš„è§†é¢‘ï¼Œéœ€è¦è®¾ç½® `render_mode = rgb_array`

[æ–‡æ¡£](https://gymnasium.farama.org/api/env/#gymnasium.Env.render) è§£é‡Šäº†â€œrgb_arrayâ€ï¼šè¿”å›ä¸€ä¸ªè¡¨ç¤ºå½“å‰ç¯å¢ƒçŠ¶æ€çš„å¸§ã€‚ä¸€ä¸ªå¸§æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º (x, y, 3) çš„ np.ndarrayï¼Œè¡¨ç¤º $x\times y$ åƒç´ å›¾åƒçš„ RGB å€¼

```python
# Create the FrozenLake-v1 environment using 4x4 map and non-slippery version and render_mode="rgb_array"
env = gym.make("FrozenLake-v1", map_name="4x4", is_slippery=False, render_mode="rgb_array") # TODO use the correct parameters
```

äº§ç”Ÿ warningï¼Œå¿½ç•¥ä¸ç®¡ï¼š

```bash
/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.
  and should_run_async(code)
```

ä¹Ÿå¯ä»¥è‡ªå®šä¹‰ç½‘æ ¼ç¯å¢ƒï¼Œç”¨ S è¡¨ç¤º starting stateï¼ŒG è¡¨ç¤º goal stateï¼ŒF è¡¨ç¤º frozen tilesï¼ŒH è¡¨ç¤º holes

```python
# è‡ªå®šä¹‰ç¯å¢ƒ
desc=["SFFF", "FHFH", "FFFH", "HFFG"]
gym.make('FrozenLake-v1', desc=desc, is_slippery=True)
```

### æŸ¥çœ‹ç¯å¢ƒ

```python
# We create our environment with gym.make("<name_of_the_environment>")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).
print("_____OBSERVATION SPACE_____ \n")
print("Observation Space", env.observation_space)
print("Sample observation", env.observation_space.sample()) # Get a random observation
```

ä¼šçœ‹åˆ°æ˜¾ç¤ºï¼š

```bash
_____OBSERVATION SPACE_____ 

Observation Space Discrete(16)
Sample observation 2
```

observation ç”¨ä¸€ä¸ªæ•´æ•°æ¥è¡¨ç¤º Agent å½“å‰çš„ä½ç½®ï¼Œè®¡ç®—å¦‚ä¸‹ï¼š

![[pic-20250316145914722.png]]

æ‰€ä»¥ $4\times4$ çš„ç½‘æ ¼è¢«ç¼–å·ä¸º 0ï½15

```python
print("\n _____ACTION SPACE_____ \n")
print("Action Space Shape", env.action_space.n)
print("Action Space Sample", env.action_space.sample()) # Take a random action
```

ä¼šçœ‹åˆ°æ˜¾ç¤ºï¼š

```bash
 _____ACTION SPACE_____ 

Action Space Shape 4
Action Space Sample 2
```

action spaceï¼ˆagent å¯èƒ½é‡‡å–çš„åŠ¨ä½œé›†åˆæ˜¯ç¦»æ•£çš„ï¼‰ï¼Œæœ‰ 4 ä¸ªå¯ç”¨çš„åŠ¨ä½œï¼š
- 0ï¼šå‘å·¦
- 1ï¼šå‘ä¸‹
- 2ï¼šå‘å³
- 3ï¼šå‘ä¸Š

Reward function ğŸ’°ï¼š
- åˆ°è¾¾ç›®æ ‡ï¼š+1
- åˆ°è¾¾å†°æ´ï¼š0
- åˆ°è¾¾æ™®é€šå†°é¢ï¼š0

### åˆ›å»ºå¹¶åˆå§‹åŒ– Q-table

![[pic-20250316151021540.png]]

è¦åˆå§‹åŒ– Q-tableï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“æœ‰å¤šå°‘è¡Œå’Œå¤šå°‘åˆ—ï¼ŒQ-table çš„æ¯ä¸€è¡Œå¯¹åº”ä¸€ç§çŠ¶æ€ï¼Œæ‰€ä»¥çŠ¶æ€ç©ºé—´å¤§å°å°±æ˜¯ Q-table è¡Œæ•°ï¼›Q-table çš„æ¯ä¸€åˆ—å¯¹åº”ä¸€ç§åŠ¨ä½œï¼Œæ‰€ä»¥åŠ¨ä½œç©ºé—´å¤§å°å°±æ˜¯ Q-table åˆ—æ•°ã€‚Gym åº“æä¾›äº† `env.action_space.n` å’Œ `env.observation_space.n` è·å–è¿™ä¸¤ä¸ªä¿¡æ¯ï¼š

```python
state_space = env.observation_space.n
print("There are ", state_space, " possible states")

action_space = env.action_space.n
print("There are ", action_space, " possible actions")
```

ç”¨ 0 åˆå§‹åŒ– Q-tableï¼š

```python
# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros
def initialize_q_table(state_space, action_space):
  Qtable = np.zeros((state_space, action_space))
  return Qtable
```

```python
Qtable_frozenlake = initialize_q_table(state_space, action_space)
```

### å®šä¹‰ greedy ç­–ç•¥

è®°ä½ Q-Learning æ˜¯ä¸€ç§ off-policy ç®—æ³•ï¼Œæ‰€ä»¥ Q-Learning ä¸­ä¼šä¸¤ç§ä¸åŒçš„ç­–ç•¥ï¼Œä¸€ç§ç”¨äºé€‰æ‹©åŠ¨ä½œï¼Œä¸€ç§ç”¨äºæ›´æ–°ä»·å€¼å‡½æ•°ï¼š
- Epsilon-greedy policy (acting policy)
- Greedy-policy (updating policy)

greedy-policy ä¹Ÿæ˜¯æœ€ç»ˆæˆ‘ä»¬å®Œæˆè®­ç»ƒéœ€è¦çš„ policyï¼Œå®ƒå°†å‘Šè¯‰æˆ‘ä»¬å¦‚ä½•é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ

![[pic-20250316151930410.png]]

```python
def greedy_policy(Qtable, state):
  # Exploitation: take the action with the highest state, action value
  action = np.argmax(Qtable[state][:])

  return action
```

### å®šä¹‰ epsilon-greedy ç­–ç•¥

epsilon-greedy æ˜¯å¤„ç† exploration/exploitation trade-off çš„è®­ç»ƒç­–ç•¥

![[pic-20250316154553437.png]]

selects the greedy actionï¼šselects the action with the highest state-action pair value

éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œæˆ‘ä»¬**é€æ¸é™ä½ epsilon**ï¼Œå› ä¸ºæˆ‘ä»¬æ¢ç´¢çš„éœ€æ±‚è¶Šæ¥è¶Šå°‘ï¼Œåˆ©ç”¨çš„éœ€æ±‚è¶Šæ¥è¶Šå¤š

```python
def epsilon_greedy_policy(Qtable, state, epsilon):
  # Randomly generate a number between 0 and 1
  random_num = random.uniform(0,1)
  # if random_num > greater than epsilon --> exploitation
  if random_num > epsilon:
    # Take the action with the highest value given a state
    # np.argmax can be useful here
    action = greedy_policy(Qtable, state)
  # else --> exploration
  else:
    action = env.action_space.sample()

  return action
```

### å®šä¹‰è®­ç»ƒçš„è¶…å‚æ•°

ä¸ exploration ç›¸å…³çš„è¶…å‚æ•°éå¸¸é‡è¦ï¼š
- æˆ‘ä»¬éœ€è¦ä¿è¯ Agent èƒ½å……åˆ†æ¢ç´¢çŠ¶æ€ç©ºé—´ï¼Œè¿›è€Œå­¦ä¹ å¥½çš„è¿‘ä¼¼å€¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦é€æ­¥é™ä½ epsilon
- å¦‚æœé™ä½ epsilon é€Ÿåº¦è¿‡å¿«ï¼ˆdecay_rate è¡°å‡ç‡è¿‡é«˜ï¼‰ï¼ŒAgent è®­ç»ƒå¯èƒ½ä¼šè¢«å¡ä½ï¼Œå› ä¸ºå®ƒè¿˜æ²¡å……åˆ†æ¢ç´¢ç¯å¢ƒï¼Œå¯¼è‡´æ— æ³•è§£å†³é—®é¢˜

```python
# Training parameters
n_training_episodes = 10000  # Total training episodes
learning_rate = 0.7          # Learning rate

# Evaluation parameters
n_eval_episodes = 100        # Total number of test episodes

# Environment parameters
env_id = "FrozenLake-v1"     # Name of the environment
max_steps = 99               # Max steps per episode
gamma = 0.95                 # Discounting rate
eval_seed = []               # The evaluation seed of the environment

# Exploration parameters
max_epsilon = 1.0             # Exploration probability at start
min_epsilon = 0.05            # Minimum exploration probability
decay_rate = 0.0005            # Exponential decay rate for exploration prob
```

### åˆ›å»ºå¾ªç¯è®­ç»ƒæ–¹æ³•

![[pic-20250316155704236.png]]

```txt
å¯¹äºæ€»è®­ç»ƒå›åˆçš„æ¯ä¸ªå›åˆ:

å‡å° epsilon (å› ä¸ºæˆ‘ä»¬å¯¹explorationçš„éœ€æ±‚è¶Šæ¥è¶Šå°‘)
é‡ç½®ç¯å¢ƒï¼Œå¼€å§‹ä¸€è½®æ¸¸æˆ

	å¯¹äºæœ€å¤§æ—¶é—´æ­¥çš„æ¯ä¸€æ­¥ï¼š
		æ ¹æ®epsilon-greedyç­–ç•¥é€‰æ‹©At
		é‡‡å–é€‰æ‹©çš„action(a)å¹¶è§‚å¯Ÿå¾—åˆ°çš„state(s')å’Œreward(s)
		ç”¨ä¸Šå›¾step4çš„Bellmanå…¬å¼æ›´æ–°Q(s,a)çš„å€¼
		è½¬åˆ°ä¸‹ä¸€çŠ¶æ€
```

```python
def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):
  for episode in tqdm(range(n_training_episodes)):
    # Reduce epsilon (because we need less and less exploration)
    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)
    # Reset the environment
    state, info = env.reset()
    step = 0
    terminated = False
    truncated = False

    # repeat
    for step in range(max_steps):
      # Choose the action At using epsilon greedy policy
      action = epsilon_greedy_policy(Qtable, state, epsilon)

      # Take action At and observe Rt+1 and St+1
      # Take the action (a) and observe the outcome state(s') and reward (r)
      new_state, reward, terminated, truncated, info = env.step(action)

      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]
      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])

      # If terminated or truncated finish the episode
      if terminated or truncated:
        break

      # Our next state is the new state
      state = new_state
  return Qtable
```

$\epsilon$ æ›´æ–°å…¬å¼ä¸ºï¼š

$$
\epsilon = \epsilon_{min} + \left(\epsilon_{max} - \epsilon_{min}\right)\cdot e^{-\lambda \cdot t} 
$$

å…¶ä¸­ï¼š$\lambda$ æ˜¯è¡°å‡ç‡ decay_rateï¼Œt æ˜¯å½“å‰è®­ç»ƒè½®æ¬¡(episode)ã€‚$t=0$ æ—¶ï¼Œ$\epsilon = \epsilon_{max}$

### è®­ç»ƒ Q-Learning æ™ºèƒ½ä½“

```python
Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)
```

æŸ¥çœ‹ Q-tableï¼š

```python
Qtable_frozenlake
```

![[pic-20250316161256840.png]]

### å®šä¹‰è¯„ä¼°æ–¹æ³•

æˆ‘ä»¬å®šä¹‰å¦‚ä¸‹è¯„ä¼°æ–¹æ³•æ¥æµ‹è¯• Agent

```python
def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):
  """
  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.
  :param env: The evaluation environment
  :param max_steps: Maximum number of steps per episode
  :param n_eval_episodes: Number of episode to evaluate the agent
  :param Q: The Q-table
  :param seed: The evaluation seed array (for taxi-v3)
  """
  episode_rewards = []
  for episode in tqdm(range(n_eval_episodes)):
    if seed:
      state, info = env.reset(seed=seed[episode])
    else:
      state, info = env.reset()
    step = 0
    truncated = False
    terminated = False
    total_rewards_ep = 0

    for step in range(max_steps):
      # Take the action (index) that have the maximum expected future reward given that state
      action = greedy_policy(Q, state)
      new_state, reward, terminated, truncated, info = env.step(action)
      total_rewards_ep += reward

      if terminated or truncated:
        break
      state = new_state
    episode_rewards.append(total_rewards_ep)
  mean_reward = np.mean(episode_rewards)
  std_reward = np.std(episode_rewards)

  return mean_reward, std_reward
```

### è¯„ä¼° Q-Learning agent

å› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯å›ºå®šç¯å¢ƒï¼ˆæ— æ»‘åŠ¨ï¼‰ï¼Œä¸”ç¯å¢ƒæƒ…å†µå¾ˆç®€å•ï¼Œä¸å‡ºæ„å¤–ä¼šè¾“å‡º mean reward=1.0

```python
# Evaluate our Agent
mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)
print(f"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}")
```

### å‘å¸ƒè®­ç»ƒå¥½çš„æ¨¡å‹åˆ° Hub

Hugging Face Hub åŸºäº Git ä»“åº“ï¼Œå¯ä»¥éšæ—¶ä¸Šä¼ æ›´æ–°ä¿å­˜çš„æ¨¡å‹

```python
from huggingface_hub import HfApi, snapshot_download
from huggingface_hub.repocard import metadata_eval_result, metadata_save

from pathlib import Path
import datetime
import json

def record_video(env, Qtable, out_directory, fps=1):
  """
  Generate a replay video of the agent
  :param env
  :param Qtable: Qtable of our agent
  :param out_directory
  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)
  """
  images = []
  terminated = False
  truncated = False
  state, info = env.reset(seed=random.randint(0,500))
  img = env.render()
  images.append(img)
  while not terminated or truncated:
    # Take the action (index) that have the maximum expected future reward given that state
    action = np.argmax(Qtable[state][:])
    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic
    img = env.render()
    images.append(img)
  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)
```

```python
def push_to_hub(
    repo_id, model, env, video_fps=1, local_repo_path="hub"
):
    """
    Evaluate, Generate a video and Upload a model to Hugging Face Hub.
    This method does the complete pipeline:
    - It evaluates the model
    - It generates the model card
    - It generates a replay video of the agent
    - It pushes everything to the Hub

    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub
    :param env
    :param video_fps: how many frame per seconds to record our video replay
    (with taxi-v3 and frozenlake-v1 we use 1)
    :param local_repo_path: where the local repository is
    """
    _, repo_name = repo_id.split("/")

    eval_env = env
    api = HfApi()

    # Step 1: Create the repo
    repo_url = api.create_repo(
        repo_id=repo_id,
        exist_ok=True,
    )

    # Step 2: Download files
    repo_local_path = Path(snapshot_download(repo_id=repo_id))

    # Step 3: Save the model
    if env.spec.kwargs.get("map_name"):
        model["map_name"] = env.spec.kwargs.get("map_name")
        if env.spec.kwargs.get("is_slippery", "") == False:
            model["slippery"] = False

    # Pickle the model
    with open((repo_local_path) / "q-learning.pkl", "wb") as f:
        pickle.dump(model, f)

    # Step 4: Evaluate the model and build JSON with evaluation metrics
    mean_reward, std_reward = evaluate_agent(
        eval_env, model["max_steps"], model["n_eval_episodes"], model["qtable"], model["eval_seed"]
    )

    evaluate_data = {
        "env_id": model["env_id"],
        "mean_reward": mean_reward,
        "n_eval_episodes": model["n_eval_episodes"],
        "eval_datetime": datetime.datetime.now().isoformat()
    }

    # Write a JSON file called "results.json" that will contain the
    # evaluation results
    with open(repo_local_path / "results.json", "w") as outfile:
        json.dump(evaluate_data, outfile)

    # Step 5: Create the model card
    env_name = model["env_id"]
    if env.spec.kwargs.get("map_name"):
        env_name += "-" + env.spec.kwargs.get("map_name")

    if env.spec.kwargs.get("is_slippery", "") == False:
        env_name += "-" + "no_slippery"

    metadata = {}
    metadata["tags"] = [env_name, "q-learning", "reinforcement-learning", "custom-implementation"]

    # Add metrics
    eval = metadata_eval_result(
        model_pretty_name=repo_name,
        task_pretty_name="reinforcement-learning",
        task_id="reinforcement-learning",
        metrics_pretty_name="mean_reward",
        metrics_id="mean_reward",
        metrics_value=f"{mean_reward:.2f} +/- {std_reward:.2f}",
        dataset_pretty_name=env_name,
        dataset_id=env_name,
    )

    # Merges both dictionaries
    metadata = {**metadata, **eval}

    model_card = f"""
  # **Q-Learning** Agent playing1 **{env_id}**
  This is a trained model of a **Q-Learning** agent playing **{env_id}** .

  ## Usage

  ```python

  model = load_from_hub(repo_id="{repo_id}", filename="q-learning.pkl")

  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)
  env = gym.make(model["env_id"])
  ```
  """

    evaluate_agent(env, model["max_steps"], model["n_eval_episodes"], model["qtable"], model["eval_seed"])

    readme_path = repo_local_path / "README.md"
    readme = ""
    print(readme_path.exists())
    if readme_path.exists():
        with readme_path.open("r", encoding="utf8") as f:
            readme = f.read()
    else:
        readme = model_card

    with readme_path.open("w", encoding="utf-8") as f:
        f.write(readme)

    # Save our metrics to Readme metadata
    metadata_save(readme_path, metadata)

    # Step 6: Record a video
    video_path = repo_local_path / "replay.mp4"
    record_video(env, model["qtable"], video_path, video_fps)

    # Step 7. Push everything to the Hub
    api.upload_folder(
        repo_id=repo_id,
        folder_path=repo_local_path,
        path_in_repo=".",
    )

    print("Your model is pushed to the Hub. You can view your model here: ", repo_url)
```

å› ä¸ºæˆ‘ç”¨çš„æ˜¯ kaggle notebookï¼Œæ‰€ä»¥å’Œ BONUS UNIT1 ä¸€æ ·ï¼Œä½¿ç”¨ api ä¸Šä¼ 

```python
from huggingface_hub.hf_api import HfFolder
HfFolder.save_token("YOUR_HUGGINGFACE_TOKEN")
```

```python
model = {
    "env_id": env_id,
    "max_steps": max_steps,
    "n_training_episodes": n_training_episodes,
    "n_eval_episodes": n_eval_episodes,
    "eval_seed": eval_seed,

    "learning_rate": learning_rate,
    "gamma": gamma,

    "max_epsilon": max_epsilon,
    "min_epsilon": min_epsilon,
    "decay_rate": decay_rate,

    "qtable": Qtable_frozenlake
}
```

![[pic-20250316162548345.png]]

```python
username = "" # FILL THIS
repo_name = "q-FrozenLake-v1-4x4-noSlippery"
push_to_hub(
    repo_id=f"{username}/{repo_name}",
    model=model,
    env=env)
```

## Part2ï¼šTaxi-v3

