# 动手实验

现在我们已经学完了 Q-Learning 算法，让我们从头开始实现它，并在两个环境中训练 Agent：

1. Frozen-Lake-v1（非滑动和滑动版本）：Agent 需要在起始状态（S）和目标状态（G）之间移动，且移动时仅能在冰冻的瓷砖（F）上行走不能掉入冰洞（H）
2. An autonomous taxi：需要学习在城市中将乘客从 A 点运输到 B 点

![[pic-20250316105549771.png]]

## 准备工作
### 🎮 环境:

- [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)
- [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)

### 📚 RL-Library:

- Python and NumPy
- [Gymnasium](https://gymnasium.farama.org/)

### 安装依赖

- gymnasium: Contains the FrozenLake-v1 ⛄ and Taxi-v3 🚕 environments
- pygame：Used for the FrozenLake-v1 and Taxi-v3 UI
- numpy：用于处理 Q-table

```bash
!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt
```

出现如下报错：

```bash
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.1 which is incompatible.
pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.
langchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < "3.11", but you have async-timeout 5.0.1 which is incompatible.
```

手动降级：

```bash
!pip install cryptography==43
!pip install pyopenssl==24.2.1
!pip install async-timeout==4.0.0
```

```bash
!sudo apt-get update
!sudo apt-get install -y python3-opengl
!apt install ffmpeg xvfb
!pip3 install pyvirtualdisplay
```

virtual screen 可能需要重启 notebook runtime，所以我们执行如下代码强制崩溃，colab 会自动重启，我们再从下一个 cell 开始运行

```python
import os
os.kill(os.getpid(), 9)
```

```python
# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()
```

### 导入包

```python
import numpy as np
import gymnasium as gym
import random
import imageio	# 生成回放视频
import os
import tqdm

import pickle5 as pickle
from tqdm.notebook import tqdm
```

准备工作完成

## Part 1：Frozen Lake ⛄ (non slippery version)

### 创建并了解 FrozenLake environment ⛄

环境的[说明文档](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)

有两种大小的环境：
- `map_name = "4x4"`：大小为 $4\times4$ 的网格
- `map_name = "8x8"`：大小为 $8\times8$ 的网格

环境有两种不同的模式：
- `is_slippery=False`：在 frozen lake 上不会滑动，Agent 可以按预期方向移动（确定的）
- `is_slippery=True`：在 frozen lake 上会打滑，Agent 可能不按预期方向移动（随机的）

现在我们用最简单的环境：大小为 $4\times4$ 且不会滑动的 frozen lake，添加一个名为 `render_mode` 的参数来指定环境可视化方式。我们希望最后录制环境的视频，需要设置 `render_mode = rgb_array`

[文档](https://gymnasium.farama.org/api/env/#gymnasium.Env.render) 解释了“rgb_array”：返回一个表示当前环境状态的帧。一个帧是一个形状为 (x, y, 3) 的 np.ndarray，表示 $x\times y$ 像素图像的 RGB 值

```python
# Create the FrozenLake-v1 environment using 4x4 map and non-slippery version and render_mode="rgb_array"
env = gym.make("FrozenLake-v1", map_name="4x4", is_slippery=False, render_mode="rgb_array") # TODO use the correct parameters
```

产生 warning，忽略不管：

```bash
/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.
  and should_run_async(code)
```

也可以自定义网格环境，用 S 表示 starting state，G 表示 goal state，F 表示 frozen tiles，H 表示 holes

```python
# 自定义环境
desc=["SFFF", "FHFH", "FFFH", "HFFG"]
gym.make('FrozenLake-v1', desc=desc, is_slippery=True)
```

### 查看环境

```python
# We create our environment with gym.make("<name_of_the_environment>")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).
print("_____OBSERVATION SPACE_____ \n")
print("Observation Space", env.observation_space)
print("Sample observation", env.observation_space.sample()) # Get a random observation
```

会看到显示：

```bash
_____OBSERVATION SPACE_____ 

Observation Space Discrete(16)
Sample observation 2
```

observation 用一个整数来表示 Agent 当前的位置，计算如下：

![[pic-20250316145914722.png]]

所以 $4\times4$ 的网格被编号为 0～15

```python
print("\n _____ACTION SPACE_____ \n")
print("Action Space Shape", env.action_space.n)
print("Action Space Sample", env.action_space.sample()) # Take a random action
```

会看到显示：

```bash
 _____ACTION SPACE_____ 

Action Space Shape 4
Action Space Sample 2
```

action space（agent 可能采取的动作集合是离散的），有 4 个可用的动作：
- 0：向左
- 1：向下
- 2：向右
- 3：向上

Reward function 💰：
- 到达目标：+1
- 到达冰洞：0
- 到达普通冰面：0

### 创建并初始化 Q-table

![[pic-20250316151021540.png]]

要初始化 Q-table，我们需要知道有多少行和多少列，Q-table 的每一行对应一种状态，所以状态空间大小就是 Q-table 行数；Q-table 的每一列对应一种动作，所以动作空间大小就是 Q-table 列数。Gym 库提供了 `env.action_space.n` 和 `env.observation_space.n` 获取这两个信息：

```python
state_space = env.observation_space.n
print("There are ", state_space, " possible states")

action_space = env.action_space.n
print("There are ", action_space, " possible actions")
```

用 0 初始化 Q-table：

```python
# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros
def initialize_q_table(state_space, action_space):
  Qtable = np.zeros((state_space, action_space))
  return Qtable
```

```python
Qtable_frozenlake = initialize_q_table(state_space, action_space)
```

### 定义 greedy 策略

记住 Q-Learning 是一种 off-policy 算法，所以 Q-Learning 中会两种不同的策略，一种用于选择动作，一种用于更新价值函数：
- Epsilon-greedy policy (acting policy)
- Greedy-policy (updating policy)

greedy-policy 也是最终我们完成训练需要的 policy，它将告诉我们如何选择最优动作

![[pic-20250316151930410.png]]

```python
def greedy_policy(Qtable, state):
  # Exploitation: take the action with the highest state, action value
  action = np.argmax(Qtable[state][:])

  return action
```

### 定义 epsilon-greedy 策略

