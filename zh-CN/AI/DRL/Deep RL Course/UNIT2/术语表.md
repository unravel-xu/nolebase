# 术语表

## Strategies to find the optimal policy

- Policy-based methods：策略通常通过神经网络进行训练，以根据给定状态选择采取什么动作。在这种情况下，神经网络直接输出代理应采取的动作，而不是使用价值函数。根据环境反馈的经验，神经网络会重新调整并提供更好的动作。
- 
- 基于价值的方法
- 在这种情况下，训练一个价值函数来输出状态或状态-动作对的价值，这些价值将代表我们的策略。然而，这个价值并不定义代理应采取的具体动作。相反，我们需要根据价值函数的输出来指定代理的行为。例如，我们可以决定采用一种策略，始终选择能带来最大奖励的动作（贪婪策略）。总结来说，策略是一种贪婪策略（或用户决定的任何策略），它利用价值函数的值来决定采取的动作。
- 
- 在基于价值的方法中，我们可以找到两种主要策略：
- 
- 状态价值函数：对于每个状态，状态价值函数是代理从该状态开始并遵循策略直到结束时的预期回报。
- 
- 动作价值函数：与状态价值函数不同，动作价值函数计算每个状态和动作对的预期回报，假设代理从该状态开始，采取该动作，然后永远遵循策略。
- 
- ε-贪婪策略
- 这是强化学习中常用的策略，用于平衡探索与利用。
- 
- 以概率 1-ε 选择具有最高预期奖励的动作。
- 
- 以概率 ε 随机选择一个动作。
- 
- ε 通常随时间递减，以逐渐将重点转向利用。
- 
- 贪婪策略
- 
- 始终基于当前对环境的了解，选择预期能带来最高奖励的动作（仅利用）。
- 
- 总是选择具有最高预期奖励的动作。
- 
- 不包含任何探索。
- 
- 在具有不确定性或未知最优动作的环境中可能不利。
- 
- 离策略与在策略算法
- 
- 离策略算法：训练时和推理时使用不同的策略。
- 
- 在策略算法：训练和推理时使用相同的策略。
- 
- 蒙特卡洛与时序差分学习策略
- 
- 蒙特卡洛（MC）：在回合结束时学习。使用蒙特卡洛方法时，我们等待回合结束，然后根据完整的回合更新价值函数（或策略函数）。
- 
- 时序差分（TD）：在每一步学习。使用时序差分学习时，我们在每一步更新价值函数（或策略函数），而不需要完整的回合。