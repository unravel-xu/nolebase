# Q-Learning 实例

为了更好的理解 Q-learning 算法，我们举一个简单的例子：

![[pic-20250315182457831.png]]

- 假设有一只处在迷宫中的小老鼠，并总是从相同的起点开始出发
- 目标是吃掉右下角的大奶酪且不能吃到毒药
- 每轮探索，如果吃到毒药、吃掉大块奶酪或行动超过 5 步，探索结束
- learning rate = 0.1    discount rate(gamma) = 0.99

奖励函数如下：

- +0：去一个没有奶酪的状态
- +1：去一个有小奶酪的状态
- +10：去一个有一大堆奶酪的状态
- -10：去一个有毒药的状态且会死亡
- +0：如果我们执行超过五步

如下使用 Q-Learning 算法训练智能体，使其能够学会最优策略（依次执行向右、向右、向下）

## 第一步：初始化 Q-table

![[pic-20250315220528183.png]]

初始时，Q-table 没什么用，需用使用 Q-Learning 算法训练 Q-function

我们进行 2 个训练时间步骤：

## 训练时间步骤 1

### 第二步：使用 Epsilon Greedy 策略选择动作

初始 epsilon = 1，所以会采取随机动作，假设随机选择了向右的动作

![[pic-20250315221715732.png]]

### 第三步：执行动作 $A_{t}$，得到 $R_{t+1}$ 和 $S_{t+1}$

向右走得到一小块奶酪，所以 $R_{t+1}=1$，并进入新的状态

![[pic-20250315223024751.png]]

### 第四步：更新 $Q\left(S_{t},A_{t}\right)$

根据公式更新 $Q\left(S_{t},A_{t}\right)$：

![[pic-20250315223222539.png]]

![[pic-20250315224002800.png]]

## 训练时间步骤 2

### 第二步：使用 Epsilon Greedy 策略选择动作

Epsilon 会随着训练的进行逐渐减少，但只经过训练时间步骤 1，Epsilon 依然接近 1，此时极大概率再次随机选择一个动作

假设随机选择了向下的动作，但这是一个糟糕的动作，小老鼠吃到了毒药

![[pic-20250315225531393.png]]

### 第三步：执行动作 $A_{t}$，得到 $R_{t+1}$ 和 $S_{t+1}$

因为吃到了毒药，所以 $R_{t+1}=-10$ 且回合结束

![[pic-20250315225625389.png]]

### 第四步：更新 $Q\left(S_{t},A_{t}\right)$

![[pic-20250315225716547.png]]

小老鼠死亡，开启新的回合。但观察 Q-table 发现，经过两步探索，agent 变得更聪明了

随着智能体继续探索和利用环境，并使用 TD target 更新 Q-values，Q-table 中的近似值越来越好。因此，在训练结束时，我们将获得最优 Q-function 的近似估计