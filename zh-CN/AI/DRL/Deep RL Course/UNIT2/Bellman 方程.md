# Bellman 方程

Bellman 公式可以帮助我们简化 state value 或 state-action value 计算

根据目前所学，计算 $V(S_{t})$ (状态 $S_{t}$ 的价值)，我们需要计算从该状态开始一直遵循某个策略行动的回报（这里为简化示例，我们假设采用贪心策略，且奖励不进行折扣）

![[pic-20250314141428296.png]]

如下是计算期望回报，也就是计算累计奖励的期望：

![[pic-20250314141337389.png]]
计算状态 $S_{t}$ 的价值：智能体从该状态开始，并在之后的时间步中遵循贪心策略（采取能够获取最佳状态值的行动），把每一步的奖励进行累加

计算 $V(S_{t+1})$：

![[pic-20250314145030728.png]]

我们发现 $V(S_{t})$ 和 $V(S_{t+1})$ 一旦轨迹很长计算就会很繁琐，且计算过程中有很多部分是重复的

类似用动态规划（DP）求 fibonacci 数的思路，我们可以使用 Bellman 方程简化 value 求解，Bellman 方程是一个递归方程，将每一个状态的value 都视为：

immediate reward $R_{t+1}$  + 下一个 state 的 discounted value $\gamma\cdot V(S_{t+1})$

![[pic-20250314145838170.png]]

回到上例计算过程，假设 $\gamma=1$：

![[pic-20250314150040062.png]]

在后续 Q-Learning 算法中，会有 $\gamma=0.99$ 的情形，这里我们只是为展示原理