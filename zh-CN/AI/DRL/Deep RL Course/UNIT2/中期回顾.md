# 中期回顾

在深入学习 Q-Learning 前，我们先总结下我们学习了什么

我们学习了两种 value-based 的函数：

- State-value function：Agent 从某一个状态 s 开始，之后每一步行动都按照策略 $\pi$ 执行
- Action-value function：Agent 从某一个状态 s 开始，先随便执行一个行动 a（有可能不按策略），之后每一步都按照固定的策略 $\pi$ 执行 action
- Value-based 的方法相比于 Policy-based 的方法：我们手动定义 policy 并学习 value function，当我们得到最优 value  function 也就得到了最优 policy

两种更新 value function 的方法：

- Monte Carlo 方法：通过完整的一个 episode 更新 value function，使用的是真实准确的回报
- TD Learning 方法：通过一次交互更新value function，使用的是 TD target 的估计回报

![[pic-20250315105638983.png]]

下一小节 Mid-way Quiz 很简单，略