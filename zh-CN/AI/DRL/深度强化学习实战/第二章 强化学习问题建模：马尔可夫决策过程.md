## 2.1 线图和本书的教学方法

![[pic-20250227125939746.png]]

神经网络的单层线图：表示神经网络层接受一个长度或维度为 n 的向量，然后产生一个维度为 m 的向量

## 2.2 解决多臂老虎机问题

假设面前有 10 台老虎机，每台老虎机保证会给你 0～10 美元的奖励，但这 10 台老虎机的平均奖励各不相同。现在，来尝试找出哪台老虎机平均奖励最高

通俗称这类问题为 n 臂老虎机问题，其中 n 是老虎机的数量

我们有 n 个可能的动作（这里 n = 10）​，其中动作意味着拉动一台特定老虎机的臂（杠杆）​，在该游戏的每一轮（轮次 k）中，我们可以选择拉动单个杠杆。在采取动作之后，我们将获得一个奖励 $R_{k}$（轮次 k 的奖励）​。每个杠杆有唯一的奖励概率分布，例如，如果有 10 台老虎机，你玩了很多轮游戏，那么 3 号老虎机的平均奖励是 9 美元，而 1 号老虎机的平均奖励为 4 美元。当然，由于每一轮游戏的奖励是随机出现的，因此 1 号杠杆有可能在某次游戏中给出 9 美元的奖励。但如果进行很多轮游戏，我们就会发现 1 号老虎机的平均奖励低于 3 号老虎机

我们的策略应该是玩几轮游戏，选择不同的杠杆并观察每个动作的奖励，然后只选择观察到的平均奖励最高的杠杆。因此，我们希望获悉基于之前轮次采取动作的期望奖励

这个期望奖励 $Q_{k}(a)$ 定义为：向函数提供一个动作 a（假设当前处于轮次 k ），它返回采取该动作时的期望奖励

$$
Q_{k}(a)=\frac{R_{1}+R_{2}+\dots+R_{k}}{k_{a}}
$$

也就是说轮次 k 中动作 a 的期望奖励为之前采取动作 a 时收到的所有奖励的算术平均值

称函数 $Q_{k}(a)$ 为价值函数，用于表示某物的价值，具体来说，它是一个 `动作-价值` 函数，用于表示采取特定动作的价值，通常用符号 Q 来表示这个函数，也称为 Q 函数

### 2.2.1 探索与利用

探索：第一次开始玩游戏时，需要进行游戏并观察从各台老虎机上获得的奖励

利用：使用现在已知的知识，并一直玩可能产生最高奖励的老虎机

总体策略涉及一定数量的利用（基于目前所知选择最佳杠杆）和一定数量的探索（随机选择杠杆来学习更多的知识）。利用和探索之间的合理平衡对于最大化奖励非常重要

最简单的算法就是选择最高 Q 值所对应的动作

$$
\forall a_{i} \in A_{k} \ ,\ a^* = argmax_{a}Q_{k}(a_{i})
$$
$A_{k}$：可采取的动作空间

$a^*$：要采取的动作

$Q_{k}(a_{i})$：采取动作 $a_{i}$ 的价值

```python
def get_best_action(actions):
    best_action=0
    max_action_value=0
    for i in range(len(actions)): 　　# ←---　循环遍历所有可能的动作
        cur_action_value=get_action_value(actions[i]) 　# ←---　获取当前动作的价值
        if cur_action_value > max_action_value:
            best_action=i
            max_action_value=cur_action_value
    return best_action
```

$Q_{k}(a)$ 依赖于之前动作以其对应的奖励，所以该方法不会评估尚未探索的动作。因此，我们之前可能已经尝试过 1 号和 3 号杠杆，并观察到 3 号杠杆返回更高的奖励，但利用该方法，我们将永远不会想到尝试另一个不为我们所知的杠杆，例如 6 号杠杆，而它实际上提供了最高的平均奖励。这种简单地选择目前为止所知的最佳杠杆的方法称为贪婪（或利用）方法

### 2.2.2 $\varepsilon$ 贪婪策略    
在上述算法基础上根据概率 $\varepsilon$ 随机选择一个动作 a，而其他时间（概率 $1-\varepsilon$）将基于当前从过去游戏中所知的信息选择最佳杠杆。大多数时候，我们会以贪婪方式玩游戏，但有时也会冒险随机选择杠杆来查看会发生什么

```python
import numpy as np
from scipy import stats
import random
import matplotlib.pyplot as plt
　
n=10　　# ←---　臂的数量
probs=np.random.rand(n) 　　# ←---　每个臂关联的隐藏概率
eps=0.2　　# ←---　用于ε贪婪动作选择的ε
```

数组 probs 中的每个位置对应一台老虎机，表示一个可能的概率。例如，第一个元素的索引位置为 0，所以动作 0 就是选择第一台老虎机。每台老虎机（每个臂）有一个关联的概率，用来衡量它能得到的奖励

选择以下方式来为每个臂实现奖励概率分布。每个臂有一个概率（例如 0.7），并且最高奖励是 10 美元。设置一个遍历到 10 的 for 循环，在每一步中，如果随机浮点数小于臂的概率，那么将奖励加 1。因此，在第一次循环中，生成了一个随机浮点数（例如 0.4）​。0.4 < 0.7，因此 reward+=1。在下一次迭代中，它将生成另一个也小于 0.7 的随机浮点数（例如 0.6）​，所以 reward+=1。这种情况将一直持续，直到完成 10 次迭代，然后返回最终的总奖励。总奖励可以是 0 和 10 之间的任意值。在臂的概率为 0.7 的情况下，这样无限玩下去的平均奖励将是 7，但在任何一轮游戏中，这个值可大可小。

```python
def get_reward(prob, itr=10):
	reward = 0
	for i in range(itr):
		if random.random() < prob:
			reward += 1
	return reward
```

```stylus
# 以概率0.7运行这段代码2000次，最终返回的平均奖励值接近7
>>> np.mean([get_reward(0.7) for _ in range(2000)])
7.001
```

我们要定义的下一个函数是选择目前为止最好的臂的贪婪策略。我们需要一种方法来追踪拉动了哪个臂，以及由此得到的奖励是什么。简单来说，我们可以使用一个列表，并向其添加像（臂,奖励）这样的观察值，例如，(2, 9) 表示选择了 2 号臂并获得了奖励 9。随着游戏的进行，这个列表会越来越长

由于只需要记录每个臂的平均奖励，并不需要存储每个观察结果，因此还有一种更简单的方法。回想一下，要计算一组数字 $x_{i}$ 的平均值，只需将所有 $x_{i}$ 相加，然后除以 $x_{i}$ 的数量（用 k 来表示）​。平均值通常用希腊字母 $\mu$ 来表示：$\mu = \frac{1}{k}\sum_{i}x_{i}$ ，在获取一个新的奖励时，$\mu_{new}=\frac{k\cdot \mu_{old} + x}{k+1}$

这样只需要跟踪每个臂的两个数学量：k（观察值的数量）和 $\mu$（当前运行的平均值）​。我们可以将它存储在一个 10×2 的 NumPy 数组中（假设有 10 个臂）​，并将该数组称为 record

```stylus
>>> record=np.zeros((n,2))
array([[0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.]])
```

record 数组的第一列存储每个臂被拉动的次数，第二列存储由此得到的平均奖励

```python
def update_record(record, action, reward):
	new_reward=(record[action, 0] * record[action, 1] + reward) / (record[action, 0] + 1)
	record[action, 0]+=1
	record[action, 1]=new_reward
	return record
```

接下来我们需要实现一个函数来选择要拉动哪个臂。我们希望能选择与最高平均奖励关联的臂，所以需要找到数组 record 中第二列取最大值所对应的索引：

```python
def get_best_arm(record):
    arm_index=np.argmax(record[:,1],axis=0) #←---　对数组record的第二列调用NumPy的argmax函数
    return arm_index
```

现在我们可以进入多臂老虎机游戏的主循环了。如果一个随机数大于 $\varepsilon$ 参数，那么只需使用 get_best_arm 函数计算最佳动作并采取该动作；否则，便会采取一个随机动作，以确保一定次数的探索。选择臂之后，执行 get_reward 函数并观察奖励值，然后使用这个新的观察值更新数组 record。多次重复该过程，数组 record 将不断更新。最终，有最高奖励概率的臂会被选择最多次，因为它提供最高的平均奖励

```python
fig, ax=plt.subplots(1,1)
ax.set_xlabel("Plays")
　
ax.set_ylabel("Avg Reward")
record=np.zeros((n,2))	←---　将数组record元素初始化为0
probs=np.random.rand(n)	←---　随机初始化每个臂的奖励概率
eps=0.2
rewards=[0]				←--- 记录每轮循环获得的平均reward 　　
for i in range(500):
    if random.random() > eps: 　　←---　以概率0.8选择最佳动作，也可以随机选择
        choice=get_best_arm(record)
    else:
        choice=np.random.randint(10)
    r=get_reward(probs[choice]) 　　←---　计算选择臂的奖励
    record=update_record(record,choice,r) 　　←---　利用新数量和臂的奖励观察值更新数组record
    mean_reward=((i+1) * rewards[-1]+r)/(i+2) 　　←---　跟踪运行的平均奖励来评估整体表现
    rewards.append(mean_reward)
ax.scatter(np.arange(len(rewards)),rewards)
```

![[pic-20250227172814682.png]]

从上图可以看出，经过很多轮次，平均奖励确实有所提高，这说明算法确实在学习，通过前面好的游戏轮次逐渐得到了强化，不过它仍然是一个比较简单的算法

这里我们考虑的问题是一个**平稳**问题，因为臂的潜在奖励概率分布**不会随着时间发生变化**。当然，我们可以考虑这个问题的一个变体——非平稳问题。在这种情况下，一种简单的修改方法是允许新的奖励观察值以一种倾斜的方式更新存储在记录中的平均奖励值，这样它将是一个加权平均值，朝着最新的观察值倾斜。如果事情随着时间发生变化，从某种程度上讲，我们就能跟踪它们

### 2.2.3 Softmax 选择策略

来看另一种类型的“老虎机”问题：一位新来的医生专门治疗心脏病患者。她有 10 种治疗方案，但针对每位病人她只能选择其中 1 种来进行治疗。出于某种原因，她只知道这 10 种治疗方案对治疗心脏病有不同的疗效和风险，但却不知道哪一种是最好的。此处我们可以使用前面方案中的多臂老虎机算法，但可能需要重新考虑每次随机选择治疗方案的 $\varepsilon$ 贪婪策略。在这个新问题中，随机选择一种治疗方案可能会导致病人死亡，而不仅仅是损失钱。我们想确保不会选择最糟糕的治疗方案，但仍然希望有能力探索以找到最好的治疗方案   

这时候使用 Softmax 可能是最合适的，在探索期间，Softmax 提供来各个选项的概率分布，而非随机选择一个动作。概率最大的选项将等同于前面解决方案中的最佳动作，它也会提供一些关于次佳动作和次次佳动作的信息。这样我们就可以随机选择并探索其他选项，同时避免选择最差的选项——因为它们的概率会很小，甚至为 0

$$
Pr(A)=\frac{e^{Q_{k}(A)/\tau}}{\sum_{i=1}^n e^{Q_k(i)/\tau}}
$$
Pr(A)接收一个动作-价值向量（数组），并返回一个动作的概率分布（数组），这样更高值的动作具有更高的概率。例如，如果动作-价值数组有 4 个可能的动作，并且它们目前都有相同的值，例如 `A=[10,10,10,10]` ​，那么 `Pr(A)=[0.25,0.25,0.25,0.25]` ​。换句话说，所有概率相同，且总和必须为 1

分数的分子是动作-价值数组除以参数 $\tau$ 后取指数幂，生成一个与输入相同大小（长度）的向量。分母是对每个动作-价值除以 $\tau$ 后取指数幂求和，从而产生单个数值

$\tau$ 是温度参数，用于**对动作的概率分布进行缩放**。较高的温度会导致概率非常接近，而较低的温度则会增大动作概率之间的差异。为这个参数选择一个值需要据理推测和进行试错

```python

# softmax函数
def softmax(actionValue, tau=1.12):
	softm = np.exp(actionValue/tau) / np.sum(np.exp(actionValue/tau))
	return softm 
```

在用 softmax 实现之前的 10 臂老虎机问题时，我们不再需要 get_best_arm 函数。Softmax 生成了可能动作的加权概率分布，所以我们可以根据动作对应的概率随机选择动作

对数组 record 的第二列（列索引为 1）应用 softmax 函数，因为这一列存储了每个动作当前的平均奖励（动作-价值）​。它会把这些动作-价值转化为概率。然后，我们使用 np.random.choice 函数。该函数接收一个任意的输入数组 x 和一个参数 p，其中 p 是一个对应于 x 中各个元素的概率数组。由于记录被初始化为 0，因此 softmax 起初将返回一个在所有老虎机上的均匀分布，但该分布很快就会倾向于最高奖励关联的动作

```stylus
>>> x=np.arange(10)
>>> x
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
>>> av=np.zeros(10)
>>> p=softmax(av)
>>> p
array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])
>>> np.random.choice(x,p=p)
3
```

如下是多臂老虎机的 Softmax 动作选择：

```python
n=10
probs=np.random.rand(n)
record=np.zeros((n,2))
fig,ax=plt.subplots(1,1)
ax.set_xlabel("Plays")
ax.set_ylabel("Avg Reward")
fig.set_size_inches(9,5)
rewards=[0]
for i in range(500):
    p=softmax(record[:,1]) 　　←---　计算每个臂对应于其当前动作-价值的Softmax概率
    choice=np.random.choice(np.arange(n),p=p) 　　←---　通过Softmax概率加权随机选择一个臂
    r=get_reward(probs[choice])
    record=update_record(record,choice,r)
    mean_reward=((i+1) * rewards[-1]+r)/(i+2)
    rewards.append(mean_reward)
ax.scatter(np.arange(len(rewards)),rewards)
```

![[pic-20250227191833915.png]]

在该问题上 Softmax 动作选择似乎比 $\varepsilon$ 贪婪算法做得更好，似乎更快地收敛到一个最优策略。然而，Softmax 的缺点就是必须手动选择 $\tau$ 参数，这里的 Softmax 对 $\tau$ 非常敏感，而且需要一些时间来找到一个较好的值。虽然利用 $\varepsilon$ 贪婪算法我们也必须设置参数 $\varepsilon$，但选择 $\varepsilon$ 更加直观

## 2.3 应用老虎机算法优化广告投放

假设有 10 个电子商务网站，它们各自销售不同类别的零售商品，例如计算机、鞋子、珠宝等。我们希望把在其中一个网站上购物的顾客引向他们可能感兴趣的另一个网站，进而实现销量的提升。当一位顾客在其中某个网站结账时，我们就会展示另一个网站的广告，以引导他去那里购买别的东西。我们也可以在同一网站上放置其他产品的广告。然而，问题是不知道应该把顾客引导到哪些网站。我们可以尝试随机投放广告，但可能存在一种更有针对性的方法

### 2.3.1 上下文老虎机

在游戏的每个轮次（每次一位顾客在某个网站结账）​，我们有 n=10 种可能的动作，对应着可以投放的 10 种不同类型的广告。关键是最佳的投放广告可能取决于当前顾客处于哪个网站。例如，在珠宝网站结账的顾客可能更愿意购买一双鞋来搭配新钻石项链，而不是购买一台新的笔记本电脑。因此，问题就变成了“找出特定网站如何与特定广告关联”​。这就引出了**状态空间(state space)** 的概念。我们提出的多臂老虎机问题有一个包含多个元素的动作空间（所有可能动作的集合）​，但是并没有“状态”的概念。也就是说，环境中没有任何信息可以帮助我们选择一个好的臂。我们确定哪个臂比较好的唯一方法就是反复试错。在广告投放问题中，我们知道顾客正在某个特定网站上购物，这可能会提供一些有关用户的偏好信息，并有助于指导我们决定投放哪个广告。我们将这种**上下文信息**称为一种状态，并将这种新类型的问题称为**上下文老虎机**

![[pic-20250227223428337.png]]

### 2.3.2 状态、动作和奖励

巩固第一章的概念

状态：指环境中可用来做出决策的信息集

强化学习算法模型世界好像仅涉及一组状态 S（状态空间，一组关于环境的特征）​、一组动作 A（动作空间，可以输入某个给定状态）和奖励 r（在特定状态下采取某个动作时产生）​。当说起在特定状态下采取特定动作时，我们通常称之为状态-动作对 `(s, a)`

> [!important]
> 任何强化学习算法的目标都是在整个事件过程中最大化奖励

在最初的多臂老虎机问题中没有状态空间，而只有动作空间，因此我们只需要了解动作和奖励之间的关系，可以使用一个查找表来存储从特定动作中获得奖励的经验来了解这种关系，正如我们存储了动作-奖励对 $(a_k, r_k)$，其中轮次 k 的奖励是与采取动作 $a_k$ 相关的过去所有轮次下的奖励均值

在多臂老虎机问题中只有 10 个动作，所以 10 行的查找表是非常合理的。但在上下文老虎机问题中引入状态空间时，我们开始会得到可能的状态-动作-奖励元组的组合性爆炸问题。例如，如果有一个包含 100 个状态的状态空间，并且每个状态与 10 个动作关联，那么需要存储和重新计算 1000 个不同的数据片段

深度学习在这里就有了用武之地。经过合适的训练，神经网络会变得很擅长学习抽象信息，摒弃没有价值的细节。它们可以学习数据中的组合模式和规律，从而能够在**有效压缩大量数据的同时保留重要信息**。因此，神经网络可以用来学习状态-动作对和奖励之间的复杂关系，而不必将所有这些经验作为原始记忆存储。通常，我们将强化学习算法中基于某些信息做出决策的部分称为智能体。为了解决所讨论的上下文老虎机问题，我们用一个神经网络作为智能体

## 2.4 利用 PyTorch 构建网络

利用 PyTorch 实例化 2×3 矩阵

```stylus
>>> import torch
　
>>> torch.Tensor([[1, 2, 3], [4, 5, 6]])
1 2 3
4 5 6
[torch.FloatTensor of size 2x3]
```

PyTorch 中把多维数组称为张量(tensor)。这也是 TensorFlow 和其他框架中使用的术语，所以我们按照约定俗成的名称，将多维数组称为张量。我们可以参考**张量的阶**，也就是张量有多少索引维度。这有点儿令人费解，因为说到向量的维度时，我们指的是向量的长度，但是说到张量的阶时，我们指的是它有多少个索引

### 2.4.1 自动微分

相比 NumPy，PyTorch 提供的重要特性是自动微分和优化。假设创建一个简单线性模型，用来预测一些感兴趣的数据：

```stylus
>>> x=torch.Tensor([2,4]) #input data
>>> m=torch.randn(2, requires_grad=True) #parameter 1
>>> b=torch.randn(1, requires_grad=True) #parameter 2
>>> y=m*x+b #linear model
>>> loss=(torch.sum(y_known - y))**2 #loss function
>>> loss.backward() #calculate gradients
>>> m.grad
tensor([ 0.7734, -90.4993])
```

只需要向计算梯度的PyTorch张量提供参数 `requires_grad=True`，然后在计算图的最后一个节点上调用backward方法——它将向所有具有 `requires_grad=True` 的节点反向传播梯度。接下来，我们可以利用自动计算的梯度实现梯度下降操作

### 2.4.2 构建模型

我们不直接处理自动计算的梯度，而会用 PyTorch 的 nn 模块轻松创建一个前馈神经网络模型，然后用内置的优化算法来自动训练神经网络，这样就不必手动指定反向传播和梯度下降机制了。下面是一个创建了优化器的简单两层神经网络：

```python
model=torch.nn.Sequential(
    torch.nn.Linear(10, 150),
    torch.nn.ReLU(),
    torch.nn.Linear(150, 4),
    torch.nn.ReLU(),
)
　
loss_fn=torch.nn.MSELoss()
optimizer=torch.optim.Adam(model.parameters(), lr=0.01)
```

我们创建了一个包含 ReLU 激活函数的两层模型，定义了一个均方误差损失函数，并创建了一个优化器。如果我们有一些标记好的训练数据，那么要训练该模型，只需启动一个训练循环：

```python
for step in range(100):
    y_pred=model(x)
    loss=loss_fn(y_pred, y_correct)
    optimizer.zero_grad()	# 清除梯度
    loss.backward()			# 反向传播
    optimizer.step()		# 更新参数
```

变量 x 是模型的输入数据。变量 y_correct 是一个代表标记的正确输出的张量。我们使用模型进行预测与计算损失，然后在计算图（通常为 loss 函数）的最后一个节点上使用 backward 方法计算梯度。接着在优化器上运行 step 方法，以执行一个梯度下降步骤。如果需要构建比顺序模型更复杂的神经网络架构，那么可以通过继承 PyTorch 的模块类来编写自己的 Python 类并使用：

```python
from torch.nn import Module, Linear
　
class MyNet(Module):
   def __init__(self):
       super(MyNet, self).__init__()
       self.fc1=Linear(784, 50)
       self.fc2=Linear(50, 10)
　
   def forward(self, x):
       x=F.relu(self.fc1(x))
       x=F.relu(self.fc2(x))
       return x
　
model=MyNet()
```

## 2.5 解决上下文老虎机问题

我们已经为上下文老虎机创建了一个模拟环境，其中模拟器包括状态（一个 0～9 的数字代表 10 个网站中的其中一个）​、奖励生成（广告点击）和选择动作的方法（10 个广告中的哪一个提供服务）

```python
class ContextBandit:
    def __init__(self, arms=10):
        self.arms=arms
        self.init_distribution(arms)
        self.update_state()
　
        def init_distribution(self, arms): 　　←---　为简单起见，状态的数量等于臂的数量。每一行代表一个状态，每一列代表一个臂
            self.bandit_matrix=np.random.rand(arms,arms)
　
        def reward(self, prob):
            reward=0
            for i in range(self.arms):
                if random.random() < prob:
                    reward+=1
            return reward
        def get_state(self):
            return self.state
　
        def update_state(self):
            self.state=np.random.randint(0,self.arms)
　
        def get_reward(self,arm):
            return self.reward(self.bandit_matrix[self.get_state()][arm])
　
        def choose_arm(self, arm): 　　←---　选择一个臂，返回一个奖励并更新状态
            reward=self.get_reward(arm)
            self.update_state()
            return reward
```​