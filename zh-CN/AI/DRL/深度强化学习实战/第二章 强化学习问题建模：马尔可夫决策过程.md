## 2.1 线图和本书的教学方法

![[pic-20250227125939746.png]]

神经网络的单层线图：表示神经网络层接受一个长度或维度为 n 的向量，然后产生一个维度为 m 的向量

## 2.2 解决多臂老虎机问题

假设面前有 10 台老虎机，每台老虎机保证会给你 0～10 美元的奖励，但这 10 台老虎机的平均奖励各不相同。现在，来尝试找出哪台老虎机平均奖励最高

通俗称这类问题为 n 臂老虎机问题，其中 n 是老虎机的数量

我们有 n 个可能的动作（这里 n = 10）​，其中动作意味着拉动一台特定老虎机的臂（杠杆）​，在该游戏的每一轮（轮次 k）中，我们可以选择拉动单个杠杆。在采取动作之后，我们将获得一个奖励 $R_{k}$（轮次 k 的奖励）​。每个杠杆有唯一的奖励概率分布，例如，如果有 10 台老虎机，你玩了很多轮游戏，那么 3 号老虎机的平均奖励是 9 美元，而 1 号老虎机的平均奖励为 4 美元。当然，由于每一轮游戏的奖励是随机出现的，因此 1 号杠杆有可能在某次游戏中给出 9 美元的奖励。但如果进行很多轮游戏，我们就会发现 1 号老虎机的平均奖励低于 3 号老虎机

我们的策略应该是玩几轮游戏，选择不同的杠杆并观察每个动作的奖励，然后只选择观察到的平均奖励最高的杠杆。因此，我们希望获悉基于之前轮次采取动作的期望奖励

这个期望奖励 $Q_{k}(a)$ 定义为：向函数提供一个动作 a（假设当前处于轮次 k ），它返回采取该动作时的期望奖励

$$
Q_{k}(a)=\frac{R_{1}+R_{2}+\dots+R_{k}}{k_{a}}
$$

也就是说轮次 k 中动作 a 的期望奖励为之前采取动作 a 时收到的所有奖励的算术平均值

称函数 $Q_{k}(a)$ 为价值函数，用于表示某物的价值，具体来说，它是一个 `动作-价值` 函数，用于表示采取特定动作的价值，通常用符号 Q 来表示这个函数，也称为 Q 函数

### 2.2.1 探索与利用

探索：第一次开始玩游戏时，需要进行游戏并观察从各台老虎机上获得的奖励

利用：使用现在已知的知识，并一直玩可能产生最高奖励的老虎机

总体策略涉及一定数量的利用（基于目前所知选择最佳杠杆）和一定数量的探索（随机选择杠杆来学习更多的知识）。利用和探索之间的合理平衡对于最大化奖励非常重要

最简单的算法就是选择最高 Q 值所对应的动作

$$
\forall a_{i} \in A_{k} \ ,\ a^* = argmax_{a}Q_{k}(a_{i})
$$
$A_{k}$：可采取的动作空间

$a^*$：要采取的动作

$Q_{k}(a_{i})$：采取动作 $a_{i}$ 的价值

```python
def get_best_action(actions):
    best_action=0
    max_action_value=0
    for i in range(len(actions)): 　　# ←---　循环遍历所有可能的动作
        cur_action_value=get_action_value(actions[i]) 　# ←---　获取当前动作的价值
        if cur_action_value > max_action_value:
            best_action=i
            max_action_value=cur_action_value
    return best_action
```

$Q_{k}(a)$ 依赖于之前动作以其对应的奖励，所以该方法不会评估尚未探索的动作。因此，我们之前可能已经尝试过 1 号和 3 号杠杆，并观察到 3 号杠杆返回更高的奖励，但利用该方法，我们将永远不会想到尝试另一个不为我们所知的杠杆，例如 6 号杠杆，而它实际上提供了最高的平均奖励。这种简单地选择目前为止所知的最佳杠杆的方法称为贪婪（或利用）方法

### 2.2.2 $\varepsilon$ 贪婪策略    
在上述算法基础上根据概率 $\varepsilon$ 随机选择一个动作 a，而其他时间（概率 $1-\varepsilon$）将基于当前从过去游戏中所知的信息选择最佳杠杆。大多数时候，我们会以贪婪方式玩游戏，但有时也会冒险随机选择杠杆来查看会发生什么

```python
import numpy as np
from scipy import stats
import random
import matplotlib.pyplot as plt
　
n=10　　# ←---　臂的数量
probs=np.random.rand(n) 　　# ←---　每个臂关联的隐藏概率
eps=0.2　　# ←---　用于ε贪婪动作选择的ε
```

数组 probs 中的每个位置对应一台老虎机，表示一个可能的概率。例如，第一个元素的索引位置为 0，所以动作 0 就是选择第一台老虎机。每台老虎机（每个臂）有一个关联的概率，用来衡量它能得到的奖励

选择以下方式来为每个臂实现奖励概率分布。每个臂有一个概率（例如 0.7），并且最高奖励是 10 美元。设置一个遍历到 10 的 for 循环，在每一步中，如果随机浮点数小于臂的概率，那么将奖励加 1。因此，在第一次循环中，生成了一个随机浮点数（例如 0.4）​。0.4 < 0.7，因此 reward+=1。在下一次迭代中，它将生成另一个也小于 0.7 的随机浮点数（例如 0.6）​，所以 reward+=1。这种情况将一直持续，直到完成 10 次迭代，然后返回最终的总奖励。总奖励可以是 0 和 10 之间的任意值。在臂的概率为 0.7 的情况下，这样无限玩下去的平均奖励将是 7，但在任何一轮游戏中，这个值可大可小。

```python
def get_reward(prob, itr=10):
	reward = 0
	for i in range(itr):
		if random.random() < prob:
			reward += 1
	return reward
```

```bash
# 以概率0.7运行这段代码2000次，最终返回的平均奖励值接近7
>>> np.mean([get_reward(0.7) for _ in range(2000)])
7.001
```

我们要定义的下一个函数是选择目前为止最好的臂的贪婪策略。我们需要一种方法来追踪拉动了哪个臂，以及由此得到的奖励是什么。简单来说，我们可以使用一个列表，并向其添加像（臂,奖励）这样的观察值，例如，(2, 9) 表示选择了 2 号臂并获得了奖励 9。随着游戏的进行，这个列表会越来越长

由于只需要记录每个臂的平均奖励，并不需要存储每个观察结果，因此还有一种更简单的方法。回想一下，要计算一组数字 $x_{i}$ 的平均值，只需将所有 $x_{i}$ 相加，然后除以 $x_{i}$ 的数量（用 k 来表示）​。平均值通常用希腊字母 $\mu$ 来表示：$\mu = \frac{1}{k}\sum_{i}x_{i}$ ，在获取一个新的奖励时，$\mu_{new}=\frac{k\cdot \mu_{old} + x}{k+1}$

这样只需要跟踪每个臂的两个数学量：k（观察值的数量）和 $\mu$（当前运行的平均值）​。我们可以将它存储在一个 10×2 的 NumPy 数组中（假设有 10 个臂）​，并将该数组称为 record

```bash
>>> record=np.zeros((n,2))
array([[0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.]])
```

record 数组的第一列存储每个臂被拉动的次数，第二列存储由此得到的平均奖励

```python
def update_record(record, action, reward):
	new_reward=(record[action, 0] * record[action, 1] + reward) / (record[action, 0] + 1)
	record[action, 0]+=1
	record[action, 1]=new_reward
	return record
```

接下来我们需要实现一个函数来选择要拉动哪个臂。我们希望能选择与最高平均奖励关联的臂，所以需要找到数组 record 中第二列取最大值所对应的索引：

```python
def get_best_arm(record):
    arm_index=np.argmax(record[:,1],axis=0) #←---　对数组record的第二列调用NumPy的argmax函数
    return arm_index
```

现在我们可以进入多臂老虎机游戏的主循环了。如果一个随机数大于 $\varepsilon$ 参数，那么只需使用 get_best_arm 函数计算最佳动作并采取该动作；否则，便会采取一个随机动作，以确保一定次数的探索。选择臂之后，执行 get_reward 函数并观察奖励值，然后使用这个新的观察值更新数组 record。多次重复该过程，数组 record 将不断更新。最终，有最高奖励概率的臂会被选择最多次，因为它提供最高的平均奖励

```python
fig, ax=plt.subplots(1,1)
ax.set_xlabel("Plays")
　
ax.set_ylabel("Avg Reward")
record=np.zeros((n,2))	←---　将数组record元素初始化为0
probs=np.random.rand(n)	←---　随机初始化每个臂的奖励概率
eps=0.2
rewards=[0]				←--- 记录每轮循环获得的平均reward 　　
for i in range(500):
    if random.random() > eps: 　　←---　以概率0.8选择最佳动作，也可以随机选择
        choice=get_best_arm(record)
    else:
        choice=np.random.randint(10)
    r=get_reward(probs[choice]) 　　←---　计算选择臂的奖励
    record=update_record(record,choice,r) 　　←---　利用新数量和臂的奖励观察值更新数组record
    mean_reward=((i+1) * rewards[-1]+r)/(i+2) 　　←---　跟踪运行的平均奖励来评估整体表现
    rewards.append(mean_reward)
ax.scatter(np.arange(len(rewards)),rewards)
```

![[pic-20250227172814682.png]]

从上图可以看出，经过很多轮次，平均奖励确实有所提高，这说明算法确实在学习，通过前面好的游戏轮次逐渐得到了强化，不过它仍然是一个比较简单的算法

这里我们考虑的问题是一个**平稳**问题，因为臂的潜在奖励概率分布**不会随着时间发生变化**。当然，我们可以考虑这个问题的一个变体——非平稳问题。在这种情况下，一种简单的修改方法是允许新的奖励观察值以一种倾斜的方式更新存储在记录中的平均奖励值，这样它将是一个加权平均值，朝着最新的观察值倾斜。如果事情随着时间发生变化，从某种程度上讲，我们就能跟踪它们

### 2.2.3 Softmax 选择策略

来看另一种类型的“老虎机”问题：一位新来的医生专门治疗心脏病患者。她有 10 种治疗方案，但针对每位病人她只能选择其中 1 种来进行治疗。出于某种原因，她只知道这 10 种治疗方案对治疗心脏病有不同的疗效和风险，但却不知道哪一种是最好的。此处我们可以使用前面方案中的多臂老虎机算法，但可能需要重新考虑每次随机选择治疗方案的 $\varepsilon$ 贪婪策略。在这个新问题中，随机选择一种治疗方案可能会导致病人死亡，而不仅仅是损失钱。我们想确保不会选择最糟糕的治疗方案，但仍然希望有能力探索以找到最好的治疗方案   

这时候使用 Softmax 可能是最合适的，在探索期间，Softmax 提供来各个选项的概率分布，而非随机选择一个动作。概率最大的选项将等同于前面解决方案中的最佳动作，它也会提供一些关于次佳动作和次次佳动作的信息。这样我们就可以随机选择并探索其他选项，同时避免选择最差的选项——因为它们的概率会很小，甚至为 0

$$
Pr(A)=\frac{e^{Q_{k}(A)/\tau}}{\sum_{i=1}^n e^{Q_k(i)/\tau}}
$$
Pr(A)接收一个动作-价值向量（数组），并返回一个动作的概率分布（数组），这样更高值的动作具有更高的概率。例如，如果动作-价值数组有 4 个可能的动作，并且它们目前都有相同的值，例如 `A=[10,10,10,10]` ​，那么 `Pr(A)=[0.25,0.25,0.25,0.25]` ​。换句话说，所有概率相同，且总和必须为 1

分数的分子是动作-价值数组除以参数 $\tau$ 后取指数幂，生成一个与输入相同大小（长度）的向量。分母是对每个动作-价值除以 $\tau$ 后取指数幂求和，从而产生单个数值

$\tau$ 是温度参数，用于**对动作的概率分布进行缩放**。较高的温度会导致概率非常接近，而较低的温度则会增大动作概率之间的差异。为这个参数选择一个值需要据理推测和进行试错

```python

# softmax函数
def softmax(actionValue, tau=1.12):
	softm = np.exp(actionValue/tau) / np.sum(np.exp(actionValue/tau))
	return softm 
```

在用 softmax 实现之前的 10 臂老虎机问题时，我们不再需要 get_best_arm 函数。Softmax 生成了可能动作的加权概率分布，所以我们可以根据动作对应的概率随机选择动作

对数组 record 的第二列（列索引为 1）应用 softmax 函数，因为这一列存储了每个动作当前的平均奖励（动作-价值）​。它会把这些动作-价值转化为概率。然后，我们使用 np.random.choice 函数。该函数接收一个任意的输入数组 x 和一个参数 p，其中 p 是一个对应于 x 中各个元素的概率数组。由于记录被初始化为 0，因此 softmax 起初将返回一个在所有老虎机上的均匀分布，但该分布很快就会倾向于最高奖励关联的动作

```bash
>>> x=np.arange(10)
>>> x
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
>>> av=np.zeros(10)
>>> p=softmax(av)
>>> p
array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])
>>> np.random.choice(x,p=p)
3
```

如下是多臂老虎机的 Softmax 动作选择：

```python
n=10
probs=np.random.rand(n)
record=np.zeros((n,2))
fig,ax=plt.subplots(1,1)
ax.set_xlabel("Plays")
ax.set_ylabel("Avg Reward")
fig.set_size_inches(9,5)
rewards=[0]
for i in range(500):
    p=softmax(record[:,1]) 　　←---　计算每个臂对应于其当前动作-价值的Softmax概率
    choice=np.random.choice(np.arange(n),p=p) 　　←---　通过Softmax概率加权随机选择一个臂
    r=get_reward(probs[choice])
    record=update_record(record,choice,r)
    mean_reward=((i+1) * rewards[-1]+r)/(i+2)
    rewards.append(mean_reward)
ax.scatter(np.arange(len(rewards)),rewards)
```

![[pic-20250227191833915.png]]

在该问题上 Softmax 动作选择似乎比 $\varepsilon$ 贪婪算法做得更好，似乎更快地收敛到一个最优策略。然而，Softmax 的缺点就是必须手动选择 $\tau$ 参数，这里的 Softmax 对 $\tau$ 非常敏感，而且需要一些时间来找到一个较好的值。虽然利用 $\varepsilon$ 贪婪算法我们也必须设置参数 $\varepsilon$，但选择 $\varepsilon$ 更加直观

## 2.3 应用老虎机算法优化广告投放

假设有 10 个电子商务网站，它们各自销售不同类别的零售商品，例如计算机、鞋子、珠宝等。我们希望把在其中一个网站上购物的顾客引向他们可能感兴趣的另一个网站，进而实现销量的提升。当一位顾客在其中某个网站结账时，我们就会展示另一个网站的广告，以引导他去那里购买别的东西。我们也可以在同一网站上放置其他产品的广告。然而，问题是不知道应该把顾客引导到哪些网站。我们可以尝试随机投放广告，但可能存在一种更有针对性的方法

### 2.3.1 上下文老虎机

在游戏的每个轮次（每次一位顾客在某个网站结账）​，我们有 n=10 种可能的动作，对应着可以投放的 10 种不同类型的广告。关键是最佳的投放广告可能取决于当前顾客处于哪个网站。例如，在珠宝网站结账的顾客可能更愿意购买一双鞋来搭配新钻石项链，而不是购买一台新的笔记本电脑。因此，问题就变成了“找出特定网站如何与特定广告关联”​。这就引出了**状态空间(state space)** 的概念。我们提出的多臂老虎机问题有一个包含多个元素的动作空间（所有可能动作的集合）​，但是并没有“状态”的概念。也就是说，环境中没有任何信息可以帮助我们选择一个好的臂。我们确定哪个臂比较好的唯一方法就是反复试错。在广告投放问题中，我们知道顾客正在某个特定网站上购物，这可能会提供一些有关用户的偏好信息，并有助于指导我们决定投放哪个广告。我们将这种**上下文信息**称为一种状态，并将这种新类型的问题称为**上下文老虎机**

![[pic-20250227223428337.png]]

### 2.3.2 状态、动作和奖励

巩固第一章的概念

状态：指环境中可用来做出决策的信息集

强化学习算法模型世界好像仅涉及一组状态 S（状态空间，一组关于环境的特征）​、一组动作 A（动作空间，可以输入某个给定状态）和奖励 r（在特定状态下采取某个动作时产生）​。当说起在特定状态下采取特定动作时，我们通常称之为状态-动作对 `(s, a)`

> [!important]
> 任何强化学习算法的目标都是在整个事件过程中最大化奖励

在最初的多臂老虎机问题中没有状态空间，而只有动作空间，因此我们只需要了解动作和奖励之间的关系，可以使用一个查找表来存储从特定动作中获得奖励的经验来了解这种关系，正如我们存储了动作-奖励对 $(a_k, r_k)$，其中轮次 k 的奖励是与采取动作 $a_k$ 相关的过去所有轮次下的奖励均值

在多臂老虎机问题中只有 10 个动作，所以 10 行的查找表是非常合理的。但在上下文老虎机问题中引入状态空间时，我们开始会得到可能的状态-动作-奖励元组的组合性爆炸问题。例如，如果有一个包含 100 个状态的状态空间，并且每个状态与 10 个动作关联，那么需要存储和重新计算 1000 个不同的数据片段

深度学习在这里就有了用武之地。经过合适的训练，神经网络会变得很擅长学习抽象信息，摒弃没有价值的细节。它们可以学习数据中的组合模式和规律，从而能够在**有效压缩大量数据的同时保留重要信息**。因此，神经网络可以用来学习状态-动作对和奖励之间的复杂关系，而不必将所有这些经验作为原始记忆存储。通常，我们将强化学习算法中基于某些信息做出决策的部分称为智能体。为了解决所讨论的上下文老虎机问题，我们用一个神经网络作为智能体

## 2.4 利用 PyTorch 构建网络

利用 PyTorch 实例化 2×3 矩阵

```bash
>>> import torch
　
>>> torch.Tensor([[1, 2, 3], [4, 5, 6]])
1 2 3
4 5 6
[torch.FloatTensor of size 2x3]
```

PyTorch 中把多维数组称为张量(tensor)。这也是 TensorFlow 和其他框架中使用的术语，所以我们按照约定俗成的名称，将多维数组称为张量。我们可以参考**张量的阶**，也就是张量有多少索引维度。这有点儿令人费解，因为说到向量的维度时，我们指的是向量的长度，但是说到张量的阶时，我们指的是它有多少个索引

### 2.4.1 自动微分

相比 NumPy，PyTorch 提供的重要特性是自动微分和优化。假设创建一个简单线性模型，用来预测一些感兴趣的数据：

```bash
>>> x=torch.Tensor([2,4]) #input data
>>> m=torch.randn(2, requires_grad=True) #parameter 1
>>> b=torch.randn(1, requires_grad=True) #parameter 2
>>> y=m*x+b #linear model
>>> loss=(torch.sum(y_known - y))**2 #loss function
>>> loss.backward() #calculate gradients
>>> m.grad
tensor([ 0.7734, -90.4993])
```

只需要向计算梯度的PyTorch张量提供参数 `requires_grad=True`，然后在计算图的最后一个节点上调用backward方法——它将向所有具有 `requires_grad=True` 的节点反向传播梯度。接下来，我们可以利用自动计算的梯度实现梯度下降操作

### 2.4.2 构建模型

我们不直接处理自动计算的梯度，而会用 PyTorch 的 nn 模块轻松创建一个前馈神经网络模型，然后用内置的优化算法来自动训练神经网络，这样就不必手动指定反向传播和梯度下降机制了。下面是一个创建了优化器的简单两层神经网络：

```python
model=torch.nn.Sequential(
    torch.nn.Linear(10, 150),
    torch.nn.ReLU(),
    torch.nn.Linear(150, 4),
    torch.nn.ReLU(),
)
　
loss_fn=torch.nn.MSELoss()
optimizer=torch.optim.Adam(model.parameters(), lr=0.01)
```

我们创建了一个包含 ReLU 激活函数的两层模型，定义了一个均方误差损失函数，并创建了一个优化器。如果我们有一些标记好的训练数据，那么要训练该模型，只需启动一个训练循环：

```python
for step in range(100):
    y_pred=model(x)
    loss=loss_fn(y_pred, y_correct)
    optimizer.zero_grad()	# 清除梯度
    loss.backward()			# 反向传播
    optimizer.step()		# 更新参数
```

变量 x 是模型的输入数据。变量 y_correct 是一个代表标记的正确输出的张量。我们使用模型进行预测与计算损失，然后在计算图（通常为 loss 函数）的最后一个节点上使用 backward 方法计算梯度。接着在优化器上运行 step 方法，以执行一个梯度下降步骤。如果需要构建比顺序模型更复杂的神经网络架构，那么可以通过继承 PyTorch 的模块类来编写自己的 Python 类并使用：

```python
from torch.nn import Module, Linear
　
class MyNet(Module):
   def __init__(self):
       super(MyNet, self).__init__()
       self.fc1=Linear(784, 50)
       self.fc2=Linear(50, 10)
　
   def forward(self, x):
       x=F.relu(self.fc1(x))
       x=F.relu(self.fc2(x))
       return x
　
model=MyNet()
```

## 2.5 解决上下文老虎机问题

我们已经为上下文老虎机创建了一个模拟环境，其中模拟器包括状态（一个 0～9 的数字代表 10 个网站中的其中一个）​、奖励生成（广告点击）和选择动作的方法（10 个广告中的哪一个提供服务）

```python
class ContextBandit:
    def __init__(self, arms=10):
        self.arms=arms
        self.init_distribution(arms)
        self.update_state()
　
	def init_distribution(self, arms): 　　←---　为简单起见，状态的数量等于臂的数量。每一行代表一个状态，每一列代表一个臂
		self.bandit_matrix=np.random.rand(arms,arms)

	def reward(self, prob):
		reward=0
		for i in range(self.arms):
			if random.random() < prob:
				reward+=1
		return reward
	
	def get_state(self):
		return self.state

	def update_state(self):
		self.state=np.random.randint(0,self.arms)

	def get_reward(self,arm):
		return self.reward(self.bandit_matrix[self.get_state()][arm])

	def choose_arm(self, arm): 　　←---　选择一个臂，返回一个奖励并更新状态
		reward=self.get_reward(arm)
		self.update_state()
		return reward
```
​
下面演示该环境的使用方法，唯一需要构建的部分是智能体，这通常是所有强化学习问题的关键，因为构建环境通常只涉及使用一些数据源设置输入/输出或调用现有 API

```bash
env=ContextBandit(arms=10)
state=env.get_state()
reward=env.choose_arm(1)
print(state)
>>> 2
print(reward)
>>> 8
```

模拟器包含一个简单的 Python 类 ContextBandit，该类可以初始化为特定数量的臂。为简单起见，此处设定状态的数量等于臂的数量，但通常来说状态空间往往比动作空间大得多

该类有两个方法：一个是 get_state，调用时不需要参数，返回一个从均匀分布中随机抽样的状态（在大多数问题中，状态将来自一个更加复杂的分布）​；另一个是 choose_arm，调用它会模拟投放一个广告的过程，并返回一个奖励（例如，与广告点击量成比例）​。我们总是需要按顺序调用 get_state 和 choose_arm，以不断地获取新的数据进行学习

ContextBandit 模块还包含一些辅助函数，例如 softmax 函数和独热(one-hot)编码器。独热编码的向量是除了 1 个元素其他元素都设置为 0 的向量，通过将唯一的非零元素设置为 1 来表示状态空间中的某个特定状态

与使用 n 个动作上的单一静态奖励概率分布不同（例如我们最初的老虎机问题）​，上下文老虎机模拟器为**每个状态的动作设置了不同的奖励分布**。也就是说，对于每一种状态，都将有 n 种不同的动作上的 Softmax 奖励分布

我们用 PyTorch 来构建神经网络。在这种情况下，我们将创建一个两层前馈神经网络，用 ReLU 作为激活函数。第一层接收一个包含 10 个元素的状态独热（也称为 1-of-K，其中除了 1 个元素，其他元素都为 0）编码向量；第二层返回一个包含 10 个元素的向量，表示在给定状态下每个动作的期望奖励

如下图展示了所描述算法的正向传递。与查找表方法不同，神经网络（智能体）将学习预测给定状态下每个动作会生成的奖励。然后，我们用 Softmax 函数给出动作的概率分布，并从该分布中抽样选择一个臂（广告）​。选择一个臂会生成一个奖励，我们将用它来训练神经网络：

![[pic-20250228110116817.png]]

get_state 函数返回一个状态值，该值被转换成一个 one-hot 编码向量，并会成为一个两层神经网络的输入。神经网络的输出是每个可能动作的预测奖励，它是一个稠密向量，通过 Softmax 从结果动作概率分布中抽样一个动作，执行所选动作后将返回一个奖励并更新环境状态。$\theta_{1}$ 和 $\theta_{2}$ 分别表示每一层的权重参数。$\mathbb{N}$、$\mathbb{R}$ 和 $\mathbb{P}$ 分别表示自然数（0，1，2……）、实数（就目的而言是浮点数）和概率。其中，上标表示向量的长度，所以 $\mathbb{P}^{10}$ 表示一个 10 元素向量，其中每个元素都是一个概率值

在状态 0 下，初始化神经网络会产生一个随机向量，例如 $[1.4,50,4.3,0.31,0.43,11,121,90,8.9,1.1]$ ​。我们将对该向量运行 softmax 函数并选择一个动作，最可能选择动作 6（动作 0～动作 9）​，因为它在示例向量中具有最大值。选择动作 6 将生成一个奖励（例如 8）​。然后训练神经网络来生成向量 $[1.4,50,4.3,0.31,0.43,11,8,90,8.9,1.1]$ ​，由于这是获得的针对动作 6 的真正奖励，因此其余值会保持不变。神经网络下一次遇到状态 0 时，将为动作 6 生成一个接近 8 的奖励预测值。随着我们不断在很多状态和动作上重复该过程，神经网络最终将学会预测给定状态下每个动作的准确奖励。也就是说，算法将每次都能选择最佳动作，从而实现奖励最大化

```python
import numpy as np
import torch

arms = 10
# N是batch size
# D_in是输入维度
# H是隐藏维度
# D_out是输出维度
N, D_in, H, D_out = 1, arms, 100, arms

# 如下我们构建一个两层的简单序列（前馈）神经网络
model = torch.nn.Sequential(
	torch.nn.Linear(D_in, H),
	torch.nn.ReLU(),
	torch.nn.Linear(H, D_out),
	torch.nn.ReLU(),
)

# 使用均方差损失函数
loss_fn = torch.nn.MSELoss()

# 创建环境
env = ContextBandit(arms)

def one_hot(arms, pos, val=1):
    one_hot_vec=np.zeros(arms)
    one_hot_vec[pos]=val
    return one_hot_vec
    
# 主训练
def train(env, epochs=5000, learning_rate=1e-2):
    cur_state=torch.Tensor(one_hot(arms,env.get_state()))　　←---　获取环境的当前状态并转换为PyTorch变量
    optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)
    rewards=[]
    for i in range(epochs):
        y_pred=model(cur_state) 　　←---　运行神经网络，以获得奖励预测
　
        av_softmax=softmax(y_pred.data.numpy(), tau=2.0) 　　←---　用softmax将奖励预测转换为概率分布
        av_softmax /=av_softmax.sum()　　←---　对分布进行正态化，确保和为1
        choice=np.random.choice(arms, p=av_softmax) 　　←---　随机选择新的动作
        cur_reward=env.choose_arm(choice) 　　←---　采取行动，接收奖励
        one_hot_reward=y_pred.data.numpy().copy()　　←---　将PyTorch张量数据转换为NumPy数组
        one_hot_reward[choice]=cur_reward　　←---　更新one_hot_reward数组，用作标记的训练数据
        reward=torch.Tensor(one_hot_reward)
        rewards.append(cur_reward)
        loss=loss_fn(y_pred, reward)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        cur_state=torch.Tensor(one_hot(arms,env.get_state()))　　←---　更新当前环境状态
    return np.array(rewards)
```

![[pic-20250228113830608.png]]

这个神经网络确实能够很好地理解上下文老虎机的状态、动作和奖励之间的关系。每个轮次的最大奖励均为 10，平均奖励接近 8.5，这接近于这个老虎机的数学优化结果

## 2.6 马尔可夫性质

在上下文老虎机问题中，神经网络引导我们在不参考任何先验状态的情况下选择给定状态下的最佳动作。我们只需向其提供当前状态，神经网络就会为每个可能的动作产生期望的奖励。在强化学习中，这种重要的特性称为**马尔可夫性质(Markov property)**。我们把具有马尔可夫性质的游戏（或任何控制任务）称为**马尔可夫决策过程(Markov Decision Process，MDP)**。在 MDP 中，**当前状态本身就包含了足够的信息来选择最优动作**，以最大化未来奖励。将控制任务建模为 MDP 是强化学习的一个关键步骤

MDP 极大地简化了强化学习问题，因为我们不需要考虑所有之前的状态或动作——**不需要记忆，只需要分析当前情况**。因此，我们总是想方设法将一个问题建模为（至少近似）一个马尔可夫决策过程。纸牌游戏“黑杰克”​（也称为 21 点）就可以视为一个 MDP，因为只需知道当前状态（我们有什么牌，以及庄家的一张明牌）​，就可以成功地玩该游戏

如下是一些具有马尔可夫性质的任务：
- 驾驶汽车
- 选择到到某个目的地的最短路径（距离）
- 用枪瞄准远处的目标并射击

如下是一些不具有马尔可夫性质的任务：
- 投资股票
- 为病人选择一种治疗方案（需要知道病人病症历史等信息）
- 预测足球比赛中哪个队会获胜

在为病人诊断疾病的例子中，你可能需要知道他们最近的症状历史，但如果这些信息已保存在他们的就诊记录中，并且将完整的就诊记录作为当前状态时，就能有效地引入马尔可夫性质。记住这一点很重要：很多问题**原本**可能不具有马尔可夫性质，但通常可以通过向状态中插入更多信息来**使其具有**马尔可夫性质

DeepMind 的深度 Q-learning（或深度 Q 网络）算法仅从原始像素数据和当前得分就学会了玩雅达利游戏。雅达利游戏具有马尔可夫性质吗？未必！在《吃豆人》游戏中，如果状态是当前帧的原始像素数据，就无法知道数块砖之外的敌人是正在接近还是远离我们，这会极大地影响我们选择要采取的动作。这就是为什么 DeepMind 的实现中实际输入的是游戏的最后 4 帧，**因为它能有效地将一个非 MDP 变成 MDP**。通过最后 4 帧，智能体可以掌握所有玩家的方向和速度

## 2.7 预测未来奖励：价值和策略函数

![[pic-20250228120330126.png]]

下面我们回顾并正式介绍一下到目前为止所学的内容​。强化学习算法本质上构造了一个在环境中活动的智能体。环境通常是一种游戏，但普遍来说是任何产生状态、动作和奖励的过程。智能体可以访问当前的环境状态，即特定时间点上关于环境的所有数据 $s_{t}\in S$。利用状态信息，智能体会采取动作 $a_{t}\in A$，从而可能确定性地或概率性地将环境变成一个新状态 $s_{t+1}$

通过采取动作将一种状态映射到一种新状态的概率称为**转移概率(transition probability)**。智能体会收到一个奖励 $r_{t}$ ，因为它在状态 $s_{t}$ 中采取了动作 $a_{t}$ 并产生了一个新状态 $s_{t+1}$ 。我们知道，智能体（强化学习算法）的最终目标是最大化它的奖励。实际上，**真正产生奖励的是状态转换** $s_{t}\to s_{t+1}$，而不是动作本身，因为动作可能会以一定概率产生糟糕的状态

### 2.7.1 策略函数

策略 $\pi$ 是指智能体在环境中的策略。例如，在 21 点游戏中，庄家的策略就是一直拿牌，直到点数达到 17 或更多，这是一个简单的固定策略。在多臂老虎机问题中，我们的策略是 $\varepsilon$ 贪婪策略。一般来说，策略是一个函数，可以将一个状态映射到该状态下一系列可能动作的概率分布上：

$$
\pi , s \to Pr(A|s)\ \ , \ \ \text{其中} \ \ s \in S
$$
$Pr(A|s)$ ：给定状态 s 下一组动作 a 的概率分布，该分步中每个动作的概率是该动作可能产生最大奖励的概率

上述公式表示：策略 $\pi$ 是从状态到（概率性地）最佳动作的映射

### 2.7.2 最优策略

策略是强化学习算法的一部分，它根据当前状态来选择动作，然后就可以制订出**最优策略(optimal policy)**，也就是使奖励最大化的策略：

$$
\pi ^{*} = argmax E(R|\pi)
$$
一个特定的策略就是一个映射或函数，所以有各种可能的策略，而最优策略就是将 argmax（选择最大值）作为函数作用在这些可能的策略上时产生的期望奖励

- 直接方法：可以直接教智能体学习在所处状态下哪个动作是最好的
- 间接方法：可以教智能体学习哪些状态是最有价值的，然后采取能够导致最有价值状态的动作。

### 2.7.3 价值函数

价值函数是将状态或状态-动作映射到处于某种状态或在某种状态下采取某种动作的期望值（期望奖励）的函数

通俗的说：价值函数是用来评估在某个状态（或在该状态下采取某个动作）有多“好”的函数。它衡量的是从当前状态（或动作）出发，未来可能获得的长期平均奖励。例如：

状态价值：你在迷宫的一个位置，状态价值告诉你从这个位置出发，平均能多快找到出口

动作价值：你在迷宫的一个位置，动作价值告诉你如果往左、右、上、下走，平均能多快找到出口

当提到价值函数时，通常指的是状态-价值函数：

$$
V_{\pi}:s \to E(R|s, \pi)
$$
该函数接受一个状态 s，返回始于该状态并根据策略 $\pi$ 采取动作时的期望奖励

在多臂老虎机问题中，我们介绍了状态-动作-价值函数，这些函数通常称为 Q 函数或 Q 值，这就是深度 Q 网络的来源，因为在第 3 章中你将看到，深度学习算法可以作为 Q 函数：

$$
Q_{\pi}:(s|a) \to E(R|a,s,\pi)
$$
$Q_{\pi}$ 是一个函数，它将状态 s 和动作 a 组合的 $(s,a)$ 映射到通过策略 $\pi$ 在状态 s 中采取动作 a 所获得的期望奖励

事实上，我们差不多实现了一个可以解决上下文老虎机问题（尽管这只是一个很浅的神经网络）的深度 Q 网络，因为它本质上是作为 Q 函数来用的。我们对其进行训练，**使其准确估计在给定状态下采取动作的期望奖励**。此外，我们的策略函数是作用于神经网络输出上的 softmax 函数

## 小结

- 策略是一种特定的对策。形式上，它是一个函数，要么接收一个状态并产生一个动作，要么产生一个给定状态下的动作空间的概率分布

- Q值是给定状态-动作对时的期望奖励，而Q函数是一个产生给定状态-动作对时的Q值的函数