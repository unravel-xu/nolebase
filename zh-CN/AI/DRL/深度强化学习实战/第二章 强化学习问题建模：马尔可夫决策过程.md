## 2.1 线图和本书的教学方法

![[pic-20250227125939746.png]]

神经网络的单层线图：表示神经网络层接受一个长度或维度为 n 的向量，然后产生一个维度为 m 的向量

## 2.2 解决多臂老虎机问题

假设面前有 10 台老虎机，每台老虎机保证会给你 0～10 美元的奖励，但这 10 台老虎机的平均奖励各不相同。现在，来尝试找出哪台老虎机平均奖励最高

通俗称这类问题为 n 臂老虎机问题，其中 n 是老虎机的数量

我们有 n 个可能的动作（这里 n = 10）​，其中动作意味着拉动一台特定老虎机的臂（杠杆）​，在该游戏的每一轮（轮次 k）中，我们可以选择拉动单个杠杆。在采取动作之后，我们将获得一个奖励 $R_{k}$（轮次 k 的奖励）​。每个杠杆有唯一的奖励概率分布，例如，如果有 10 台老虎机，你玩了很多轮游戏，那么 3 号老虎机的平均奖励是 9 美元，而 1 号老虎机的平均奖励为 4 美元。当然，由于每一轮游戏的奖励是随机出现的，因此 1 号杠杆有可能在某次游戏中给出 9 美元的奖励。但如果进行很多轮游戏，我们就会发现 1 号老虎机的平均奖励低于 3 号老虎机

我们的策略应该是玩几轮游戏，选择不同的杠杆并观察每个动作的奖励，然后只选择观察到的平均奖励最高的杠杆。因此，我们希望获悉基于之前轮次采取动作的期望奖励

这个期望奖励 $Q_{k}(a)$ 定义为：向函数提供一个动作 a（假设当前处于轮次 k ），它返回采取该动作时的期望奖励

$$
Q_{k}(a)=\frac{R_{1}+R_{2}+\dots+R_{k}}{k_{a}}
$$

也就是说轮次 k 中动作 a 的期望奖励为之前采取动作 a 时收到的所有奖励的算术平均值

称函数 $Q_{k}(a)$ 为价值函数，用于表示某物的价值，具体来说，它是一个 `动作-价值` 函数，用于表示采取特定动作的价值，通常用符号 Q 来表示这个函数，也称为 Q 函数

### 2.2.1 探索与利用

探索：第一次开始玩游戏时，需要进行游戏并观察从各台老虎机上获得的奖励

利用：使用现在已知的知识，并一直玩可能产生最高奖励的老虎机

总体策略涉及一定数量的利用（基于目前所知选择最佳杠杆）和一定数量的探索（随机选择杠杆来学习更多的知识）。利用和探索之间的合理平衡对于最大化奖励非常重要

最简单的算法就是选择最高 Q 值所对应的动作

$$
\forall a_{i} \in A_{k} \ ,\ a^* = argmax_{a}Q_{k}(a_{i})
$$
$A_{k}$：可采取的动作空间

$a^*$：要采取的动作

$Q_{k}(a_{i})$：采取动作 $a_{i}$ 的价值

```python
def get_best_action(actions):
    best_action=0
    max_action_value=0
    for i in range(len(actions)): 　　# ←---　循环遍历所有可能的动作
        cur_action_value=get_action_value(actions[i]) 　# ←---　获取当前动作的价值
        if cur_action_value > max_action_value:
            best_action=i
            max_action_value=cur_action_value
    return best_action
```

$Q_{k}(a)$ 依赖于之前动作以其对应的奖励，所以该方法不会评估尚未探索的动作。因此，我们之前可能已经尝试过 1 号和 3 号杠杆，并观察到 3 号杠杆返回更高的奖励，但利用该方法，我们将永远不会想到尝试另一个不为我们所知的杠杆，例如 6 号杠杆，而它实际上提供了最高的平均奖励。这种简单地选择目前为止所知的最佳杠杆的方法称为贪婪（或利用）方法

### 2.2.2 $\varepsilon$ 贪婪策略    
在上述算法基础上根据概率 $\varepsilon$ 随机选择一个动作 a，而其他时间（概率 $1-\varepsilon$）将基于当前从过去游戏中所知的信息选择最佳杠杆。大多数时候，我们会以贪婪方式玩游戏，但有时也会冒险随机选择杠杆来查看会发生什么

```python
import numpy as np
from scipy import stats
import random
import matplotlib.pyplot as plt
　
n=10　　# ←---　臂的数量
probs=np.random.rand(n) 　　# ←---　每个臂关联的隐藏概率
eps=0.2　　# ←---　用于ε贪婪动作选择的ε
```

数组 probs 中的每个位置对应一台老虎机，表示一个可能的概率。例如，第一个元素的索引位置为 0，所以动作 0 就是选择第一台老虎机。每台老虎机（每个臂）有一个关联的概率，用来衡量它能得到的奖励

选择以下方式来为每个臂实现奖励概率分布。每个臂有一个概率（例如 0.7），并且最高奖励是 10 美元。设置一个遍历到 10 的 for 循环，在每一步中，如果随机浮点数小于臂的概率，那么将奖励加 1。因此，在第一次循环中，生成了一个随机浮点数（例如 0.4）​。0.4 < 0.7，因此 reward+=1。在下一次迭代中，它将生成另一个也小于 0.7 的随机浮点数（例如 0.6）​，所以 reward+=1。这种情况将一直持续，直到完成 10 次迭代，然后返回最终的总奖励。总奖励可以是 0 和 10 之间的任意值。在臂的概率为 0.7 的情况下，这样无限玩下去的平均奖励将是 7，但在任何一轮游戏中，这个值可大可小。

```python
def get_reward(prob, itr=10):
	reward = 0
	for i in range(itr):
		if random.random() < prob:
			reward += 1
	return reward
```

```stylus
# 以概率0.7运行这段代码2000次，最终返回的平均奖励值接近7
>>> np.mean([get_reward(0.7) for _ in range(2000)])
7.001
```

我们要定义的下一个函数是选择目前为止最好的臂的贪婪策略。我们需要一种方法来追踪拉动了哪个臂，以及由此得到的奖励是什么。简单来说，我们可以使用一个列表，并向其添加像（臂,奖励）这样的观察值，例如，(2, 9) 表示选择了 2 号臂并获得了奖励 9。随着游戏的进行，这个列表会越来越长

由于只需要记录每个臂的平均奖励，并不需要存储每个观察结果，因此还有一种更简单的方法。回想一下，要计算一组数字 $x_{i}$ 的平均值，只需将所有 $x_{i}$ 相加，然后除以 $x_{i}$ 的数量（用 k 来表示）​。平均值通常用希腊字母 $\mu$ 来表示：$\mu = \frac{1}{k}\sum_{i}x_{i}$ ，在获取一个新的奖励时，$\mu_{new}=\frac{k\cdot \mu_{old} + x}{k+1}$

这样只需要跟踪每个臂的两个数学量：k（观察值的数量）和 $\mu$（当前运行的平均值）​。我们可以将它存储在一个 10×2 的 NumPy 数组中（假设有 10 个臂）​，并将该数组称为 record

```stylus
>>> record=np.zeros((n,2))
array([[0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.]])
```

record 数组的第一列存储每个臂被拉动的次数，第二列存储由此得到的平均奖励

```python
def update_record(record, action, reward):
	new_reward=(record[action, 0] * record[action, 1] + reward) / (record[action, 0] + 1)
	record[action, 0]+=1
	record[action, 1]=new_reward
	return record
```

接下来我们需要实现一个函数来选择要拉动哪个臂。我们希望能选择与最高平均奖励关联的臂，所以需要找到数组 record 中第二列取最大值所对应的索引：

```python
def get_best_arm(record):
    arm_index=np.argmax(record[:,1],axis=0) #←---　对数组record的第二列调用NumPy的argmax函数
    return arm_index
```

现在我们可以进入多臂老虎机游戏的主循环了。如果一个随机数大于 $\varepsilon$ 参数，那么只需使用 get_best_arm 函数计算最佳动作并采取该动作；否则，便会采取一个随机动作，以确保一定次数的探索。选择臂之后，执行 get_reward 函数并观察奖励值，然后使用这个新的观察值更新数组 record。多次重复该过程，数组 record 将不断更新。最终，有最高奖励概率的臂会被选择最多次，因为它提供最高的平均奖励

```python
fig, ax=plt.subplots(1,1)
ax.set_xlabel("Plays")
　
ax.set_ylabel("Avg Reward")
record=np.zeros((n,2))	←---　将数组record元素初始化为0
probs=np.random.rand(n)	←---　随机初始化每个臂的奖励概率
eps=0.2
rewards=[0]				←--- 记录每轮循环获得的平均reward 　　
for i in range(500):
    if random.random() > eps: 　　←---　以概率0.8选择最佳动作，也可以随机选择
        choice=get_best_arm(record)
    else:
        choice=np.random.randint(10)
    r=get_reward(probs[choice]) 　　←---　计算选择臂的奖励
    record=update_record(record,choice,r) 　　←---　利用新数量和臂的奖励观察值更新数组record
    mean_reward=((i+1) * rewards[-1]+r)/(i+2) 　　←---　跟踪运行的平均奖励来评估整体表现
    rewards.append(mean_reward)
ax.scatter(np.arange(len(rewards)),rewards)
```

![[pic-20250227172814682.png]]

从上图可以看出，经过很多轮次，平均奖励确实有所提高，这说明算法确实在学习，通过前面好的游戏轮次逐渐得到了强化，不过它仍然是一个比较简单的算法

这里我们考虑的问题是一个**平稳**问题，因为臂的潜在奖励概率分布**不会随着时间发生变化**。当然，我们可以考虑这个问题的一个变体——非平稳问题。在这种情况下，一种简单的修改方法是允许新的奖励观察值以一种倾斜的方式更新存储在记录中的平均奖励值，这样它将是一个加权平均值，朝着最新的观察值倾斜。如果事情随着时间发生变化，从某种程度上讲，我们就能跟踪它们

### 2.2.3 Softmax 选择策略

来看另一种类型的“老虎机”问题：一位新来的医生专门治疗心脏病患者。她有 10 种治疗方案，但针对每位病人她只能选择其中 1 种来进行治疗。出于某种原因，她只知道这 10 种治疗方案对治疗心脏病有不同的疗效和风险，但却不知道哪一种是最好的。此处我们可以使用前面方案中的多臂老虎机算法，但可能需要重新考虑每次随机选择治疗方案的 $\varepsilon$ 贪婪策略。在这个新问题中，随机选择一种治疗方案可能会导致病人死亡，而不仅仅是损失钱。我们想确保不会选择最糟糕的治疗方案，但仍然希望有能力探索以找到最好的治疗方案   

这时候使用 Softmax 可能是最合适的，在探索期间，Softmax 提供来各个选项的概率分布，而非随机选择一个动作。概率最大的选项将等同于前面解决方案中的最佳动作，它也会提供一些关于次佳动作和次次佳动作的信息。这样我们就可以随机选择并探索其他选项，同时避免选择最差的选项——因为它们的概率会很小，甚至为 0

$$
Pr(A)=\frac{e^{Q_{k}(A)/\tau}}{\sum_{i=1}^n e^{Q_k(i)/\tau}}
$$
Pr(A)接收一个动作-价值向量（数组），并返回一个动作的概率分布（数组），这样更高值的动作具有更高的概率。例如，如果动作-价值数组有 4 个可能的动作，并且它们目前都有相同的值，例如 `A=[10,10,10,10]` ​，那么 `Pr(A)=[0.25,0.25,0.25,0.25]` ​。换句话说，所有概率相同，且总和必须为 1

分数的分子是动作-价值数组除以参数 $\tau$ 后取指数幂，生成一个与输入相同大小（长度）的向量。分母是对每个动作-价值除以 $\tau$ 后取指数幂求和，从而产生单个数值

$\tau$ 是温度参数，用于**对动作的概率分布进行缩放**。较高的温度会导致概率非常接近，而较低的温度则会增大动作概率之间的差异。为这个参数选择一个值需要据理推测和进行试错

```python

# softmax函数
def softmax(actionValue, tau=1.12):
	softm = np.exp(actionValue/tau) / np.sum(np.exp(actionValue/tau))
	return softm 
```

在用 softmax 实现之前的 10 臂老虎机问题时，我们不再需要 get_best_arm 函数。Softmax 生成了可能动作的加权概率分布，所以我们可以根据动作对应的概率随机选择动作

对数组 record 的第二列（列索引为 1）应用 softmax 函数，因为这一列存储了每个动作当前的平均奖励（动作-价值）​。它会把这些动作-价值转化为概率。然后，我们使用 np.random.choice 函数。该函数接收一个任意的输入数组 x 和一个参数 p，其中 p 是一个对应于 x 中各个元素的概率数组。由于记录被初始化为 0，因此 softmax 起初将返回一个在所有老虎机上的均匀分布，但该分布很快就会倾向于最高奖励关联的动作

```stylus
>>> x=np.arange(10)
>>> x
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
>>> av=np.zeros(10)
>>> p=softmax(av)
>>> p
array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])
>>> np.random.choice(x,p=p)
3
```

如下是多臂老虎机的 Softmax 动作选择：

```python
n=10
probs=np.random.rand(n)
record=np.zeros((n,2))
fig,ax=plt.subplots(1,1)
ax.set_xlabel("Plays")
ax.set_ylabel("Avg Reward")
fig.set_size_inches(9,5)
rewards=[0]
for i in range(500):
    p=softmax(record[:,1]) 　　←---　计算每个臂对应于其当前动作-价值的Softmax概率
    choice=np.random.choice(np.arange(n),p=p) 　　←---　通过Softmax概率加权随机选择一个臂
    r=get_reward(probs[choice])
    record=update_record(record,choice,r)
    mean_reward=((i+1) * rewards[-1]+r)/(i+2)
    rewards.append(mean_reward)
ax.scatter(np.arange(len(rewards)),rewards)
```

![[pic-20250227191833915.png]]

在该问题上 Softmax 动作选择似乎比 $\varepsilon$ 贪婪算法做得更好，似乎更快地收敛到一个最优策略。然而，Softmax 的缺点就是必须手动选择 $\tau$ 参数，这里的 Softmax 对 $\tau$ 非常敏感，而且需要一些时间来找到一个较好的值。虽然利用 $\varepsilon$ 贪婪算法我们也必须设置参数 $\varepsilon$，但选择 $\varepsilon$ 更加直观

## 2.3 应用老虎机算法优化广告投放

