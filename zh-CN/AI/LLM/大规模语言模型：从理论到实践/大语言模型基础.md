语言模型的目标：建模自然语言的概率分布

对语言的建模也是自然语言处理基础任务之一

## Transformer 结构

Transformer 由论文 [Attention is All You Need](https://arxiv.org/abs/1706.03762) 提出，并首先应用于机器翻译的神经网络模型架构。机器翻译的目标是从源语言（Source Language）转换到目标语言（Target Language）。Transformer 结构完全通过**注意力机制**完成对源语言序列和目标语言序列全局依赖的建模。当前几乎全部大语言模型都是基于 Transformer 结构，本节以应用于机器翻译的基于 Transformer 的编码器和解码器介绍该模型。

### 模型概览

首先将整个 Transformer 看成一个黑盒子，如下图所示，对于机器翻译来说，它的输入是源语言的句子，输出是目标语言的句子：

![[pic-20250216162059927.png|650]]

稍微展开一点，Transformer（或者任何神经机器翻译系统）都可以分成 Encoder 和 Decoder 两个部分，如下图所示：

![[pic-20250216162317170.png|650]]

再展开一点，Encoders 由多个结构一样的 Encoder 堆叠（Stack）而成，Decoder 也是一样：

![[pic-20250216162526556.png|650]]



## 参考

[Transformer图解](https://fancyerii.github.io/2019/03/09/transformer-illustrated/)