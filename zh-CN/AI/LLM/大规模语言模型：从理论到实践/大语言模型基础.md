语言模型的目标：建模自然语言的概率分布

对语言的建模也是自然语言处理基础任务之一

## Transformer 结构

Transformer 由论文 [Attention is All You Need](https://arxiv.org/abs/1706.03762) 提出，并首先应用于机器翻译的神经网络模型架构。机器翻译的目标是从源语言（Source Language）转换到目标语言（Target Language）。Transformer 结构完全通过**注意力机制**完成对源语言序列和目标语言序列全局依赖的建模。当前几乎全部大语言模型都是基于 Transformer 结构，本节以应用于机器翻译的基于 Transformer 的编码器和解码器介绍该模型。

### 模型概览

首先将整个 Transformer 看成一个黑盒子，如下图所示，对于机器翻译来说，它的输入是源语言的句子，输出是目标语言的句子：

![[pic-20250216162059927.png]]

稍微展开一点，Transformer（或者任何神经机器翻译系统）都可以分成 Encoder 和 Decoder 两个部分，如下图所示：

![[pic-20250216162317170.png]]

再展开一点，Encoders 由多个结构一样的 Encoder 堆叠（Stack）而成，Decoder 也是一样：

![[pic-20250216162526556.png]]

每个 Encoder 的输入是下一层 Encoder 输出，最底层 Encoder 的输入是原始输入（法语句子）

Decoder 类似，每层输入是下一层的输出，但**最后一层 Encoder 的输出会输入给每一个 Decoder 层**

#### Encoder

展开 Encoder 结构，它是由一个 Self-Attention 层和一个前馈神经网络（全连接网络）构成：

![[pic-20250216172236438.png]]

#### Decoder

展开 Decoder 结构，和 Encoder 相比，它除了 Self-Attention 层和全连接层外还多了一个普通的 Encoder-Decoder Attention 层，**这个 Attention 层使得 Decoder 在解码时会考虑最后一层 Encoder 所有时刻的输出**：

![[pic-20250216172853168.png]]

## 加入 Tensor

前面的图示只是说明了 Transformer 的模块，接下来加入 Tensor，使不同模块串联起来

> [!note] 什么是 Embedding
> NLP 中的 Embedding 就是将文本转换为连续向量，使得语义上相似的单词在向量空间中位置相近

输入的句子是词（ID）的序列，首先通过 Embedding 把它变成一个连续稠密的向量，如图所示：

![[pic-20250216185638830.png]]



## 参考

[Transformer图解](https://fancyerii.github.io/2019/03/09/transformer-illustrated/)