语言模型的目标：建模自然语言的概率分布

对语言的建模也是自然语言处理基础任务之一

## Transformer 结构

Transformer 由论文 [Attention is All You Need](https://arxiv.org/abs/1706.03762) 提出，并首先应用于机器翻译的神经网络模型架构。机器翻译的目标是从源语言（Source Language）转换到目标语言（Target Language）。Transformer 结构完全通过**注意力机制**完成对源语言序列和目标语言序列全局依赖的建模。当前几乎全部大语言模型都是基于 Transformer 结构，本节以应用于机器翻译的基于 Transformer 的编码器和解码器介绍该模型。

### 模型概览

首先将整个 Transformer 看成一个黑盒子，如下图所示，对于机器翻译来说，它的输入是源语言的句子，输出是目标语言的句子：

![[pic-20250216162059927.png]]

稍微展开一点，Transformer（或者任何神经机器翻译系统）都可以分成 Encoder 和 Decoder 两个部分，如下图所示：

![[pic-20250216162317170.png]]

再展开一点，Encoders 由多个结构一样的 Encoder 堆叠（Stack）而成，Decoder 也是一样：

![[pic-20250216162526556.png]]

每个 Encoder 的输入是下一层 Encoder 输出，最底层 Encoder 的输入是原始输入（法语句子）

Decoder 类似，每层输入是下一层的输出，但**最后一层 Encoder 的输出会输入给每一个 Decoder 层**

#### Encoder

展开 Encoder 结构，它是由一个 Self-Attention 层和一个前馈神经网络（全连接网络）构成：

![[pic-20250216172236438.png]]

#### Decoder

展开 Decoder 结构，和 Encoder 相比，它除了 Self-Attention 层和全连接层外还多了一个普通的 Encoder-Decoder Attention 层，**这个 Attention 层使得 Decoder 在解码时会考虑最后一层 Encoder 所有时刻的输出**：

![[pic-20250216172853168.png]]

### 加入 Tensor

前面的图示只是说明了 Transformer 的模块，接下来加入 Tensor，使不同模块串联起来

> [!note] 什么是 Embedding
> NLP 中的 Embedding 就是将文本转换为连续向量，使得语义上相似的单词在向量空间中位置相近

输入的句子是词（ID）的序列，首先通过 Embedding 把它变成一个连续稠密的向量，如图所示：

![[pic-20250216185638830.png]]

Embedding之后的序列会输入Encoder：首先经过Self-Attention层然后再经过全连接层，如图所示：

![[pic-20250216193610702.png]]

在计算 $z_{i}$ 时需要依赖所有时刻的输入 $x_{1},\dots,x_{n}$，可以用矩阵运算同时把所有的 $z_{i}$ 计算得到。而图中 Feed Forward 实际上是多个独立的全连接网络，计算 $i$ 时刻的输出只需要输入 $z_{i}$ 就足够了，因此可以实现并行计算，如下图是 Feed Forward 更详细的结构：

![[pic-20250216194327005.png]]

### Self-Attention 简介 

比如我们要翻译如下句子"The animal didn’t cross the street because it was too tired"（这个动物无法穿越马路，因为它太累了）。这里的 it 到底指代什么呢，是 animal 还是 street？要知道具体的指代，我们需要在理解 it 的时候同时关注所有的单词，重点是 animal、street 和 tired，然后根据知识（常识）我们知道只有 animal 才能 tired，而 street 是不能 tired 的。Self-Attention 用 Encoder 在编码一个词的时候会考虑句子中所有其它的词，从而确定怎么编码当前词。如果把 tired 换成 narrow，那么 it 就指代的是 street 了

而 LSTM（即使是双向的）无法实现上面的逻辑。为什么呢？比如前向的 LSTM，我们在编码 it 的时候根本没有看到后面是 tired 还是 narrow，所有它无法把 it 编码成哪个词。而后向的 LSTM 呢？当然它看到了 tired，但是到 it 的时候它还没有看到 animal 和 street 这两个单词，当然就也无法编码 it 的内容

多层的 LSTM 理论上是可以编码这个语义的，它需要下层的 LSTM 同时编码了 animal 和 street 以及 tired 三个词的语义，然后由更高层的 LSTM 来把 it 编码成 animal 的语义，但是这样模型更加复杂

下图是 Encoder 最顶层的 Attention 可视化，使用 tensor2tensor 工具得到，可以看到在编码 it 的时候有一个 Attention Head 注意到了 Animal，因此编码后的 it 有 Animal 的语义

![[pic-20250216195446121.png]]

### Attention 详细介绍

Attention（注意力）机制很像人类看图片的逻辑，当我们快速看图片时，并没有看清图片的全部内容，而是将注意力集中在了部分重点上。同理，Attention 机制将有限的注意力集中在重点信息上，从而节省资源，快速获得最有效的信息

Attention 机制最早在计算机视觉里应用，后来在自然语言处理领域获得广泛关注，特别是由于 2018 年 BERT 和 GPT 的出色表现，促使 Transformer 和 Attention 机制成为研究的焦点

之所以要引入 Attention 机制，主要是 3 个原因：
1. 参数少：模型复杂度跟 CNN、RNN 相比，复杂度更小，参数也更少
2. 速度快：Attention 机制每一步计算不依赖于上一步的结果，克服了 RNN 不能并行计算的问题
3. 效果好：在 Attention 机制引入之前， 有一个问题大家一直很苦恼：长距离的信息会被弱化，就好像记忆能力弱的人，记不住过去的事情是一样的。Attention 是挑重点，就算文本比较长，也能从中间抓住重点，不丢失重要的信息

下面用一个例子来解释 attention 的原理：

![[pic-20250216223646439.png]]

图书管（source）里有很多书（value），为了方便查找，我们给书做了编号（key）。当我们想要了解漫威（query）的时候，我们就可以看那些动漫、电影、甚至二战（美国队长）相关的书籍。为了提高效率，并不是所有的书都会仔细看，针对漫威来说，动漫，电影相关的会看的仔细一些（权重高），但是二战的就只需要简单扫一下即可（权重低）。当我们全部看完后就对漫威有一个全面的了解了

Attention 计算分 3 步：

![[pic-20250216232416712.png]]

第一步： Query 和 $Key_{i}$ 进行相似度计算得到相似性评分 $S_{i}$

第二步：将评分进行归一化，得到直接可用的权重

第三步：将权重和 Value 进行加权求和，得到 Attention 向量

以一个实例来详细讲解：

![[pic-20250217121608255.png]]

如图，我们已知部分 Key 和 Value 的对应关系，当需要预测腰围 57 时，自然地推断其体重应该会在 43~48 之间，但还需要定量计算 Value。为了充分利用 `(Key, Value)` 的已知信息，假设用 $\alpha(q, k_{i})$ 来表示 $q$ 和 $k$ 对应的注意力权重，则体重预测值 $f(q)$ 为：

$f(q)=\alpha(q,k_{1})\cdot v_{1} + \alpha(q, k_{2})\cdot v_{2} + \alpha(q,k_{3})\cdot v_{3}=\Sigma_{i=1}^3 \alpha(q,k_{i})\cdot v_{i}$

$\alpha$ 是任意能刻画相关性的函数，但需要归一化，以高斯核（注意力分数）为例（包括 softmax 函数）

$\alpha(q,k_{i})=softmax\left( -\frac{1}{2}(q-k_{i})^2 \right)$，其中 $-\frac{1}{2}(q-k_{i})^2$ 是注意力分数，带入 q = 57，先求得各注意力权重 $\alpha_{i}$，然后带入 $f(q)$ 中求得体重的估计值，这就是 Attention 机制

现在将情况拓展到多维：

![[pic-20250219121927651.png]]

假设已经知道两个人的腰围和胸围（$q_{1}, q_{2}$），需要估计出他们的体重和身高

先写成矩阵形式：

$$Key = \left[\begin{matrix}k_{1} \\ k_{2} \\ k_{3} \\ \end{matrix} \right]=\left[\begin{matrix} 51 & 70 \\ 56 & 82 \\ 58 & 88 \\ \end{matrix} \right]$$

$$Value = \left[\begin{matrix}v_{1} \\ v_{2} \\ v_{3} \\ \end{matrix} \right]=\left[\begin{matrix} 40 & 155 \\ 43 & 159 \\ 48 & 162 \\ \end{matrix} \right]$$

$$Query = \left[\begin{matrix}q_{1} \\ q_{2} \\ \end{matrix} \right]=\left[\begin{matrix} 57 & 83 \\ 55 & 77 \\ \end{matrix} \right]$$

注意力分数不能用简单情况的计算方法计算，这里引入三种计算方式：

|   模型    |                                                          公式                                                           |                                               说明                                               |
| :-----: | :-------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |
|  加性注意力  | $a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R}$ | 其中可学习的参数是 $W_{q}\in\mathbb{R}^{h\times q}、W_{k}\in\mathbb{R}^{h\times k}、W_{v}\in\mathbb{R}^h$ |
|  点积注意力  |                                $a(\mathbf q, \mathbf k) = \mathbf{q}^\top \mathbf{k}$                                 |                              计算效率更高，但要求 `Query` 和 `Key` 具有相同长度 d                               |
| 缩放点积注意力 |                           $a(\mathbf q, \mathbf k) = \mathbf{q}^\top \mathbf{k}  /\sqrt{d}$                           |                               为确保无论向量长度如何，点积的方差在不考虑向量长度的情况下仍然是 1                               |

以最简单的点积为例，注意力分数 $\alpha(q, k_{i})=softmax(a(q,k_{i}))$

Attention 有很多不同的类型：

![[pic-20250216232744170.png]]

在 Transformer 中使用的是 self Attention，另外多头 Attention 就是 Attention is All You Need 中提到的 multi-head attention，用到了多个 query 对一段原文进行了多次 attention，每个 query 都关注到原文的不同部分，相当于重复做多次单层 attention


## 参考

[Transformer图解](https://fancyerii.github.io/2019/03/09/transformer-illustrated/)

[一文看懂 Attention（本质原理+3大优点+5大类型）](https://easyaitech.medium.com/%E4%B8%80%E6%96%87%E7%9C%8B%E6%87%82-attention-%E6%9C%AC%E8%B4%A8%E5%8E%9F%E7%90%86-3%E5%A4%A7%E4%BC%98%E7%82%B9-5%E5%A4%A7%E7%B1%BB%E5%9E%8B-e4fbe4b6d030)

[注意力机制](https://lulaoshi.info/deep-learning/attention/)

[注意力机制的本质](https://www.bilibili.com/video/BV1dt4y1J7ov/?spm_id_from=333.788.comment.all.click&vd_source=b404977d131639c9b47b18fe2cc16c2c)