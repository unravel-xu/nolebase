大规模语言模型（Large Language Models，LLM），也称**大语言模型**或**大型语言模型**，是一种由包含数百亿以上参数的深度神经网络构建的语言模型，通常使用自监督学习方法通过大量无标注文本进行训练

## 基本概念

语言模型（Language Model，LM）目标就是建模自然语言的**概率分布**

词汇表 $\mathbb{V}$ 上的语言模型，由函数 $P(w_{1}w_{2}\dots w_{m})$ 表示，表示词序列 $w_{1}w_{2}\dots w_{m}$ 作为一个句子出现的可能性大小

`m-gram`：由连续 $m$ 个词构成的单元，也称 $m$ 元语法单元

由于联合概率 $P(w_{1}w_{2}\dots w_{m})$ 的参数量非常巨大，直接计算很困难：
1. 组合爆炸（Combinatorial Explosion）：一个句子包含 $m$ 个词，假设词汇表大小为 $\mathbb{V}$，所有可能的词组合为 $\mathbb{V}^m$
2. 数据稀疏性（Data Sparsity）：实际语料库中，绝大多数长词序列（如 5-gram 或更长）从未出现过，导致概率估计为 0 或极低

为了减少 $P(w_{1}w_{2}\dots w_{m})$ 模型的参数空间，可以利用句子序列通常情况下从左至右的生成过程分解：
$$
\begin{aligned}
P(w_{1}w_{2}\dots w_{m}) = 
P(w_{1})P(w_{2}|w_{1})P(w_{3}|w_{1}w_{2})\dots P(w_{m}|w_{1}w_{2}\dots w_{m-1})= \prod \limits_{i=1}^Lp(w_{i}|w_{1:i-1})
\end{aligned}
$$
$p(w_{i}|w_{1:i-1})$：给定序列 $w_{1},\dots,w_{i-1}$ 后，下一个符号是 $w_{i}$ 的条件概率

自回归语言模型：通过历史信息逐步预测序列数据的生成模型，如给出 $w_{1},\dots,w_{i-1}$ 生成 $w_{i}$，每一步生成仅依赖已生成的序列

但仅通过上述过程模型的参数空间大小依然没有下降，故进一步假设任意单词 $w_{i}$ 出现的概率只与过去 $n-1$ 个词相关：

$$
P(w_{i}|w_{1}w_{2}\dots w_{i-1}) = P(w_{i}|w_{i-(n-1)}w_{i-{n-2}}\dots w_{i-1})
$$

记 $P(w_{i}|w_{1}w_{2}\dots w_{i-1})=P(w_{i}|w_{1}^{i-1})$，则公式简记为 ：

$$P(w_{i}|w_{1}^{i-1})=P(w_{i}|w_{i-n+1}^{i-1})$$

满足公式的模型称为 $n$ 元语法或 $n$ 元文法模型

语言具备无尽的可能性，再庞大的训练语料也无法覆盖所有的 `n-gram`，而训练语料中的**零频率并不代表零概率**。因此，需要使用平滑技术（Smoothing）来解决这一问题，对**所有可能出现**的字符串都分配一个非零的概率值

平滑是指为了产生更合理的概率，对最大似然估计进行调整的一类方法，也称为**数据平滑**

平滑处理的基本思想是提高低概率事件，降低高概率事件，使整体的概率分布趋于均匀，这类方法通常称为**统计语言模型（Statistical Language models，SLM）**

平滑算法虽然较好解决了零概率问题，但是基于稀疏表示的 n 元语言模型仍然有三个较为明显的缺点：
1. 无法建模长度超过 n 的上下文
2. 依赖人工设计规则的平滑技术
3. 当 n 增大时，数据的稀疏性随之增大，模型的参数量更是指数级增加，且模型受到数据稀疏问题的影响，其参数难以被准确学习
此外，n 元文法中单词的离散表示也忽略了单词之间的相似性

2000 年 Bengio 等提出了使用前馈神经网络对 $P(w_{i}|w_{i-n+1}\dots w_{i-1})$ 进行估计的语言模型，词的 one-hot 编码被映射为一个低维稠密的实数向量，称为词向量（Word Embedding）。此后，RNN，CNN，End-to-End Memory Networks 等神经网络方法都成功应用于语言模型建模。相较于 n-gram 模型，神经网络方法可以在**一定程度上避免数据稀疏问题**，有些模型还可以避免对历史长度的限制，从而更好建模长距离依赖关系。这类方法通常称为**神经语言模型（Neural Language Models，NLM）**

深度神经网络训练需用采用有监督方法，使用标注数据进行训练，因此，语言模型的训练过程也不可避免需要构造训练语料。但是由于训练目标可以通过无标注文本直接获得，从而使得模型的训练仅需要大规模**无标注文本**即可。语言模型也成为了典型的自监督学习（Self-supervised Learning）任务

受到 CV 领域采用 ImageNet 对模型进行一次预训练，使得模型可以通过海量图像充分学习如何提取特征，然后再根据任务目标进行模型精调的预训练范式影响，NLP 领域基于预训练语言模型的方法也逐渐成为主流。以 ELMo 为代表的动态词向量模型开启了语言模型预训练的大门，此后以 GPT 和 BERT 为代表的基于 Transformer 架构的大规模预训练语言模型的出现，使得自然语言处理全面进入了预训练微调范式新时代。将预训练模型应用于下游任务时，不需要了解太多的任务细节，不需要设计特定的神经网络结构，只需要“微调”预训练模型，使用具体任务的标注数据在预训练语言模型上进行监督训练，就可以取得显著的性能提升。这类方法通常称为**预训练语言模型（Pre-trained Language Models，PLM）**

> [!note] 什么是语境学习
> 让模型在推理过程中学习，通过简单的任务说明或少量的标签数据即可以灵活地处理不同的任务
> 
> 比如，当我们期待模型完成情感识别任务时，可以额外增添几个例子。能够让模型通过类比的方式把握任务内容
> 
> ref：[语境学习](https://www.cnblogs.com/Miracevin/p/18663124)

2020 年 Open AI 发布了由包含1750亿参数的神经网络构成的生成式大规模预训练语言模型GPT-3（Generative Pre-trained Transformer 3）。开启了大规模语言模型的新时代。由于大规模语言模型的参数量巨大，如果在不同任务上都进行微调需要消耗大量的计算资源，因此预训练微调范式不再适用于大规模语言模型。但是研究人员发现，通过语境学习（In-context Learning，ICL）等方法，直接使用大规模语言模型就可以在很多任务的少样本场景下取得很好的效果。此后，研究人员们提出了面向大规模语言模型的提示词（Prompt）学习方法、模型即服务范式（Model as a Service，MaaS）、指令微调（Instruction Tuning）等方法，在不同任务上都取得了很好的效果

Kaplan 等人提出了缩放法则（Scaling Laws），指出模型的性能依赖于模型的规模，包括：
1. 参数数量
2. 数据集大小
3. 计算量
模型的效果会随着三者的指数增加而平稳提高。如图所示，模型的损失（Loss）值随着模型规模的指数增大而线性降低。这意味着模型的能力是可以根据这三个变量估计的，提高模型参数量，扩大数据集规模都可以使得模型的性能可预测地提高。这为继续提升大模型的规模给出了定量分析依据

![[pic-20250215155951127.png]]

## 大规模语言模型发展历程

基础模型阶段（18-21 年）：研究主要集中语言模型本身，包括仅编码器（Encoder Only）、编码器-解码器（Encoder-Decoder）、仅解码器（Decoder Only）等各种类型的模型结构都有相应的研究。模型大小与BERT相类似的算法，通常采用预训练微调范式，针对不同下游任务进行微调。但是模型参数量在10亿以上时，由于微调的计算量很高，这类模型的影响力在当时相较BERT类模型有不小的差距

能力探索阶段（19-22 年）：由于大规模语言模型很难针对特定任务进行微调，研究人员们开始探索在不针对单一任务进行微调的情况下如何能够发挥大规模语言模型的能力。这些方法不需要修改语言模型的参数，模型在处理不同任务时无需花费的大量计算资源进行模型微调。但是仅依赖基于语言模型本身，其性能在很多任务上仍然很难达到有监督学习效果，因此研究人员们提出了指令微调（Instruction Tuning）方案，将大量各类型任务，统一为生成式自然语言理解框架，并构造训练语料进行微调。大规模语言模型一次性学习数千种任务，并在未知任务上展现出了很好的泛化能力。2022年Ouyang等人提出了使用有监督微调再结合强化学习方法，使用少量数据有监督就可以使得大规模语言模型服从人类指令的InstructGPT算法。Nakano等人则探索了结合搜索引擎的问题回答算法WebGPT。这些方法从直接利用大规模语言模型进行零样本（zero-shot）和少样本（few-shot）学习的基础上，逐渐扩展到利用生成式框架针对大量任务进行有监督微调的方法

突破发展阶段（22 年-）：以2022年11月ChatGPT的发布为起点，2023年3月GPT-4发布，相较于ChatGPT又有了非常明显的进步，并具备了多模态理解能力

## 大规模语言模型构建流程

openAI 所使用的大规模语言模型构建流程如图所示，主要包含四个阶段：

![[pic-20250215162023789.png]]

这四个阶段都需要不同规模的数据集合以及不同类型的算法，会产生不同类型的模型，同时需要的资源也有非常大的差别

### 预训练

预训练（Pretraining）阶段需要利用海量的训练数据，包括互联网网页、维基百科、书籍、GitHub、论文、问答网站等，构建包含数千亿甚至数万亿单词的具有多样性的内容。利用由数千块高性能 GPU 和高速网络组成超级计算机，花费数十天完成深度神经网络参数训练，构建**基础语言模型（Base Model）**。基础大模型构建了长文本的建模能力，**使得模型具有语言生成能力**，根据输入的提示词（Prompt），模型可以生成文本补全句子。也有部分研究人员认为，语言模型建模过程中也隐含地构建了包括事实性知识（Factual Knowledge）和常识知识（Common sense）在内的世界知识（World Knowledge）。由于预训练过程需要消耗大量的计算资源，并很容易受到超参数影响，如何能够提升分布式计算效率并使得模型训练稳定收敛是本阶段的重点研究内容

### 有监督微调

有监督微调（Supervised Fine tuning），也称为指令微调（Instruction Tuning），利用少量高质量数据集合，通过有监督训练可以使得模型具备完成问题回答、翻译、写作等能力。有监督微调的数据包含用户输入的提示词（Prompt）和对应的理想输出结果。用户输入包括问题、闲聊对话、任务指令等多种形式和任务

例如：提示词（Prompt）：复旦大学有几个校区？

理想输出：复旦大学现有4个校区，分别是邯郸校区、新江湾校区、枫林校区和张江校区。其中邯郸校区是复旦大学的主校区，邯郸校区与新江湾校区都位于杨浦区，枫林校区位于徐汇区，张江校区位于浦东新区。

利用这些有监督数据，使用**与预训练阶段相同的**语言模型训练算法，在基础语言模型基础上再进行训练，从而得到有监督微调模型（Supervised Finetuning，SFT模型）。经过训练的SFT模型具备了初步的指令理解能力和上下文理解能力，能够完成开放领域问题、阅读理解、翻译、生成代码等能力，也具备了一定的对未知任务的泛化能力。由于有监督微调阶段的所需的训练语料数量较少，SFT模型的训练过程并不需要消耗非常大量的计算。当前的一些研究表明有监督微调阶段数据选择对SFT模型效果有非常大的影响，因此如何构造少量并且高质量的训练数据是本阶段有监督微调阶段的研究重点

### 奖励建模

奖励建模（Reward Modeling）阶段目标是构建一个文本质量对比模型，对于同一个提示词，SFT模型给出的多个不同输出结果的质量进行排序。奖励模型可以通过二分类模型，对输入的两个结果之间的优劣进行判断。RM模型与基础语言模型和SFT模型不同，**RM模型本身并不能单独提供给用户使用**。奖励模型的训练通常和SFT模型一样，使用数十块GPU，通过几天时间完成训练。由于**RM模型的准确率对于强化学习阶段的效果有着至关重要的影响**，因此对于该模型的训练通常需要大规模的训练数据。Andrej Karpathy在报告中指出，该部分需要百万量级的对比数据标注，而且其中很多标注需要花费非常长的时间才能完成。下图给出了InstructGPT系统中奖励模型训练样本标注示例。可以看到，示例中文本表达都较为流畅，标注其质量排序需要制定非常详细的规范，标注人员也需要非常认真地对标规范内容进行标注，需要消耗大量的人力，同时如何保持众包标注人员之间的一致性，也是奖励建模阶段需要解决的难点问题之一。此外奖励模型的泛化能力边界也是本阶段需要重点研究的另一个问题。如果RM模型的目标是针对所有提示词系统所生成输出都能够高质量的进行判断，该问题所面临的难度在某种程度上与文本生成等价，因此如何限定RM模型应用的泛化边界也是本阶段难点问题

![[pic-20250215175257899.png]]

### 强化学习

强化学习（Reinforcement Learning）阶段根据数十万用户给出的提示词，利用在前一阶段训练的 RM 模型，给出 SFT 模型对用户提示词补全结果的质量评估，并与语言模型建模目标综合得到更好的效果。该阶段所使用的提示词数量与有监督微调阶段类似，数量在十万量级，并且不需要人工提前给出该提示词所对应的理想回复。使用强化学习，在SFT模型基础上调整参数，使得最终生成的文本可以获得更高的奖励（Reward）。该阶段所需要的计算量相较预训练阶段也少很多，通常也仅需要数十块GPU，经过数天时间的即可完成训练。在模型参数量相同的情况下，强化学习可以得到相较于有监督微调好得多的效果。此外，Andrej Karpathy也指出强化学习也并不是没有问题的，它会使得基础模型的熵降低，从而减少了模型输出的多样性。在经过强化学习方法训练完成后的RL模型，就是最终提供给用户使用具有理解用户指令和上下文的类ChatGPT系统。由于强化学习方法稳定性不高，并且超参数众多，使得模型收敛难度大，再叠加RM模型的准确率问题，使得在大规模语言模型如何能够有效应用强化学习非常困难