大规模语言模型（Large Language Models，LLM），也称**大语言模型**或**大型语言模型**，是一种由包含数百亿以上参数的深度神经网络构建的[[../大模型理论基础/引言#什么是语言模型|语言模型]]，通常使用自监督学习方法通过大量无标注文本进行训练

## 基本概念

语言模型（Language Model，LM）目标就是建模自然语言的**概率分布**

词汇表 $\mathbb{V}$ 上的语言模型，由函数 $P(w_{1}w_{2}\dots w_{m})$ 表示，表示词序列 $w_{1}w_{2}\dots w_{m}$ 作为一个句子出现的可能性大小

`m-gram`：由连续 $m$ 个词构成的单元，也称 $m$ 元语法单元

由于联合概率 $P(w_{1}w_{2}\dots w_{m})$ 的参数量非常巨大，直接计算很困难：
1. 组合爆炸（Combinatorial Explosion）：一个句子包含 $m$ 个词，假设词汇表大小为 $\mathbb{V}$，所有可能的词组合为 $\mathbb{V}^m$
2. 数据稀疏性（Data Sparsity）：实际语料库中，绝大多数长词序列（如 5-gram 或更长）从未出现过，导致概率估计为 0 或极低

为了减少 $P(w_{1}w_{2}\dots w_{m})$ 模型的参数空间，可以利用句子序列通常情况下从左至右的生成过程分解：
$$
P(w_{1}w_{2}\dots w_{m}) = P(w_{1})P(w_{2}|w_{1})P(w_{3}|w_{1}w_{2})\dots P(w_{m}|w_{1}w_{2}\dots w_{m-1})=\prod \limits_{i=1}^Lp(w_{i}|w_{1:i-1})
\tag{1.1}
$$
$p(w_{i}|w_{1:i-1})$：给定序列 $w_{1},\dots,w_{i-1}$ 后，下一个符号是 $w_{i}$ 的条件概率

自回归语言模型：通过历史信息逐步预测序列数据的生成模型，如给出 $w_{1},\dots,w_{i-1}$ 生成 $w_{i}$，每一步生成仅依赖已生成的序列

但仅通过上述过程模型的参数空间大小依然没有下降，故进一步假设任意单词 $w_{i}$ 出现的概率只与过去 $n-1$ 个词相关：
$$
P(w_{i}|w_{1}w_{2}\dots w_{i-1}) = P(w_{i}|w_{i-(n-1)}w_{i-{n-2}}\dots w_{i-1}) \tag{1.2}\label{eq:1.2}
$$

记 $P(w_{i}|w_{1}w_{2}\dots w_{i-1})=P(w_{i}|w_{1}^{i-1})$，则公式 $\eqref{eq:1.2}$ 简记为：

$$P(w_{i}|w_{1}^{i-1})=P(w_{i}|w_{i-n+1}^{i-1})\tag{1.3}\label{eq:1.3}$$
满足公式 $\eqref{eq:1.3}$ 的模型称为 $n$ 元语法或 $n$ 元文法模型

语言具备无尽的可能性，再庞大的训练语料也无法覆盖所有的 `n-gram`，而训练语料中的**零频率并不代表零概率**。因此，需要使用平滑技术（Smoothing）来解决这一问题，对**所有可能出现**的字符串都分配一个非零的概率值

平滑是指