大规模语言模型（Large Language Models，LLM），也称**大语言模型**或**大型语言模型**，是一种由包含数百亿以上参数的深度神经网络构建的[[../大模型理论基础/引言#什么是语言模型|语言模型]]，通常使用自监督学习方法通过大量无标注文本进行训练

## 基本概念

语言模型（Language Model，LM）目标就是建模自然语言的**概率分布**

词汇表 $\mathbb{V}$ 上的语言模型，由函数 $P(w_{1}w_{2}\dots w_{m})$ 表示，表示词序列 $w_{1}w_{2}\dots w_{m}$ 作为一个句子出现的可能性大小

`m-gram`：由连续 $m$ 个词构成的单元，也称 $m$ 元语法单元

由于联合概率 $P(w_{1}w_{2}\dots w_{m})$ 的参数量非常巨大，直接计算很困难：
1. 组合爆炸（Combinatorial Explosion）：一个句子包含 $m$ 个词，假设词汇表大小为 $\mathbb{V}$，所有可能的词组合为 $\mathbb{V}^m$
2. 数据稀疏性（Data Sparsity）：实际语料库中，绝大多数长词序列（如 5-gram 或更长）从未出现过，导致概率估计为 0 或极低

为了减少 $P(w_{1}w_{2}\dots w_{m})$ 模型的参数空间，可以利用句子序列通常情况下从左至右的生成过程分解：
$$
P(w_{1}w_{2}\dots w_{m}) = P(w_{1})P(w_{2}|w_{1})P(w_{3}|w_{1}w_{2})\dots P(w_{m}|w_{1}w_{2}\dots w_{m-1})=\prod \limits_{i=1}^Lp(w_{i}|w_{1:i-1})
\tag{1.1}
$$
$p(w_{i}|w_{1:i-1})$：给定序列 $w_{1},\dots,w_{i-1}$ 后，下一个符号是 $w_{i}$ 的条件概率

自回归语言模型：通过历史信息逐步预测序列数据的生成模型，如给出 $w_{1},\dots,w_{i-1}$ 生成 $w_{i}$，每一步生成仅依赖已生成的序列

但仅通过上述过程模型的参数空间大小依然没有下降，故进一步假设任意单词 $w_{i}$ 出现的概率只与过去 $n-1$ 个词相关：

$$
P(w_{i}|w_{1}w_{2}\dots w_{i-1}) = P(w_{i}|w_{i-(n-1)}w_{i-{n-2}}\dots w_{i-1}) \tag{1.2}
$$

记 $P(w_{i}|w_{1}w_{2}\dots w_{i-1})=P(w_{i}|w_{1}^{i-1})$，则公式 1.2 简记为 ：

$$P(w_{i}|w_{1}^{i-1})=P(w_{i}|w_{i-n+1}^{i-1})\tag{1.3}$$

满足公式 1.3 的模型称为 $n$ 元语法或 $n$ 元文法模型

语言具备无尽的可能性，再庞大的训练语料也无法覆盖所有的 `n-gram`，而训练语料中的**零频率并不代表零概率**。因此，需要使用平滑技术（Smoothing）来解决这一问题，对**所有可能出现**的字符串都分配一个非零的概率值

平滑是指为了产生更合理的概率，对最大似然估计进行调整的一类方法，也称为**数据平滑**

平滑处理的基本思想是提高低概率事件，降低高概率事件，使整体的概率分布趋于均匀，这类方法通常称为**统计语言模型（Statistical Language models，SLM）**

平滑算法虽然较好解决了零概率问题，但是基于稀疏表示的n元语言模型仍然有三个较为明显的缺点：
1. 无法建模长度超过 n 的上下文
2. 依赖人工设计规则的平滑技术
3. 当 n 增大时，数据的稀疏性随之增大，模型的参数量更是指数级增加，且模型受到数据稀疏问题的影响，其参数难以被准确学习
此外，n 元文法中单词的离散表示也忽略了单词之间的相似性

2000 年 Bengio 等提出了使用前馈神经网络对 $P(w_{i}|w_{i-n+1}\dots w_{i-1})$ 进行估计的语言模型，词的 one-hot 编码被映射为一个低维稠密的实数向量，称为词向量（Word Embedding）。此后，RNN，CNN，End-to-End Memory Networks 等神经网络方法都成功应用于语言模型建模。相较于 n-gram 模型，神经网络方法可以在**一定程度上避免数据稀疏问题**，有些模型还可以避免对历史长度的限制，从而更好建模长距离依赖关系。这类方法通常称为**神经语言模型（Neural Language Models，NLM）**


