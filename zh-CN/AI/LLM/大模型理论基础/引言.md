参考资料：[大模型理论基础](https://datawhalechina.github.io/so-large-lm/)

## 什么是语言模型
任务：基于给定的文本信息输入，给出对应的新的文本/符号输出

例如：给出中文得到英文（文本翻译）、给出文本得到是否是正面情感（文本分类）、给出部分文本续写（文本扩写）……

要实现这样一个任务要解决两个问题：
1. 输入序列问题：如何将输入文本转化为神经网络可以处理的数值
2. 输出序列问题：如何将神经网络的计算结果数值转化为字符输出

上述两个问题都可以用One-Hot 编码解决：将分类值映射到整数值，每个整数值被表示为二进制向量

问题 1：例如有三个样本 `[中国, 日本, 韩国]` -> `中国：[1,0,0] 日本：[0,1,0] 韩国：[0,0,1]`，神经网络就可以读入矩阵 

问题 2：假设神经网络的输出是 3 分类，若输出 `[1,0,0]`，则可以将其映射为 `中国`

==语言模型（LM）==经典定义：一种对词元序列 (token) 的概率分布

假设有词汇表 $V$，语言模型 $p$ 为每个词元序列 $x_{1}, \dots, x_{L} \in V$ 分配一个概率：$p(x_{1},\dots,x_{L})$

通过概率告诉我们标记序列“好不好”（所谓好不好就是指这个序列可不可能出现），如果词汇表为{ate, ball, cheese, mouse, the}，语言模型可能会分配以下概率：
$$\begin{aligned}
& p(the, mouse, ate, the, cheese)=0.02
\\
& p(the, cheese ate, the, mouse)=0.01
\\
& p(mouse, the, the, cheese, ate)=0.0001
\end{aligned}$$
说明 `the mouse ate the cheese` 这句话最有可能符合人类语言习惯

