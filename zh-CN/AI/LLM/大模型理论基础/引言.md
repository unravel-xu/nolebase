参考资料：[大模型理论基础](https://datawhalechina.github.io/so-large-lm/)

## 什么是语言模型
任务：基于给定的文本信息输入，给出对应的新的文本/符号输出

例如：给出中文得到英文（文本翻译）、给出文本得到是否是正面情感（文本分类）、给出部分文本续写（文本扩写）……

要实现这样一个任务要解决两个问题：
1. 输入序列问题：如何将输入文本转化为神经网络可以处理的数值
2. 输出序列问题：如何将神经网络的计算结果数值转化为字符输出

上述两个问题都可以用One-Hot 编码解决：将分类值映射到整数值，每个整数值被表示为二进制向量

问题 1：例如有三个样本 `[中国, 日本, 韩国]` -> `中国：[1,0,0] 日本：[0,1,0] 韩国：[0,0,1]`，神经网络就可以读入矩阵 

问题 2：假设神经网络的输出是 3 分类，若输出 `[1,0,0]`，则可以将其映射为 `中国`

==语言模型（LM）==经典定义：一种对词元序列 (token) 的概率分布

假设有词汇表 $V$，语言模型 $p$ 为每个词元序列 $x_{1}, \dots, x_{L} \in V$ 分配一个概率：$p(x_{1},\dots,x_{L})$

通过概率告诉我们标记序列“好不好”（所谓好不好就是指这个序列可不可能出现），如果词汇表为{ate, ball, cheese, mouse, the}，语言模型可能会分配以下概率：
$$\begin{aligned}
& p(the, mouse, ate, the, cheese)=0.02
\\
& p(the, cheese ate, the, mouse)=0.01
\\
& p(mouse, the, the, cheese, ate)=0.0001
\end{aligned}$$
说明 `the mouse ate the cheese` 这句话最有可能符合人类语言习惯

语言模型也可以做生成任务：输入一段文本，并给定词表，在每个时刻根据当前已有文本，预测下一个词出现的概率，输出概率最高的词（生成任务也就转化成了分类任务）

当然，由于文本生成任务需要考虑上下文，语法结构等额外信息，单纯的基于概率的语言模型没法生成理想的文本

### 自回归语言模型

由全概率公式：记 $p(x_{1: L}) = p(x_{1},\dots,x_{L}) = p(x_{1})p(x_{2}|x_{1})p(x_{3}|x_{1},x_{2})\dots p(x_{L}|x_{1: L-1})=\prod \limits_{i=1}^Lp(x_{i}|x_{1:i-1})$

$p(x_{i}|x_{1:i-1})$：给定序列 $x_{1},\dots,x_{i-1}$ 后，下一个符号是 $x_{i}$ 的条件概率

自回归语言模型：通过历史信息逐步预测序列数据的生成模型，如给出 $x_{1},\dots,x_{i-1}$ 生成 $x_{i}$，每一步生成仅依赖已生成的序列

词元（token）：在文本处理中被识别为**基本单位**的单词或符号

故 $token_{i}$ 的生成概率计算为：$x_{i}\sim p(x_{i}|x_{1:i-1})^{1/T}$

称 $T\geq 0$ 为语言模型的温度参数，它决定了每次生成 $token_{i}$ 的随机性：
- T = 0 时：确定性地在每个位置 i 选择最可能的词元 $x_{i}$
- T = 1 时：从
