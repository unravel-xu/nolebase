# 用中学数学从零开始理解大型语言模型

原文链接：[Understanding LLMs from Scratch Using Middle School Math](https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876/)

参考译文：[中学生就能看懂：从零开始理解LLM内部原理](https://mp.weixin.qq.com/s?__biz=Mzk1NzQ1ODk5NQ==&mid=2247522402&idx=1&sn=9aab727e53e1f63cff791f6e745c057c&scene=21#wechat_redirect)

本文中将从头开始讲解大语言模型（LLM）的工作原理——假设你只会加法和乘法，也不会引用其他知识来源。我们从用纸和笔构建一个简单的神经网络模型开始，然后逐步深入，带你全面理解现代 LLM 和 Transformer 架构的所有细节。文章会尽量剥离掉机器学习中的复杂术语和行话，把所有内容还原为最简单的形式：数字。但必要时我们会解释相关术语，以便你在阅读带有术语的内容时能有所参照

你可以把神经网络模型看作一个魔法“盒子”，放进去一些信息，会“吐出“你期望的信息。比如，放进去一张图片，输出图片的类别；放进去一段文字，输出文字的情感类别。但要注意的是，**神经网络只能接受数字作为输入，也只能输出数字——没有例外**。所以，设计的核心就在于如何将输入转化为数字，将输出数字解释为对目标的实现，最终构建能够处理你提供的输入信息，并生成所需输出信息的神经网络

## 一个简单的神经网络

现在来看如何用加法与乘法构建一个能够对物体进行分类的简单神经网络。在这个模型中：

输入信息是已知物体的数据：
- 颜色值（RGB 值）
- 体积（Volume 单位：毫升）

目标输出是物体的分类：
- 叶子（Leaf）
- 花朵（Flower）

如下是用数字来代表“叶子”和“花朵”的示例：

|     | Leaf | Flower |
| :-: | :--: | :----: |
|  R  |  32  |  241   |
|  G  | 107  |  200   |
|  B  |  56  |   4    |
| Vol | 11.2 |  59.5  |

叶子的颜色由 RGB 值 (32, 107, 56) 表示，体积为 11.2 毫升。花朵的颜色由 RGB 值 (241, 200, 4) 表示，体积为 59.5 毫升。表中这些数据用于训练神经网络，让它学会根据“颜色”和“体积”来识别叶子和花朵

构建一个能够完成此分类任务的神经网络首先需要决定的是如何解释输入和输出，即让输入输出“数字化”。由于这里的输入已经是数字，因此可以直接送入神经网络。但输出的是类别—叶子或者花朵，而神经网络无法直接输出这些类别。因此，考虑两个方案让输出的数字和类别对应：

方案 1：输出一个数字。如果数字为正数，判断为叶子，否则判断为花朵
方案 2：输出两个数字。分别代表叶子和花朵（的概率），较大的数字代表分类结果

我们采用方案 2，因为方案 2 在后续的扩展中更具有通用性，如下是方案 2 的神经网络示意图

![[pic-20250408204111995.png]]

图中，我们输入了四个数字（色值与体积），输出了两个数字（概率）。由于输出代表叶子的概率为 0.8，代表花朵的概率为 0.2，所以最终分类结果就是叶子（0.8>0.2)

解释图中的三个术语：
- **神经元/节点（Neurons/nodes）**：图中圆圈内的数字
- **权重（Weights）**：图中连线上的数字
- **层（Layers）**：一组神经元被称为一层。上面的神经网络有三层：输入层（4 个神经元）、中间层（3 个神经元）和输出层（2 个神经元）

计算神经网络的预测/输出（也称为“前向传播 forward pass”）：
1. 从左侧开始，将 4 个数字（RGB 与体积）填入到第一层（输入层）的四个神经元中
2. 前向传播到下一层：将神经元中的数字与连线上的权重相乘，并将结果相加，填入到下一层对应的神经元，依次类推（图中演示了蓝色和橙色神经元的计算）
3. 计算完输出层的神经元后，较大的数字就对应分类结果。在上图中，较大的数字输出在代表叶子的神经元，所以分类结果就是“叶子”。经过良好训练的网络可以处理各种（RGB, Vol）输入并正确分类物体

注意，神经网络模型本身并不理解叶子、花朵或 RGB 和体积的含义。它的任务仅仅是接收 4 个数字作为输入，并输出 2 个数字。我们根据自己的解释，将输入视为 RGB 和体积，将输出视为叶子或花朵的概率。这种解释完全由我们决定，而模型的任务只是通过合适的权重将输入映射到最后的输出

有趣的是，我们可以用同一个神经网络处理完全不同的任务。比如，输入云量、湿度等 4 个数字，输出“晴天”或“雨天”的概率。如果权重校准得当，那么这个神经网络就可以同时完成叶子/花朵分类和天气预测的任务！神经网络只会输出两个数字，至于如何解释这些数字，完全由我们决定

为便于理解，上述神经网络省略了一些细节：

### 激活层（Activation Layer）

激活层是神经网络中不可缺少的一部分，它对每个神经元的输出值应用一个非线性函数，使得它能够处理更复杂的问题。激活函数常用的有 ReLU (Rectified Linear Unit)，它的规则很简单：
- 如果输入值是负数，输出设置为 0
- 如果输入值是正数，输出保持不变

![[pic-20250409104817712.png]]

在之前的神经网络中，中间层的神经元值分别是-26.6、2.52、-47.1，如果我们使用 ReLU 激活函数，-26.6、-47.1 都是负数会被替换为 0，而 2.52 是正数保持不变。在应用 ReLU 激活函数后，神经网络的计算会继续向前传播，使用这些修改后的值来计算下一层的神经元值

> [!important] 激活层是必要的

1. 如果没有激活层，神经网络的所有计算（加法和乘法）实际上可以被简化为一个简单的线性计算公式（介绍完 bias 会说明）。即输出层的每个神经元的值可以直接用输入层神经元值加权求和得到，而不需要额外的中间层
2. 激活层通过引入非线性，使得网络能够学习和拟合更复杂的关系，比如曲线分类问题，而不仅仅是简单的线性分类

### 偏置（Bias）

偏置是神经元的一个额外数值，用于调整神经元的最终输出值。它与加权求和的结果相加，用于提升模型的表达能力。每个神经元都可以有一个偏置值

![[pic-20250409124834874.png]]

假设中间层蓝色节点的偏置值是 0.25，那么计算流程如下：

![[pic-20250409122011629.png]]

如果没有偏置，模型只能通过调整权重来拟合/校准输入与输出之间的关系。引入偏置后，即使输入为零，神经元仍然可以输出一个值（偏置的值），从而使网络具有更大的灵活性

下面我们证明激活层的必要性，约定向量 $X$ 表示输入数据，第 $i$ 层的权重为 $W^{i}$，第 $i$ 层的偏置为 $B^{i}$

如果不含激活层：第一层输出可以表示为 $O^{1} = W^{1}\cdot X + B^{1}$

第二层输出可以表示为：

$$\begin{aligned}O^{2} & = W^{2}\cdot O^{1} + B ^{2}\\ & =W^{2}\cdot{\left(W^{1}\cdot X + B^{1}\right)} + B^{2} \\ 
& = W^{2}\cdot W^{1}\cdot X + W^{2}\cdot B^{1} + B^{2} \\
& = W^{'}\cdot X + B^{'}\end{aligned}$$

可以发现第二层输出和原始输入依然是线性关系

### Softmax 函数

Softmax 是一种将输出层的值转换为概率分布的函数。它的作用是：
- 将所有输出值转化为非负数（视作概率时，概率值不能为负）
- 确保所有输出值加起来等于 1（视作概率时，各情况的概率值之和为 1）

$$Softmax(z_{i}) = \frac{e^{z_{i}}}{\sum_{j=1}^{n}e^{z_{j}}}$$

其中 $z_{i}$ 是输出层的第 $i$ 个神经元的值

上图例子中，输出层的两个神经元值分别是：0.8 对应叶子，0.2 对应花朵

经过 Softmax 处理后，叶子变为：$\frac{e^{0.8}}{e^{0.8}+e^{0.2}}\approx 0.64$，花朵变为：$\frac{e^{0.2}}{e^{0.8}+e^{0.2}}\approx 0.36$

### 小结

- 激活层：引入非线性计算（否则多层和两层没有区别），提升神经网络表达能力
- 偏置：增加模型灵活性，使得输出不完全依赖于输入
- Softmax：把输出转化为概率分布

## 神经网络是如何被训练的

在上述进行叶子和花朵分类的神经网络中，我们给模型预设了合适的参数信息。但在实际应用中，参数值是如何获得的呢？获得合适参数值的过程称为“训练模型”或“训练神经网络”，这个过程需要使用一些“训练数据”，也就是“投喂”给模型用来学习的知识

现在假设我们有一组针对这个神经网络的训练数据：其中包含输入数据及其对应的叶子或花的标签。因为每组数据（R, G, B, Vol）都已经明确标注了它是“leaf”还是“flower”，所以这是一种“有标注的数据”：

|  R  |  G  |  B  | Vol  | Label  |
| :-: | :-: | :-: | :--: | :----: |
| 242 | 124 | 242 | 17.1 | flower |
| 97  | 48  | 49  | 14.9 | flower |
| 181 | 216 | 210 | 12.0 |  leaf  |
| 27  | 50  | 31  | 11.0 |  leaf  |
| 173 | 55  | 127 | 7.9  | flower |
| 229 | 71  | 216 | 7.8  | flower |

训练神经网络的具体流程如下，记住，**训练的目的是为了确定“参数”**：

### Step1. 随机初始化权重

首先，我们将每个神经元的权重初始化为一个随机值。比如像下图，这里连线上的权重数字都是随机生成（图中未列全）：

![[pic-20250409234128155.png]]

### Step2. 输入训练数据，并计算输出

以一组标注为“叶子”的输入数据为例：R = 181，G = 216，B = 210，Vol = 12.0，label = "Leaf"

由于我们需要模型根据输入数据输出正确的预测结果（也就是 label 对应的类别），所以我们的目标就是让模型输出层中表示“叶子”概率的神经元数值更大。例如，希望叶子的输出为 1，而花的输出为 0

由于当前模型的参数是随机的，因此输出数值可能并不符合我们的预期。假设模型输出为 0.6（叶子）和 0.4（花）。如下：

![[pic-20250409234725079.png]]

### Step3. 计算损失

输出层的结果（0.6，0.4）与我们期望的结果（1，0）并不一样。所以用一个数值来衡量当前输出与期望输出的差距，也就是**损失（loss）**。计算损失的方法是“损失函数”，这里用最简单的计算方法：

$$
L_{1}\ Loss = \sum\mid 期望值 - 输出值 \mid
$$

带入数值：$Loss = \mid 1 - 0.6 \mid + \mid 0 - 0.4 \mid = 0.4 + 0.4 = 0.8$

损失值越小，代表模型输出越接近我们期望的结果。理想情况下，我们希望损失接近于零，也就是“最小化损失”

### Step4. 调整权重

为了最小化损失，我们需要调整模型的权重，也就是图中连线上的数字，看看增加或减少它是否会使损失变小。比如在上面例子中：当前某权重为 0.17，输出层的叶子值比期望值低，且叶子的神经元值是正数。现在我们尝试将这个权重增加到 0.18，重新输出并计算损失。观察是否损失减小。如果减小了，说明增加这个权重是正确方向

当然在实际训练中不会每次只调整一个权重。通过这样反复多轮的调整，将所有的权重增大或者减小，最终损失就会降低

这是权重调整的最简单描述，但实际训练中要复杂的多，你还需要理解几个概念：

#### 梯度

在实际训练时，不可能是这样“摸索”式的、无方向的调整。每次调整权重你需要知道：

- **向哪个方向调整，是增大还是减小？**

比如把某个权重 0.17 应该调整到 0.18 还是 0.16？

- **调整会导致输出损失变化的速率多大？**

这决定了把某个权重 0.17 调整到 0.18，还是调整到 1.8？

在神经网络中，用“梯度”这个概念来帮助调整权重。**权重的梯度就是用来衡量损失在当前权重下的变化方向与变化率**。也就是：当改变这个权重时，损失会增大还是减小，及其变化得有多快。比如，某两个权重的梯度分别为+100，-20，意味着增大第一个权重会使得损失变大，增大第二个权重则会使损失变小；且第一个权重调整时，损失变化的更快

![[pic-20250410105143605.png]]

#### 梯度下降

现在我们可以方便的调整权重：只需要根据梯度指引的方向，根据一定的算法把所有权重朝损失减小的方向调整

这种通过梯度来调整权重（参数），让损失不断变小的方法称为**梯度下降（Gradient Descent）**，而这里的优化算法就是梯度下降算法。简单说，**梯度下降算法就是一个根据旧权重与梯度来生成新权重的公式**：

$$新权重 = 旧权重  - 学习率 \times 权重对应梯度 \ \ (学习率是一个控制调整步幅的值)$$

现在假设某个权重是 0.17，梯度是 200，学习率 0.01，那么新权重 = 0.17 - 0.01 * 200 = -1.83

把这个新权重更新到模型，准备开始下一轮的训练过程

![[pic-20250410105715972.png]]

#### 梯度计算

约定当前神经网络的权重参数为：$\theta_{0} \dots \theta_{n}$，则 $\theta_{i}$ 的梯度表达式为：

$$
\frac{\partial}{\partial \theta_{i}}J\left(\theta_{0},\theta_{1},\dots,\theta_{n}\right)
$$

其中 $J\left(\theta_{0},\theta_{1},\dots,\theta_{n}\right)$ 表示参数为 $\theta_{0},\theta_{1},\dots,\theta_{n}$ 的神经网络损失函数



如下，我们进行实例演示，约定我们使用均方误差：

$$
MSE \ Loss = \frac{1}{N} \sum \left(actual \ value - predicted \ value\right)^2
$$

### Step5. 重复迭代，逐渐优化模型

每次更新所有权重后，我们重新计算神经网络的输出与损失，并再次调整权重。随着多次迭代（即“训练”），损失会逐渐减小，模型的参数趋于合理。通常，完整训练一个数据集的过程称为**一个周期（epoch）**。通过多次周期训练，就可以让模型在整个训练集上表现良好

以上就是一个神经网络模型的训练过程，用下图做个总结：

![[pic-20250410184745799.png]]

神经网络的一次迭代（epoch）：
1. **前向传播（Forward Propagation）**：输入数据送入神经网络，输出结果
2. **计算损失（Loss Calculation）**：通过损失函数计算输出结果与期望的差异
3. **反向传播（Backward Propagation）**：计算损失关于参数(权重与偏置）的梯度
4. **参数更新（Parameter Update）**：根据梯度下降算法更新训练参数

[机器学习：一步步教你理解反向传播方法](https://yongyuan.name/blog/back-propagtion.html)

### 补充说明

- 平均损失
  
  上面计算损失时，我们都是用一个样本举例。但实际上模型训练中会涉及大量训练样本时，每个样本的损失可能并不一致。例如，模型对样本 A 的预测可能很好，但对样本 B 的预测误差很大。如果只根据某一个样本调整权重，会导致对其他样本的预测变差
  
  为了克服这一问题，通常定义**平均损失（Average Loss）**，即对所有训练样本的损失求平均值。训练过程中，我们通过优化平均损失，确保模型在整体上对所有样本表现更好，而不是对单个样本
  
  通过梯度下降算法，我们根据平均损失调整权重，既能降低高损失样本的误差，也能维持低损失样本的准确性，从而能使模型在整个数据集上有更均衡的表现

- 梯度失控
  
  在实际操作中，训练深度神经网络是一个艰难且复杂的过程，因为在训练过程中，梯度很容易失控（特别是在非常深的神经网络中）
  
  权重梯度可能会变得非常小 **（梯度消失）**：导致权重无法根据梯度做更新
  
  权重梯度也可能非常大 **（梯度爆炸）**：权重变化时导致损失变化过于剧烈
  
  梯度消失与梯度保障会导致训练不稳定。实际应用中，会通过改进的损失计算函数、激活函数等方法来改进，暂时你只需要知道这个问题的含义即可

## 神经网络如何生成语言

我们已经了解到什么是神经网络模型，以及一个神经网络模型大致是如何被训练的。这个训练好的神经网络可以被用于预测一组数据代表的是“叶子”还是“花”，只要有正确的权重设置，它甚至可以被训练用来预测“未来一小时的天气”。但我们需要研究的是语言模型（LLM），而不是一个预测模型。那么问题来了：

这个可以预测“叶子”与“花”的神经网络模型如何用来生成自然语言呢？

方法很简单。神经网络的本质：输入一些数字，经过模型内部的数学计算（基于训练好的参数），输出另一些数字。 而这些输入和输出的数字代表什么，完全取决于你如何解释和训练

所以，你当然也可以训练一个神经网络模型，将输入的数字代表一个句子的前几个字符，而输出的数字则被解释为这个句子的下一个字符的概率

这里有一些关键问题要解决，如下：

### 输出层的扩展

与预测叶子或花的二分类不同，自然语言输出的字符要多得多，以英文字符为例，数量远多于 2。假设我们需要预测下一个英文字符是什么，那么可以设计神经网络的输出层至少要包含 26 个神经元（其实还需要考虑一些符号如空格、句号等），让每个神经元对应一个英文字母（或符号），并将输出层每个神经元的数字解释为对应字母的概率。即：**输出层中数值最大的神经元对应的字母就是预测的下一个字符**

举个例子，假设输入的内容是 “I love y”，经过我们的训练并设置好正确的权重，现在神经网络的输出如下：

![[pic-20250410233655839.png]]

输出层神经元的数值及其对应的字母为：$a=0.11,b=0.23,c=0.08\dots,o=0.80\dots$

我们选择数值最大的神经元（0.80），对应字母 “o”，也就是预测的下一个字符就是 “o”，于是现在我们可以把内容变成：“I love y*o*”

### 输入层的扩展

由于神经网络只能接收数字输入，无法直接理解字符。因此需要将输入的字符序列（如 “I love y”）转化为数字。一种简单的方法是：

**给每个输入字符分配一个数字**，比如 a=1，b=2，…，z=26，空格用 27 表示等等。那么如果你需要输入“I love y"，则对应输入层神经元的各个值为：$[9,27,12,15,22,5,27,25]$

现在，我们已经拥有了一个设置好权重的神经网络，能够准确预测下一个字符。这里我们输入了字符"I love y"（实际输入 $[9,27,12,15,22,5,27,25]$)，预测了下一个字符“o“（实际输出是每个字符的概率，o 的概率最大）

### 生成完整句子

我们可以生成一个字符，借助递归思想，就可以生成一个完整的句子：

- 输入“I love y”，模型预测输出 “o”
- 将预测的 “o” 加入输入中，形成新的序列 “I love yo”
- 将新的序列“I love yo"输入到神经网络，预测出下一个字符“u"
- 依此类推，不断重复这个过程，最后我们递归生成了一个句子“I love you so much"。这种方式下，神经网络就具备了生成自然语言的能力，成为一个非常基础的生成式 AI 语言模型

### 上下文长度问题

由于神经网络的输入层大小是固定的，比如只能接受 8 个字符，也就是接受“I love y”的每个字符。但是当我们预测了第一个字符“o”以后，就无法把完整的“I love yo”这 9 个字符输入到神经网络中

解决方法是：用类似队列（queue）“先进先出”的方式保持固定的队列大小，也可以想象成一种“滑动窗口”。把最开始的第一个字符"I"踢出去，只发送最近的 8 个字符，即“ love yo"，然后预测出"u"；接着再输入"love you"，预测出空格...。类似下图（为了方便查看，这里空格用_表示），输入的灰色字符表示在本轮被丢弃：

![[pic-20250411111230803.png]]

因此，如果最后生成 “I love you so much”，到最后生成“h”时，前面的“I love yo”字符在输入时已经被丢弃。这样的设计会导致神经网络逐渐“遗忘”早期的信息，尤其在生成长句子时，会影响预测质量。这种固定的输入到神经网络的长度也被称为**上下文长度**，即**提供给神经网络模型用来预测的最大输入长度**。现代神经网络已经显著提高了上下文长度（数千上万个单词），因此模型可以参考更多上下文来生成更连贯的文本

### 输入和输出

你可能注意到，当输入字符时，我们用简单的数字编码（如 “i” = 9）；而在输出时，神经网络需要输出多个数字（即代表不同字符概率的数值），并挑选最大的那个，为什么不使用相同的编码方式呢？比如预测出“o”，那么就直接输出数字 15？

原因是：**不同的输入和输出解释，更有助于模型的训练和表现**

- 输入：在输入时，通常希望信息尽可能精确并方便处理，事实上，这里的简单输入数字的方式也不是最优的，一些更复杂的编码方式（如嵌入向量）能更有效地表达字符之间的关系，后面会介绍
- 输出：使用多个神经元，每个代表一个可能的字符（概率），能更灵活地让模型量化每个候选字符的输出情况，也更容易优化

所以，输入的重点是表达清楚，方便模型理解复杂的关系；而输出的重点是表达不确定性，让模型更好的预测多种可能性并不断优化。因为自然语言中充满复杂关系，灵活地选择输入输出的解释方式，能让模型更容易应对这些复杂性。这种不对称设计，是现代语言模型（如 GPT 等）获得高效性能的关键

## 什么让 LLM 如此有效？

最早的模型通过逐字符生成“I love you so much”，这与当前最先进的大型语言模型的功能相去甚远，但它却是这些先进模型的核心原理。通过一系列创新和改进，生成式 AI 从这种简单的形式演变为能够进行类人对话的机器人。那么，当前的先进模型究竟在哪些方面做了改进呢？

## 嵌入（Embeddings）

我们已经把一个能够预测“叶子”还是“花”的简单神经网络，扩展成预测句子“下一个输出字符”的初步语言模型：

![[pic-20250411120513566.png]]

### 从简单的字符映射到嵌入

上述模型中，我们将输入字符表示为某个特定数字。例如，a = 1，b = 2，依此类推。这种方法虽然简单，但存在明显的缺陷：这些数字没有任何语义信息，无法捕捉字符之间的丰富的语义关系或上下文

“嵌入”就是为了解决这个问题。**嵌入是一种将字符、单词或符号映射到一组数字的方法。这些数字不是随意选择的，而是通过训练模型学习得到的**。核心思想是：通过优化输入表示，使模型能够更好地捕捉语言的结构和语义

我们称这一组数字为“向量”，向量是一个有序的数字集合。例如，一个长度为 10 的向量可以表示为 $[0.1, 0.2, 0.3, ..., 0.10]$。注意，每个数字在向量中的位置是固定的，交换位置会得到不同的向量。就像“叶子 / 花朵”的数据，如果交换输入的 R 和 G 值，就会得到不同的颜色，也就不再是同一个向量

### 嵌入向量是如何训练出来的？

训练嵌入向量与训练神经网络相似，通过梯度下降来获得最优的向量表示：

1. 初始化嵌入向量：首先为每个字符分配一个随机的向量，即一组数字
2. 输入到网络：我们将这些向量传递到神经网络中进行处理
3. 优化向量：与优化参数类似，通过计算损失函数，来优化这些嵌入向量的值。通过梯度下降，不断调整这些向量，使得模型输出更接近我们期望的结果

随着训练的进行，这些向量会逐渐变得更加“智能”，它们会捕捉到每个字符的语义特征，并且这种嵌入方式可以在不同的模型中复用。例如，如果我们为字符“a”学习到的向量是 $[0.2, 0.5, 0.1, 0.3]$，那么每次我们遇到字符“a”时，模型都会使用这个向量进行处理，而不必每次重新随机初始化

### 向量如何输入神经网络

假设我们为每个字符分配一个长度为 10 的向量。那么，如果输入“humpty dumpt”这一串 12 个字符，就需要将每个字符的向量拼接起来，形成一个长度为 120 的输入层（12 字符 $\times$ 10 数字/字符）

- 输入层的扩展：原本的输入层可能只有 12 个神经元（每个字符对应一个神经元），但现在扩展到了 120 个神经元。每个字符的向量被依次排列，输入到网络中
- 所有的嵌入向量长度必须相同，否则我们就无法将所有字符组合输入到网络中。例如，“humpty dumpt” 和下一次迭代中的 “umpty dumpty”，在这两种情况下，我们都要向网络中输入 12 个字符，对应 120 个神经元

### 嵌入矩阵

为了方便管理和使用嵌入向量，这里引入嵌入矩阵的概念

嵌入矩阵是一个二维数组，其中每一列对应一个字符或单词的嵌入向量。假设我们有 26 个字母，每个字母的向量长度是 10，那么嵌入矩阵的大小就是 $10\times26$：

![[pic-20250411125409536.png]]

当我们需要表示某个字符时，只需查找嵌入矩阵中对应的列。字母“a”的向量就是嵌入矩阵的第一列

嵌入矩阵不仅可以用于字符，还可以用于单词、符号，甚至更复杂的语言单元。它的灵活性使得嵌入成为现代语言模型的核心组件之一。通过嵌入将词汇转换成的向量还有一个特征，这些向量可以捕捉到词汇之间的相似性。例如，词汇的嵌入向量可以捕捉到“king”和“queen”之间的语义关系，即它们的嵌入向量距离非常接近

## 子词分词器（Sub-word tokenizers）



## 其他参考资料

- [动手学机器学习](https://hml.boyuai.com/)
- [Google 机器学习教育](https://developers.google.com/machine-learning?hl=zh-cn)