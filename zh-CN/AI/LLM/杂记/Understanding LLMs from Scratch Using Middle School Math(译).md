# 用中学数学从零开始理解大型语言模型

原文链接：[Understanding LLMs from Scratch Using Middle School Math](https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876/)

参考译文：[中学生就能看懂：从零开始理解LLM内部原理](https://mp.weixin.qq.com/s?__biz=Mzk1NzQ1ODk5NQ==&mid=2247522402&idx=1&sn=9aab727e53e1f63cff791f6e745c057c&scene=21#wechat_redirect)

本文中将从头开始讲解大语言模型（LLM）的工作原理——假设你只会加法和乘法，也不会引用其他知识来源。我们从用纸和笔构建一个简单的神经网络模型开始，然后逐步深入，带你全面理解现代 LLM 和 Transformer 架构的所有细节。文章会尽量剥离掉机器学习中的复杂术语和行话，把所有内容还原为最简单的形式：数字。但必要时我们会解释相关术语，以便你在阅读带有术语的内容时能有所参照

你可以把神经网络模型看作一个魔法“盒子”，放进去一些信息，会“吐出“你期望的信息。比如，放进去一张图片，输出图片的类别；放进去一段文字，输出文字的情感类别。但要注意的是，**神经网络只能接受数字作为输入，也只能输出数字——没有例外**。所以，设计的核心就在于如何将输入转化为数字，将输出数字解释为对目标的实现，最终构建能够处理你提供的输入信息，并生成所需输出信息的神经网络

## 一个简单的神经网络

现在来看如何用加法与乘法构建一个能够对物体进行分类的简单神经网络。在这个模型中：

输入信息是已知物体的数据：
- 颜色值（RGB 值）
- 体积（Volume 单位：毫升）

目标输出是物体的分类：
- 叶子（Leaf）
- 花朵（Flower）

如下是用数字来代表“叶子”和“花朵”的示例：

|     | Leaf | Flower |
| :-: | :--: | :----: |
|  R  |  32  |  241   |
|  G  | 107  |  200   |
|  B  |  56  |   4    |
| Vol | 11.2 |  59.5  |

叶子的颜色由 RGB 值 (32, 107, 56) 表示，体积为 11.2 毫升。花朵的颜色由 RGB 值 (241, 200, 4) 表示，体积为 59.5 毫升。表中这些数据用于训练神经网络，让它学会根据“颜色”和“体积”来识别叶子和花朵

构建一个能够完成此分类任务的神经网络首先需要决定的是如何解释输入和输出，即让输入输出“数字化”。由于这里的输入已经是数字，因此可以直接送入神经网络。但输出的是类别—叶子或者花朵，而神经网络无法直接输出这些类别。因此，考虑两个方案让输出的数字和类别对应：

方案 1：输出一个数字。如果数字为正数，判断为叶子，否则判断为花朵
方案 2：输出两个数字。分别代表叶子和花朵（的概率），较大的数字代表分类结果

我们采用方案 2，因为方案 2 在后续的扩展中更具有通用性，如下是方案 2 的神经网络示意图

![[pic-20250408204111995.png]]

图中，我们输入了四个数字（色值与体积），输出了两个数字（概率）。由于输出代表叶子的概率为 0.8，代表花朵的概率为 0.2，所以最终分类结果就是叶子（0.8>0.2)

解释图中的三个术语：
- **神经元/节点（Neurons/nodes）**：图中圆圈内的数字
- **权重（Weights）**：图中连线上的数字
- **层（Layers）**：一组神经元被称为一层。上面的神经网络有三层：输入层（4 个神经元）、中间层（3 个神经元）和输出层（2 个神经元）

计算神经网络的预测/输出（也称为“前向传播 forward pass”）：
1. 从左侧开始，将 4 个数字（RGB 与体积）填入到第一层（输入层）的四个神经元中
2. 前向传播到下一层：将神经元中的数字与连线上的权重相乘，并将结果相加，填入到下一层对应的神经元，依次类推（图中演示了蓝色和橙色神经元的计算）
3. 计算完输出层的神经元后，较大的数字就对应分类结果。在上图中，较大的数字输出在代表叶子的神经元，所以分类结果就是“叶子”。经过良好训练的网络可以处理各种（RGB, Vol）输入并正确分类物体

注意，神经网络模型本身并不理解叶子、花朵或 RGB 和体积的含义。它的任务仅仅是接收 4 个数字作为输入，并输出 2 个数字。我们根据自己的解释，将输入视为 RGB 和体积，将输出视为叶子或花朵的概率。这种解释完全由我们决定，而模型的任务只是通过合适的权重将输入映射到最后的输出

有趣的是，我们可以用同一个神经网络处理完全不同的任务。比如，输入云量、湿度等 4 个数字，输出“晴天”或“雨天”的概率。如果权重校准得当，那么这个神经网络就可以同时完成叶子/花朵分类和天气预测的任务！神经网络只会输出两个数字，至于如何解释这些数字，完全由我们决定

为便于理解，上述神经网络省略了一些细节：

### 激活层（Activation Layer）

激活层是神经网络中不可缺少的一部分，它对每个神经元的输出值应用一个非线性函数，使得它能够处理更复杂的问题。激活函数常用的有 ReLU (Rectified Linear Unit)，它的规则很简单：
- 如果输入值是负数，输出设置为 0
- 如果输入值是正数，输出保持不变

![[pic-20250409104817712.png]]

在之前的神经网络中，中间层的神经元值分别是-26.6、2.52、-47.1，如果我们使用 ReLU 激活函数，-26.6、-47.1 都是负数会被替换为 0，而 2.52 是正数保持不变。在应用 ReLU 激活函数后，神经网络的计算会继续向前传播，使用这些修改后的值来计算下一层的神经元值

> [!important] 激活层是必要的

1. 如果没有激活层，神经网络的所有计算（加法和乘法）实际上可以被简化为一个简单的线性计算公式（介绍完 bias 会说明）。即输出层的每个神经元的值可以直接用输入层神经元值加权求和得到，而不需要额外的中间层
2. 激活层通过引入非线性，使得网络能够学习和拟合更复杂的关系，比如曲线分类问题，而不仅仅是简单的线性分类

### 偏置（Bias）

偏置是神经元的一个额外数值，用于调整神经元的最终输出值。它与加权求和的结果相加，用于提升模型的表达能力。每个神经元都可以有一个偏置值

![[pic-20250409124834874.png]]

假设中间层蓝色节点的偏置值是 0.25，那么计算流程如下：

![[pic-20250409122011629.png]]

如果没有偏置，模型只能通过调整权重来拟合/校准输入与输出之间的关系。引入偏置后，即使输入为零，神经元仍然可以输出一个值（偏置的值），从而使网络具有更大的灵活性

下面我们证明激活层的必要性，约定向量 $X$ 表示输入数据，第 $i$ 层的权重为 $W^{i}$，第 $i$ 层的偏置为 $B^{i}$

如果不含激活层：第一层输出可以表示为 $O^{1} = W^{1}\cdot X + B^{1}$

第二层输出可以表示为：

$$\begin{aligned}O^{2} & = W^{2}\cdot O^{1} + B ^{2}\\ & =W^{2}\cdot{\left(W^{1}\cdot X + B^{1}\right)} + B^{2} \\ 
& = W^{2}\cdot W^{1}\cdot X + W^{2}\cdot B^{1} + B^{2} \\
& = W^{'}\cdot X + B^{'}\end{aligned}$$

可以发现第二层输出和原始输入依然是线性关系

### Softmax 函数

Softmax 是一种将输出层的值转换为概率分布的函数。它的作用是：
- 将所有输出值转化为非负数（视作概率时，概率值不能为负）
- 确保所有输出值加起来等于 1（视作概率时，各情况的概率值之和为 1）

$$Softmax(z_{i}) = \frac{e^{z_{i}}}{\sum_{j=1}^{n}e^{z_{j}}}$$

其中 $z_{i}$ 是输出层的第 $i$ 个神经元的值

上图例子中，输出层的两个神经元值分别是：0.8 对应叶子，0.2 对应花朵

经过 Softmax 处理后，叶子变为：$\frac{e^{0.8}}{e^{0.8}+e^{0.2}}\approx 0.64$，花朵变为：$\frac{e^{0.2}}{e^{0.8}+e^{0.2}}\approx 0.36$

### 小结

- 激活层：引入非线性计算（否则多层和两层没有区别），提升神经网络表达能力
- 偏置：增加模型灵活性，使得输出不完全依赖于输入
- Softmax：把输出转化为概率分布

## 神经网络是如何被训练的

在上述进行叶子和花朵分类的神经网络中，我们给模型预设了合适的参数信息。但在实际应用中，参数值是如何获得的呢？获得合适参数值的过程称为“训练模型”或“训练神经网络”，这个过程需要使用一些“训练数据”，也就是“投喂”给模型用来学习的知识

现在假设我们有一组针对这个神经网络的训练数据：其中包含输入数据及其对应的叶子或花的标签。因为每组数据（R, G, B, Vol）都已经明确标注了它是“leaf”还是“flower”，所以这是一种“有标注的数据”：

|  R  |  G  |  B  | Vol  | Label  |
| :-: | :-: | :-: | :--: | :----: |
| 242 | 124 | 242 | 17.1 | flower |
| 97  | 48  | 49  | 14.9 | flower |
| 181 | 216 | 210 | 12.0 |  leaf  |
| 27  | 50  | 31  | 11.0 |  leaf  |
| 173 | 55  | 127 | 7.9  | flower |
| 229 | 71  | 216 | 7.8  | flower |

训练神经网络的具体流程如下，记住，**训练的目的是为了确定“参数”**：

### Step1. 随机初始化权重

首先，我们将每个神经元的权重初始化为一个随机值。比如像下图，这里连线上的权重数字都是随机生成（图中未列全）：

![[pic-20250409234128155.png]]

### Step2. 输入训练数据，并计算输出

以一组标注为“叶子”的输入数据为例：R = 181，G = 216，B = 210，Vol = 12.0，label = "Leaf"

由于我们需要模型根据输入数据输出正确的预测结果（也就是 label 对应的类别），所以我们的目标就是让模型输出层中表示“叶子”概率的神经元数值更大。例如，希望叶子的输出为 1，而花的输出为 0

由于当前模型的参数是随机的，因此输出数值可能并不符合我们的预期。假设模型输出为 0.6（叶子）和 0.4（花）。如下：

![[pic-20250409234725079.png]]

### Step3. 计算损失

输出层的结果（0.6，0.4）与我们期望的结果（1，0）并不一样。所以用一个数值来衡量当前输出与期望输出的差距，也就是**损失（loss）**。计算损失的方法是“损失函数”，这里用最简单的计算方法：

$$
L_{1}\ Loss = \sum\mid 期望值 - 输出值 \mid
$$

带入数值：$Loss = \mid 1 - 0.6 \mid + \mid 0 - 0.4 \mid = 0.4 + 0.4 = 0.8$

损失值越小，代表模型输出越接近我们期望的结果。理想情况下，我们希望损失接近于零，也就是“最小化损失”

### Step4. 调整权重

为了最小化损失，我们需要调整模型的权重，也就是图中连线上的数字，看看增加或减少它是否会使损失变小。比如在上面例子中：当前某权重为 0.17，输出层的叶子值比期望值低，且叶子的神经元值是正数。现在我们尝试将这个权重增加到 0.18，重新输出并计算损失。观察是否损失减小。如果减小了，说明增加这个权重是正确方向

当然在实际训练中不会每次只调整一个权重。通过这样反复多轮的调整，将所有的权重增大或者减小，最终损失就会降低

这是权重调整的最简单描述，但实际训练中要复杂的多，你还需要理解几个概念：

#### 梯度

在实际训练时，不可能是这样“摸索”式的、无方向的调整。每次调整权重你需要知道：

- **向哪个方向调整，是增大还是减小？**

比如把某个权重 0.17 应该调整到 0.18 还是 0.16？

- **调整会导致输出损失变化的速率多大？**

这决定了把某个权重 0.17 调整到 0.18，还是调整到 1.8？

在神经网络中，用“梯度”这个概念来帮助调整权重。**权重的梯度就是用来衡量损失在当前权重下的变化方向与变化率**。也就是：当改变这个权重时，损失会增大还是减小，及其变化得有多快。比如，某两个权重的梯度分别为+100，-20，意味着增大第一个权重会使得损失变大，增大第二个权重则会使损失变小；且第一个权重调整时，损失变化的更快

![[pic-20250410105143605.png]]

#### 梯度下降

现在我们可以方便的调整权重：只需要根据梯度指引的方向，根据一定的算法把所有权重朝损失减小的方向调整

这种通过梯度来调整权重（参数），让损失不断变小的方法称为**梯度下降（Gradient Descent）**，而这里的优化算法就是梯度下降算法。简单说，**梯度下降算法就是一个根据旧权重与梯度来生成新权重的公式**：

$$新权重 = 旧权重  - 学习率 \times 权重对应梯度 \ \ (学习率是一个控制调整步幅的值)$$

现在假设某个权重是 0.17，梯度是 200，学习率 0.01，那么新权重 = 0.17 - 0.01 * 200 = -1.83

把这个新权重更新到模型，准备开始下一轮的训练过程

![[pic-20250410105715972.png]]

#### 梯度计算

约定当前神经网络的权重参数为：$\theta_{0} \dots \theta_{n}$，则 $\theta_{i}$ 的梯度表达式为：

$$
\frac{\partial}{\partial \theta_{i}}J\left(\theta_{0},\theta_{1},\dots,\theta_{n}\right)
$$

其中 $J\left(\theta_{0},\theta_{1},\dots,\theta_{n}\right)$ 表示参数为 $\theta_{0},\theta_{1},\dots,\theta_{n}$ 的神经网络损失函数



如下，我们进行实例演示，约定我们使用均方误差：

$$
MSE \ Loss = \frac{1}{N} \sum \left(actual \ value - predicted \ value\right)^2
$$

### Step5. 重复迭代，逐渐优化模型

每次更新所有权重后，我们重新计算神经网络的输出与损失，并再次调整权重。随着多次迭代（即“训练”），损失会逐渐减小，模型的参数趋于合理。通常，完整训练一个数据集的过程称为**一个周期（epoch）**。通过多次周期训练，就可以让模型在整个训练集上表现良好

以上就是一个神经网络模型的训练过程，用下图做个总结：

![[pic-20250410184745799.png]]

神经网络的一次迭代（epoch）：
1. **前向传播（Forward Propagation）**：输入数据送入神经网络，输出结果
2. **计算损失（Loss Calculation）**：通过损失函数计算输出结果与期望的差异
3. **反向传播（Backward Propagation）**：计算损失关于参数(权重与偏置）的梯度
4. **参数更新（Parameter Update）**：根据梯度下降算法更新训练参数

[机器学习：一步步教你理解反向传播方法](https://yongyuan.name/blog/back-propagtion.html)

### 补充说明

- 平均损失
  
  上面计算损失时，我们都是用一个样本举例。但实际上模型训练中会涉及大量训练样本时，每个样本的损失可能并不一致。例如，模型对样本 A 的预测可能很好，但对样本 B 的预测误差很大。如果只根据某一个样本调整权重，会导致对其他样本的预测变差
  
  为了克服这一问题，通常定义**平均损失（Average Loss）**，即对所有训练样本的损失求平均值。训练过程中，我们通过优化平均损失，确保模型在整体上对所有样本表现更好，而不是对单个样本
  
  通过梯度下降算法，我们根据平均损失调整权重，既能降低高损失样本的误差，也能维持低损失样本的准确性，从而能使模型在整个数据集上有更均衡的表现



## 其他参考资料

- [动手学机器学习](https://hml.boyuai.com/)
- [Google 机器学习教育](https://developers.google.com/machine-learning?hl=zh-cn)