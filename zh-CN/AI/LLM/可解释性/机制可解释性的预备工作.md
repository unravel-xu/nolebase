# 机制可解释性的预备工作

原文链接：[Mechanistic Interpretability Prerequisites](https://www.neelnanda.io/mechanistic-interpretability/prereqs)

## 写这篇博客的起因

这篇博客介绍了理解、编写代码并为机制可解释性研究做出贡献所必备的技能

## 核心技能

- 数学部分：
	- 线性代数：[3Blue1Brown](https://www.youtube.com/watch?v=fNk_zzaMoSs) 或 [Linear Algebra Done Right](https://linear.axler.net/)
		- 核心目标——能深入直观的理解如下概念：
			- Basis（基）
			- Change of basis（基变化）
			- 一个向量空间是一个几何对象（geometric object），它不一定具有规范基
			- 矩阵是两个向量空间之间的线性映射（或某个向量空间到其自身的映射）
		- 有用的额外概念：
			- 什么是 SVD？为什么 SVD 有用？
			- 什么是正交/单位正交矩阵，将正交基变化和一般基变化有什么重要区别？
			- 什么是特征值和特征向量，它们反应了线性映射的哪些信息？
	- 概率论基础
		- 基础分布：期望，标准差，正态分布
		- 对数似然（Log likelihood）
		- 最大值估计
		- 随机变量
		- 中心极限定理
	- 微积分基础
		- 梯度
		- 链式法则
		- 反向传播的直观理解——反向传播本质就是多元函数的链式法则
- 编程部分：
	- Python 基础
		- 已经有太多教你编程的优质课程，你可以挑一个适合自己的
		- Zac Hatfield-Dodds 推荐先阅读 Al Sweigart 的 [Automate the Boring Stuff with Python](https://automatetheboringstuff.com/) 再阅读 [Beyond the Basic Stuff with Python](https://inventwithpython.com/beyond/)（这两本书都可以免费阅读）如果您喜欢传统教科书风格，[Think Python 2e](https://greenteapress.com/wp/think-python-2e/) 是一个优先的选择
	- NumPy 基础
		- 尝试完成 [100 numpy exercises](https://github.com/rougier/numpy-100) 前三分之一的习题。推荐使用 pytorch 中的 tensor 完成
- 机器学习：
	- 机器学习基础
		- [fast.ai](https://course.fast.ai/) 是一个不错的入门教程，但比起必要学习的内容稍显繁琐。如果想要高效学习（按照 80/20 法则——最重要的仅有 20%），推荐重点观看 Andrej Karpathy 的[神经网络讲解视频](https://www.youtube.com/watch?v=VMj-3S1tku0)
	- PyTorch 基础
		- 学到哪儿查到哪儿
		- 目标：在 Google Colab 的 GPU 上搭建并运行线性回归模型
		- 使用 PyTorch 时最容易出问题的地方就是张量（tensor）操作，尤其是张量的乘法。**强烈推荐学习使用 [einops](https://einops.rocks/1-einops-basics/)**（操作单个张量的库）和 [einsum](https://rockt.github.io/2018/04/30/einsum)（PyTorch 内置函数，实现了[爱因斯坦求和约定](https://zhuanlan.zhihu.com/p/71639781)）
	- Transformers——LLM 和传统 ML 关于 mechanistic interpretability 区别在于你需要非常深入的理解所使用的 LLM 架构、LLM 中的组成部分以及它们是如何配合工作的。而 LLM 中最重要的组件就是 Transformer！
		- 视频教程：[what is a transformer](https://www.neelnanda.io/transformer-tutorial) and [implementing GPT-2 From Scratch](https://www.neelnanda.io/transformer-tutorial-2)
			- [My transformer glossary/explainer](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=pndoEIqJ6GPvC1yENQkEfZYR)
		- 值得做的练习：手动实现与教程配套的 [template notebook](https://www.neelnanda.io/transformer-template)
			- 这个 notebook 附带了测试用例，因此你可以验证你的代码是否正确运行。当你完成这个练习时，你将拥有一个可以实际运行的 GPT-2
			- 当你完成上述任务，可以说你基本上已经完全理解了 Transformer 架构
		- 其他可能有助于理解的资料：
			- Nelson Elhage 的 [Transformers for Software Engineers](https://blog.nelhage.com/post/transformers-for-software-engineers/)
			- [the illustrated transformer](https://jalammar.github.io/illustrated-transformer/)
	- Bonus: [Jacob Hilton’s Deep learning for Alignment syllabus](https://github.com/jacobhilton/deep_learning_curriculum)

当学完上述预备知识，可以阅读[[学习Transformer可解释性的具体步骤]]

**请一定要动手实践！**